[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "posts",
    "section": "",
    "text": "Building a university event embedding recommender\n\n\nPart 1: MVP\n\n\n\ncs\n\n\nwolverine events\n\n\n\n\n\n\n\n\n\n10/11/24\n\n\nE. Cantu\n\n\n\n\n\n\n\n\n\n\n\n\nRNNs from scratch exercises\n\n\n\n\n\n\ndeep learning\n\n\nexercises\n\n\n\n\n\n\n\n\n\n9/27/24\n\n\nE. Cantu\n\n\n\n\n\n\n\n\n\n\n\n\nA Closer Look at Memorization in Deep Networks\n\n\nAn attempted (partial) paper reproduction\n\n\n\ndeep learning\n\n\npaper\n\n\n\n\n\n\n\n\n\n9/7/24\n\n\nE. Cantu\n\n\n\n\n\n\n\n\n\n\n\n\nApproximate Nearest Cosine Neighbors\n\n\n\n\n\n\ncs\n\n\nquick intro\n\n\n\nUsing Random Hyperplane LSH\n\n\n\n\n\n8/9/24\n\n\nE. Cantu\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Batch Normalization\n\n\nAn attempted (partial) paper reproduction\n\n\n\ndeep learning\n\n\npaper\n\n\n\n\n\n\n\n\n\n7/17/24\n\n\nE. Cantu\n\n\n\n\n\n\n\n\n\n\n\n\nBatch Norm exercises\n\n\n\n\n\n\ndeep learning\n\n\nexercises\n\n\n\n\n\n\n\n\n\n7/2/24\n\n\nE. Cantu\n\n\n\n\n\n\n\n\n\n\n\n\nLeNet exercises\n\n\n\n\n\n\ndeep learning\n\n\nexercises\n\n\n\n\n\n\n\n\n\n6/21/24\n\n\nE. Cantu\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning is Robust to Massive Label Noise\n\n\nAn attempted (partial) paper reproduction\n\n\n\ndeep learning\n\n\npaper\n\n\n\n\n\n\n\n\n\n6/18/24\n\n\nE. Cantu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/wolv-events/index.html",
    "href": "posts/wolv-events/index.html",
    "title": "Building a university event embedding recommender",
    "section": "",
    "text": "I am currently a grad student at the University of Michigan and like to attend seminars, lectures, and social events when I can. However, I often attend the same weekly seminar or other recurring events because sifting through all events (588 this week) to find ones that interest me is very time-consuming. Last semester, I tried feeding the events to ChatGPT and asking for recommendations, but they were not great.\nSo, given it’s fall break, I want to hack into a simple recommender system to do a better job. The goal is to get to an MVP that other students can use this weekend and iterate on it during the semester. Let’s see what we can come up with.\n\nOverview\nI want to keep the recommender and overall system as simple as possible. We’ll start with content-based filtering, where users are recommended similar events to ones they have previously enjoyed. To do so, we need a way for users to give feedback on events and a way of determining how similar two events are.\nFor feedback, we can allow users to downvote, upvote, and add an event to their calendar (in which case they are interested and plan on actually attending):\n\n\n\n\n\nFor similarity, we can simply represent our events as strings, obtain their embeddings, and use the cosine distance between events. To keep things simple (and affordable), we can just use OpenAI’s API.\nBecause I’d like to implement changes quickly and don’t expect more than a handful of users, we’ll use Python, Flask, and SQLite. Let’s jump in.\n\n\nData\nThe university’s event page, Happening @ Michigan is great. It provides openly available endpoints to fetch events with all of their details in JSON format. Great! No scrapping is necessary. We can simply fetch the events of the day or week to keep the system up-to-date.\n\n\nEmbeddings\nOne of the biggest challenges (as far as I can tell) in recommender systems is coming up with vector representations of the items (movies, products, events, etc.) such that distances between are semantically meaningful. Luckily, it’s now very easy and convenient to throw items to a neural network and get such embeddings.\nFirst, let’s take a look at three events and how we might represent them as strings:\n\nfor e in [69, 70, 71]: print(f'{\"=\"*50}\\n{to_embed[e]}')\n\n==================================================\nCareers / Jobs:Workshop: UROP Resume Review Open House at The Career Center\nUROP students are welcome to drop in for resume reviews, as they prepare to apply for upcoming research projects.  Career Center staff and UROP Peer Advisors will be on hand to answer application and resume questions.\nWhere:The Career Center\nWhen:Friday 12\nSponsors:University Career Center\n==================================================\nCareers / Jobs:Workshop: Freshman Friday\nJoin us on Fridays from 12-1:00 for FREE FOOD and fun staff!  Meet The Career Center Advisors in an informal setting.\nWhere:The Career Center\nWhen:Friday 12\nSponsors:University Career Center\n==================================================\nCareers / Jobs:Workshop: UROP Resume Workshop\nThis is a closed resume workshop for students in the UROP program.\nWhere:The Career Center\nWhen:Friday 11\nSponsors:University Career Center\n\n\nWe include the event type before the title, the description (truncated to about 200 characters), the location name and date, and sponsoring groups. Note that we only include the day of the week and the start hour in the date to make it easier for the text model to make sense of. For now, we’ll use OpenAI’s text-embedding-3-small model, but we can try others later.\nWe now have a 1536-dimentional vector representing each event and can use dot products to determine similarity. For example, For the events above, we have:\n\n(E[[69, 70, 71]] @ E[[69, 70, 71]].T)[0]\n\narray([0.99999999, 0.64823593, 0.89352812])\n\n\nwhich makes sense since the first event is most similar to itself and then to the other resume workshop.\n\n\nKNN recommender\nIf we encode downvotes as -1, upvotes as 1 and “adds to calendar” as 2, we could implement a very simple recommender. Given a user and a new event, look up the event’s k-nearest neighbors (using embeddings and dot products) with ratings by the user and return the average rating. This average rating can be considered the estimated rating the user would give to the new event. Repeat for each new event and return events with the highest predicted ratings.\nThis system would be fairly simple to spin up, and we could even keep the KNN lookup constant by using approximate methods like those provided in faiss or any of the many vector databases.\nHowever, we still have to keep around a growing list of past events (at least their embeddings) and user ratings. To avoid running out of a cheap VM’s disk space (being very, very optimistic about user count), we could prune the oldest ratings once in a while or come up with some other simple scheme. But I wanted to try something else out.\n\n\nClustering: constant storage?\nWhat if we …\n\ncluster a bunch of past events and obtain their centroids\nhave users somehow rate clusters\ncan estimate a new event’s rating by their distance to different centroids\n\nThen it seems that we only need to maintain a user’s rating of each centroid and forget the individual events they up/down vote for. I.e., once a week passes, we can delete those events from our database.\nTo see how this would work, we first cluster past events. We get events from the last 10 years, keep only the nonrecurring ones (about 60k), and embed them to obtain\n\nE.shape\n\n(59745, 1536)\n\n\nNow use K-means to cluster them into 1,000 clusters. Why 1,000? I’m sure there are better methodologies, but for now I inspected the resulting assignments for 10, 100, 500, etc. and picked where I felt the clusters were specific enough while still having a good number of events. It’s not perfect, and I’ll probably come back and play with it some more.\nAnyway, now we have 1,000 centroids:\n\ncentroids.shape\n\n(1000, 1536)\n\n\nAnd each of which is rated by a user. We set a user’s initial ratings to be random but close to zero, to give some diversity in their recommendations when they sign up:\n\nuser_ratings = np.random.uniform(-1e-5, 1e-5, 1000)\nuser_ratings.shape\n\n(1000,)\n\n\nNow, when we get the new events of the week, we can embed them (curr_E) and calculate their distances to each centroid:\n\ndists_to_centroids = euclidean_distances(curr_E, centroids)\ncurr_E.shape, dists_to_centroids.shape\n\n((641, 1536), (641, 1000))\n\n\nHow do we get the expected rating the user will give to each event? Intuitively, we would like each centroid to “have a say” in the final rating, with closer events given more priority. I.e., we’ll weigh centroids inversely to their distance to the new events:\n\ninv_temp = 1\ncluster_weights =  1 / (dists_to_centroids + 1e-6)\ncluster_weights = softmax(cluster_weights * inv_temp)\n\nWhere we use softmax to have the weights add up to one. In addition, we can play around with the “inverse temperature” \\(\\tau\\) parameter to weigh the closest centroids more (higher \\(\\tau\\)) or less (smaller \\(\\tau\\) ) heavily.\n\n\nPlaying with inv. temperature\nw =  1 / (dists_to_centroids + 1e-6)\ntaus = [0.5, 1, 2]\nfor tau in reversed(taus):\n    w = softmax(cluster_weights * tau)\n    plt.hist(w[0], label = r'$\\tau = $' + f'{tau}', alpha = 0.50)\n\nplt.axvline(1/len(centroids), label = '1 / num_clusters', color = 'black', linestyle = '--')\nplt.gca().set_xticks(plt.gca().get_xticks()[1::2])\nplt.xlabel('Cluster weights')\nplt.legend()\n\n\n\n\n\n\n\n\n\nAwesome, we can now weigh each cluster. So for a new event, we can get the estimated rating by taking the weighted sum:\n\nnew_event_ix = 0\ncluster_weights[new_event_ix] @ user_ratings\n\n-1.3426919962571645e-07\n\n\nAnd then recommend those new events with the highest predicted ratings:\n\nn_recs = 5\npred_ratings = cluster_weights @ user_ratings\nrec_ixs = np.argsort(pred_ratings)[-n_recs:]\nrec_ixs\n\narray([475,  22, 555, 554, 201])\n\n\nThe only thing missing is a way to update the user’s ratings of centroids when they rate an event. Intuitively, once a user rates an event, we should change the entries in user_ratings proportionally to the weight they were given in the prediction and the difference between the predicted and actual rating. In fact, we made the prediction on event \\(i\\) with\n\\[\n\\hat y_i(w_i, u) = w_i^\\top u\n\\]\nWhere \\(w_i\\) are the cluster weights for the event and \\(u\\) the user’s ratings of the centroids. Let \\(y_i\\) the actual rating given by the user. We can then define our loss (with a convenient 1/2 that cancels later) as\n\\[\nL(\\hat y_i, y_i) = \\frac{1}{2}(\\hat y_i - y_i)^2\n\\]\nNoting that we only want to update \\(u\\) and not the embeddings or centroids, we can get\n\\[\n\\frac{\\partial L}{\\partial u} = (\\hat y_i - y_i) \\frac{\\partial \\hat y_i}{\\partial u} = (\\hat y_i - y_i) w_i^\\top\n\\]\nAnd can perform gradient descent:\n\\[\nu \\leftarrow u - \\lambda (\\hat y_i - y_i) w_i^\\top\n\\]\nOr in code:\n\n# Before update\nrated_ix, rating = 0, 2 # event rated and rating given\npred_ratings[rated_ix]\n\n0.00020218347827034036\n\n\n\n# Update\nlr = 0.1  # step size / lambda\nuser_ratings -= lr * cluster_weights[rated_ix] * (pred_ratings[rated_ix] - rating)\n\n# After update\npred_ratings = cluster_weights @ user_ratings\npred_ratings[rated_ix]\n\n0.00040448075950620985\n\n\nAnd that’s the basics. Later on, I’ll play with different optimizers that can account for sparse features, clustering algorithms, and so on. But for now, I just want to implement it.\n\n\nImplementation\nAssume we have embedded and clustered the bunch of past events and have the centroids. The basic idea is to fetch the events for the upcoming week Sundays and embed them. We then compute their distances to the centroids (or even the weights) and store them (the NumPy arrays as blobs in the db). And those are the only things we batch.\nI drew some inspiration and referenced from karpathy’s arxiv-sanity lite. For auth, I decided to go with Signin With Google as the university is on GSuite. Users can also join a community mailing list to get reminded weekly when recommendations are ready.\nThe code is openly available here and site hosted here.\n\n\nA lot of room\nfor improvement. For example:\n\nOnce the clusters and centroids are computed, there is no way to change them. Either make sure they are good enough so that we never have to change them or come up with something different.\nFind a way to set parameters (learning rate, inv. temperature) in a more principled way, or test thoroughly. Should each user have their own learning rate?\n\nAnd I’m sure I’ll come up with others."
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html",
    "href": "dl-playground/d2l-exercises/8-5.html",
    "title": "Batch Norm exercises",
    "section": "",
    "text": "Where I attempt to solve the exercises in section 8.5 of the d2l book from scratch in pytorch (without using the d2l library).\nImports\ntry:\n    import matplotlib.pyplot as plt\nexcept:\n    !pip3 install matplotlib\n    import matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os, itertools, time, random\n\ndevice = 'cpu'"
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#context",
    "href": "dl-playground/d2l-exercises/8-5.html#context",
    "title": "Batch Norm exercises",
    "section": "Context",
    "text": "Context\nThe section introduces how to implement batch norm (BN) and some of the intuitions behind its effectiveness.\nAs a quick recap, batch norm layers apply the following transform to their inputs:\n\\[\nBN(x) = \\gamma \\odot \\frac{x-\\hat \\mu_B}{\\hat \\sigma_B} + \\beta\n\\]\nWhere \\(\\gamma\\), \\(\\beta\\) are learned and \\(\\mu_B\\), \\(\\sigma_B\\) are estimated using the input’s minibatch \\(B\\) during training.\nI.e. batch norm first normalizes the input to have mean \\(0\\) and std \\(1\\), facilitating convergence during optimization.\nHowever, since BN is typically applied before activation (at least traditionally), doing so will reduce the expressive power of the layer. For instance, as pointed out by the original paper, “normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity.” Below, we plot a sigmoid and note that in the [-1, 1] range (where most of the normalized data would fall) it is essentially linear.\n\n\nPlot sigmoid\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\nt_constrained = np.linspace(-1, 1, 100)\nt_full = np.linspace(-4, 4, 100)\nsigmoid = lambda t:1/(1 + np.exp(-t))\nax1.plot(t_full, sigmoid(t_full))\nax1.plot(t_constrained, sigmoid(t_constrained), c = 'r')\nax2.plot(t_constrained, sigmoid(t_constrained), c = 'r')\nax1.set_ylabel('sigmoid(x)')\nax1.set_xlabel('x')\nax2.set_xlabel('x')\nf.tight_layout()\n\n\n\n\n\n\n\n\n\nSo to maintain the layer’s expressive power (degrees of freedom), “we make sure that the transformation inserted in the network can represent the identity transform” and introduce \\(\\gamma\\) and \\(\\beta\\). So if it is optimal to leave the input unchanged, the network can learn to do so by setting \\(\\gamma = \\sigma_B(x)\\) and \\(\\beta = \\bar x_B\\). (This part was confusing as the section only mentions: “Next, we apply a scale coefficient and an offset to recover the lost degrees of freedom” - the paper provided clarification).\nFinally, the second reason batch norm seems to help is the implicit regularization it provides by injecting noise into the training process. What noise? \\(\\hat \\mu_B\\) and \\(\\hat \\sigma_B\\) are (noisy) estimates calculated on a sample (the minibatch). Thus, the size of the minibatch \\(|B|\\) plays an important role: too small and the estimates are too high variance; too big and the estimates become too stable (noiseless).\nAnyway, let’s get to the exercises."
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q1",
    "href": "dl-playground/d2l-exercises/8-5.html#q1",
    "title": "Batch Norm exercises",
    "section": "Q1",
    "text": "Q1\nShould we remove the bias parameter from the fully connected layer or the convolutional layer before the batch normalization? Why?\nI believe we could remove the bias parameter from both the fully connected and convolution layers if BN is applied as described in the section: right after the fully connected / convolution layer but before the activation \\(\\phi\\). Why? essentially BN is location invariant because it centers the minibatch at 0:\n\\[\nBN_{\\gamma, \\beta}(x + \\alpha) =  BN_{\\gamma, \\beta'}(x)\n\\]\nThus in the fully connected layer case: \\[\n\\boldsymbol h = \\phi(BN_{\\gamma, \\beta}(\\boldsymbol{Wx + b})) = \\phi(BN_{\\gamma, \\beta'}(\\boldsymbol{Wx}))\n\\]\nIn convolution layers, we apply BN per channel, across all locations. I.e. “each channel has its own scale and shift parameters, both of which are scalars”. And since the convolution layer also outputs a scalar bias per channel, a similar argument applies.\nBut should we remove the biases? Yes. We get the same expressive power with fewer params.\nLet’s try it out empirically on MNIST by training the BNLeNet network defined in the section, removing bias on linear and convolution layers, in turn. Although we’ll not get the same learned parameters (\\(\\beta \\to \\beta'\\)), we should get comparable performance.\n\n\nModel definition\ndef init_cnn(module):\n    \"\"\"Initialize weights for CNNs.\"\"\"\n    if type(module) in [nn.Linear, nn.Conv2d]:\n        nn.init.xavier_uniform_(module.weight)\n\n\nclass BNLeNet(nn.Module):\n    def __init__(self, num_classes=10, removed_bias = 'linear'):\n        super().__init__()\n        assert removed_bias in ['linear', 'conv', 'none']\n        self.net = nn.Sequential(\n            nn.LazyConv2d(6, kernel_size=5, bias = removed_bias == 'conv'), nn.LazyBatchNorm2d(),\n            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5, bias = removed_bias == 'conv'), nn.LazyBatchNorm2d(),\n            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(), nn.LazyLinear(120, bias = removed_bias == 'linear'), nn.LazyBatchNorm1d(),\n            nn.Sigmoid(), nn.LazyLinear(84, bias = removed_bias == 'linear'), nn.LazyBatchNorm1d(),\n            nn.Sigmoid(), nn.LazyLinear(num_classes))\n        \n    def forward(self, x):\n        return self.net(x)\n\n\n\n\nTraining function\ndef train(net, train_loader, test_loader, num_epochs = 5, lr = 0.1, verbose = True):\n    # Infer input shapes, initialize weights and move to device\n    _ = net(next(iter(train_loader))[0]) # Necessary before initing weights\n    net.apply(init_cnn)\n    net.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=lr)\n    \n    test_accs = []\n    for epoch in range(num_epochs):\n        net.train()\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = net(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        \n        eval_acc = eval_net(net, test_loader)\n        test_accs.append(eval_acc)\n        if verbose: print(f'Epoch {epoch + 1}/{num_epochs}, Test acc: {eval_acc}')\n        \n    return test_accs\n\n\n\n\nTrain and evaluate\n# Set data loader seed for reproducibility\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ntest_loader = DataLoader(testing_data, batch_size = 128, shuffle = False)\n\nfor removed_bias in ['none', 'linear', 'conv']:\n\n    torch.manual_seed(0); np.random.seed(0); random.seed(0)\n    g = torch.Generator()\n    g.manual_seed(0)\n    net = BNLeNet(removed_bias = removed_bias)\n    train_loader = DataLoader(training_data, batch_size = 128, shuffle = True, worker_init_fn = seed_worker, generator = g)\n\n    train(net, train_loader, test_loader, num_epochs = 10, lr = 0.1, verbose = False)\n    print(f'{removed_bias} final test acc: \\t{eval_net(net, test_loader)}')\n\n\nnone final test acc:    0.9821\nlinear final test acc:  0.9869\nconv final test acc:    0.9824\n\n\nClose enough."
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q2",
    "href": "dl-playground/d2l-exercises/8-5.html#q2",
    "title": "Batch Norm exercises",
    "section": "Q2",
    "text": "Q2\nCompare the learning rates for LeNet with and without batch normalization.\n\nPlot the increase in validation accuracy.\nHow large can you make the learning rate before the optimization fails in both cases?\n\n\n\nModel definitions\nclass LeNet(nn.Module):\n\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.LazyLinear(120), nn.Sigmoid(),\n            nn.LazyLinear(84), nn.Sigmoid(),\n            nn.LazyLinear(num_classes)\n        )\n\n    def forward(self, X):\n        return self.net(X)\n        \nclass BNLeNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.LazyLinear(120), nn.LazyBatchNorm1d(), nn.Sigmoid(),\n            nn.LazyLinear(84), nn.LazyBatchNorm1d(), nn.Sigmoid(),\n            nn.LazyLinear(num_classes))\n        \n    def forward(self, x):\n        return self.net(x)\n\n\nWe tried \\(lr \\in \\{4, 2, 1, 0.5, 0.1, 0.05\\}\\) as \\(0.05\\) and \\(4\\) are the lowest and highest rates where LeNet still trains (in 10 epochs).\n\n\nPlot results\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey = True)\n\nfor ax, name, results in zip([ax1, ax2], ['Without BN', 'With BN'], [without_bn, with_bn]):\n    for lr, accs in zip(lrs, results):\n        ax.plot(accs, label=f'lr={lr}')\n        ax.set_title(f'{name}')\n        ax.set_xlabel('Epoch')\n\nax1.set_ylabel('Validation Accuracy')\nax2.legend()\nf.tight_layout()\n\n\n\n\n\n\n\n\n\nAnd we observe that applying batch norm helps early training (we achieve better performance earlier) and makes training more robust to learning rate selection.\nUnderstanding Batch Normalization poses that batch norm’s main benefit is that it allows for greater learning rates by containing activation blowup (especially in later layers), which in turn biases the optimization to “flatter” minimas with better generalization.\nIt seems our small experiment aligns with the paper, even though our network is quite shallow."
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q3",
    "href": "dl-playground/d2l-exercises/8-5.html#q3",
    "title": "Batch Norm exercises",
    "section": "Q3",
    "text": "Q3\nDo we need batch normalization in every layer? Experiment with it.\nThe paper demonstrates that it is more beneficial in later layers. Let’s see if it is in our case and remove each batch norm layer in turn. We logged test accuracies and activations and tested 5 nets per configuration.\n\n\nRedefine the model\nclass BNLeNet(nn.Module):\n    def __init__(self, num_classes = 10, ex_bn_layers = []):\n        # ex_bn_layers: list of BN layers to exclude (1, ..., 4)\n\n        super().__init__()\n        for i in ex_bn_layers: assert i in range(1, 5), 'There are only 4 BN layers'\n\n        layers = [\n            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.LazyLinear(120), nn.LazyBatchNorm1d(), nn.Sigmoid(),\n            nn.LazyLinear(84), nn.LazyBatchNorm1d(), nn.Sigmoid(),\n            nn.LazyLinear(num_classes)\n        ]\n\n        bn_idx = [i for i, module in enumerate(layers) if 'BatchNorm' in str(module)]\n        for i in ex_bn_layers: layers[bn_idx[i - 1]] = nn.Identity()\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n\n\n\nTrain and evaluate\ntrain_loader = DataLoader(training_data, batch_size = 128, shuffle = True)\ntest_loader = DataLoader(testing_data, batch_size = 128, shuffle = False)\nimgs, labels = next(iter(train_loader))\n\nex_bn_layers = [[], [1], [2], [3], [4]]\nmag_layers = [2, 6, 11, 14]\nresults = []\n\nfor ex in ex_bn_layers:\n\n    print(ex)\n\n    for epochs in [2, 4, 8]:\n\n        print(epochs)\n    \n        for i in range(5):\n\n            net = BNLeNet(ex_bn_layers = ex)\n            test_accs = train(net, train_loader, test_loader, num_epochs = epochs, lr = 0.1, verbose = False)\n\n            tmp = {\n                'Excluded BN layers': ex[0] if ex else 0,\n                'Epochs': epochs,\n                'Test accuracy': test_accs[-1],\n                'Iter': i\n            }\n            \n            for lyr in mag_layers:\n                sub = net.net[:lyr]\n                with torch.no_grad(): \n                    mag = (sub(imgs) ** 2).mean()\n                    tmp[f'Mean mag at {lyr}'] = mag.item()\n            \n            results.append(tmp)\n\n\nDoes performance change?\n\n\nPlot test accuracies\nres = pd.DataFrame(results)\nres['Excluded BN layers'].replace({0: 'None', 1: '2', 2: '6', 3: '11', 4: '14'}, inplace = True)\n\nsns.barplot(data = res, x = 'Excluded BN layers', y = 'Test accuracy', hue = 'Epochs')\nplt.ylim(0.5, 1)\n\n\n\n\n\n\n\n\n\nIt seems that removing the first batch norm (in the second layer) has the strongest hit on performance.\nWhat about activations? Let’s plot the \\(l_2\\) norm of activations of layers that come right after batch norm layers for a single minibatch:\n\n\nPlot mean magnitudes\nf, axs = plt.subplots(1, 4, figsize=(16, 4))\n\nfor i, lyr in enumerate(mag_layers):\n    sns.barplot(data = res, x = 'Excluded BN layers', y = f'Mean mag at {lyr}', ax = axs[i], hue='Epochs')\n    axs[i].set_title(f'Layer {lyr}')\n    axs[i].set_ylabel('')\n\naxs[0].set_ylabel('Mean act L2 magnitude')\nf.tight_layout()\n\n\n\n\n\n\n\n\n\nWe can definitely see an effect when we remove batch norm layers. In general, the activation magnitudes decrease in the removed layer. I.e. layer 2’s magnitudes when its batch norm is removed are lower than in the original network, and so on for the other layers. It also seems that other layers compensate for this decrease by increasing their magnitudes. Finally, as the paper points out, the effects appear stronger in early epochs."
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q4",
    "href": "dl-playground/d2l-exercises/8-5.html#q4",
    "title": "Batch Norm exercises",
    "section": "Q4",
    "text": "Q4\nImplement a “lite” version of batch normalization that only removes the mean, or alternatively one that only removes the variance. How does it behave?\nWe can freeze BatchNorm’s weight and bias params respectively:\n\n\nTrain and evaluate\nfor remove_mean, remove_var in [[True, True], [False, True], [True, False], [False, False]]:\n\n    # All nets \"see\" the same data\n    torch.manual_seed(0); np.random.seed(0); random.seed(0)\n    g = torch.Generator()\n    g.manual_seed(0)\n\n    net = BNLeNet()\n    for module in net.net:\n        if 'BatchNorm' in str(module):\n            if remove_mean:\n                module.bias.requires_grad = False\n            if remove_var:\n                module.weight.requires_grad = False\n\n    train_loader = DataLoader(training_data, batch_size = 128, shuffle = True, worker_init_fn = seed_worker, generator = g)\n\n    test_accs = train(net, train_loader, test_loader, num_epochs = 10, lr = 0.1, verbose = False)\n    print(f'Remove mean: {remove_mean} Remove var: {remove_var}\\t\\tfinal test acc: {test_accs[-1]}')\n\n\nRemove mean: True Remove var: True      final test acc: 0.9674\nRemove mean: False Remove var: True     final test acc: 0.9676\nRemove mean: True Remove var: False     final test acc: 0.9796\nRemove mean: False Remove var: False        final test acc: 0.98\n\n\nIt appears that only removing the variance has more of an effect in our case. Is this generally true? Haven’t found anything online yet."
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q5",
    "href": "dl-playground/d2l-exercises/8-5.html#q5",
    "title": "Batch Norm exercises",
    "section": "Q5",
    "text": "Q5\nFix the parameters beta and gamma. Observe and analyze the results.\nWe can accomplish this by affine = False in BatchNorm layers.\n\n\nModel definition\nclass BNLeNet(nn.Module):\n    def __init__(self, num_classes = 10, affine_bn = True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(affine = affine_bn), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(affine = affine_bn), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.LazyLinear(120), nn.LazyBatchNorm1d(affine = affine_bn), nn.Sigmoid(),\n            nn.LazyLinear(84), nn.LazyBatchNorm1d(affine = affine_bn), nn.Sigmoid(),\n            nn.LazyLinear(num_classes))\n        \n    def forward(self, x):\n        return self.net(x)\n\n\n\n\nTrain and evaluate\nresults = {'affine': [], 'non affine': []}\nos.makedirs('nets', exist_ok = True)\n\nfor affine in [True, False]:\n    aff = 'affine' if affine else 'non affine'\n    for seed in range(5):\n\n        # All nets \"see\" the same data\n        torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n        g = torch.Generator()\n        g.manual_seed(seed)\n\n        net = BNLeNet(affine_bn = affine)\n\n        train_loader = DataLoader(training_data, batch_size = 128, shuffle = True, worker_init_fn = seed_worker, generator = g)\n        test_accs = train(net, train_loader, test_loader, num_epochs = 10, lr = 0.1, verbose = False)\n\n        results[aff].append(test_accs)\n        torch.save(net, f'nets/{aff}_{seed}.pt')\n\n        print(f'Affine: {affine}\\t\\tSeed: {seed}\\t\\tfinal test acc: {test_accs[-1]}')\n\n\n\n\nPlot validation accuracies\ntmp = []\nfor aff, res in results.items():\n    for i, test_accs in enumerate(res):\n        for epoch, acc in enumerate(test_accs):\n            tmp.append({'Affine': aff, 'Val accuracy': acc, 'Seed': i, 'Epoch': epoch + 1})\ntmp = pd.DataFrame(tmp)\nsns.lineplot(data = tmp, x = 'Epoch', y = 'Val accuracy', hue = 'Affine')\n\n\n\n\n\n\n\n\n\nIt seems performance is worse during early epochs but converges as training progresses. Let’s see the batch norm with affine=True layers learned weights and biases somewhat departed from their defaults (\\(1\\) and \\(0\\) respectively).\n\n\nPlot weights and biases\nlayers = [1, 5, 10, 13]\nweights = {l:[] for l in layers}\nbiases = {l:[] for l in layers}\n\nfor fname in os.listdir('nets'):\n    if 'non' in fname: continue\n    net = torch.load('nets/' + fname)\n    for layer in layers:\n        weights[layer].extend(net.net[layer].weight.detach().tolist())\n        biases[layer].extend(net.net[layer].bias.detach().tolist())\n\nf, axs = plt.subplots(2, 4, figsize=(12, 6))\nfor i, (layer, w) in enumerate(weights.items()):\n    sns.histplot(w, ax = axs[0, i])\n    axs[0, i].set_title(f'Layer {layer} BN weights')\n    axs[0, i].axvline(1, c = 'r')\n\n    sns.histplot(biases[layer], ax = axs[1, i])\n    axs[1, i].set_title(f'Layer {layer} BN biases')\n    axs[1, i].axvline(0, c = 'r')\n\n    if i &gt; 0: axs[0, i].set_ylabel(''); axs[1, i].set_ylabel('')\n\nf.tight_layout()\n\n\n\n\n\n\n\n\n\nIt seems that they did, especially the weights.\nNote: In Q4 and Q5 we removed various components of batch norm layers were removed and performance was compared. However, we used the same fixed learning rate \\(0.1\\) for all configurations. A more thorough analysis would have found the optimal learning rate for each configuration, as Appendix F of the paper does:"
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q6",
    "href": "dl-playground/d2l-exercises/8-5.html#q6",
    "title": "Batch Norm exercises",
    "section": "Q6",
    "text": "Q6\nCan you replace dropout by batch normalization? How does the behavior change?"
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q7",
    "href": "dl-playground/d2l-exercises/8-5.html#q7",
    "title": "Batch Norm exercises",
    "section": "Q7",
    "text": "Q7\nResearch ideas: think of other normalization transforms that you can apply:\n\nCan you apply the probability integral transform?\nCan you use a full-rank covariance estimate? Why should you probably not do that?\nCan you use other compact matrix variants (block-diagonal, low-displacement rank, Monarch, etc.)?\nDoes a sparsification compression act as a regularizer?\nAre there other projections (e.g., convex cone, symmetry group-specific transforms) that you can use?"
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html",
    "href": "dl-playground/d2l-exercises/7-6.html",
    "title": "LeNet exercises",
    "section": "",
    "text": "Where I attempt to solve the exercises in section 7.6 of the d2l book from scratch in pytorch (without using the d2l library).\nImports\n!pip3 install matplotlib\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os, itertools, time\n\ndevice = 'cuda'"
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html#context",
    "href": "dl-playground/d2l-exercises/7-6.html#context",
    "title": "LeNet exercises",
    "section": "Context",
    "text": "Context\nThe book section ties together convolution/pooling layer concepts seen in previous sections and introduces the original LeNet architecture:\nWhich is succinctly defined in PyTorch as\nnn.Sequential(\n    nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Flatten(),\n    nn.LazyLinear(120), nn.Sigmoid(),\n    nn.LazyLinear(84), nn.Sigmoid(),\n    nn.LazyLinear(num_classes)\n)\nWithout forgetting to use uniform Xavier initialization. Let’s define the net:\n\nclass LeNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.LazyLinear(120), nn.Sigmoid(),\n            nn.LazyLinear(84), nn.Sigmoid(),\n            nn.LazyLinear(num_classes)\n        )\n\n    def forward(self, X):\n        return self.net(X)\n\n# Used later on to initialize the weights\ndef init_weights(m):\n    if isinstance(m, nn.Linear) or type(m) == nn.Conv2d:\n        torch.nn.init.xavier_uniform_(m.weight)\n\nAnd try to train it without using the d2l library.\nWe first get our dataset and dataloaders. We’ll wrap it in a function for convinience later.\n\n\nGet data and loaders\ndef get_data(dataset, batch_size):\n    \n    data_params = {'root': 'data', 'transform': transforms.ToTensor(), 'download': True}\n    train_dataset = dataset(train = True, **data_params)\n    test_dataset = dataset(train = False, **data_params)\n    \n    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n\n    return train_dataset, test_dataset, train_loader, test_loader\n\n_, _, train_loader, test_loader = get_data(datasets.FashionMNIST, 128)\n\n\nAnd functions to evaluate our models and plot losses.\n\n\nModel eval and loss ploting functions\ndef eval_model(model, test_loader):  \n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            _, pred = torch.max(model(images), 1)\n            correct += (pred == labels).float().sum().item()\n    return correct / len(test_loader.dataset)\n\ndef plot_results(losses, test_acc):\n    plt.figure()\n    plt.plot(losses)\n    plt.xlabel('Epoch'); plt.ylabel('Cross Entropy Loss')\n    plt.title(f'Test Accuracy: {test_acc:.2f}')\n\n\nAnd finally train it using SGD with learning rate 0.1 for 15 epochs\n\n\nCode\ndef train(net, train_loader, lr = 0.1, epochs = 15, verbose = True):\n\n    # Infer input shapes, initialize weights and move to device\n    _ = net(next(iter(train_loader))[0]) # Necessary before initing weights\n    net.apply(init_weights)\n    net.to(device)\n    net.train()\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr = lr)\n\n    losses = []\n    for epoch in range(epochs):\n    \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(net(images), labels)\n            loss.backward()\n            optimizer.step()\n            \n        losses.append(loss.item())\n        if verbose: print(f'Epoch: {epoch + 1} \\tLoss: {loss.item():.2f}')\n\n    return losses\n        \nnet = LeNet(10)\nlosses = train(net, train_loader, verbose = False)\ntest_acc = eval_model(net, test_loader) \nplot_results(losses, test_acc)\n\n\n\n\n\n\n\n\n\nAnd we achieve reasonable performance. Let’s now attempt the section questions."
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html#q1",
    "href": "dl-playground/d2l-exercises/7-6.html#q1",
    "title": "LeNet exercises",
    "section": "Q1",
    "text": "Q1\nLet’s modernize LeNet. Implement and test the following changes:\n\nReplace average pooling with max-pooling.\nReplace the softmax layer with ReLU.\n\nWe define another module and replace nn.Sigmoid’s for nn.ReLu’s and nn.AvgPool2d’s for nn.MaxPool2d’s:\n\nclass ModernLeNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.LazyLinear(120), nn.ReLU(),\n            nn.LazyLinear(84), nn.ReLU(),\n            nn.LazyLinear(num_classes)\n        )\n\n    def forward(self, X):\n        return self.net(X)\n\n\n\nCode\nnet = ModernLeNet(10)\nlosses = train(net, train_loader, verbose = False)\ntest_acc = eval_model(net, test_loader) \nplot_results(losses, test_acc) \n\n\n\n\n\n\n\n\n\nAnd we achieve a non-trivial improvement in performance. We even observed the loss increase slightly, indicating that our learning rate it too high (or we should decrease it on a schedule)."
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html#q2",
    "href": "dl-playground/d2l-exercises/7-6.html#q2",
    "title": "LeNet exercises",
    "section": "Q2",
    "text": "Q2\nTry to change the size of the LeNet style network to improve its accuracy in addition to max-pooling and ReLU.\n\nAdjust the convolution window size.\n\nLets make everything a parameter:\n\nclass TweakableModernLeNet(nn.Module):\n\n    def __init__(\n            self, num_classes, conv_kernel = 5, out_channels = [6, 16],\n            hidden_dims = [120, 84]):\n        \n        super().__init__()\n        layers = [\n            nn.Sequential(\n            nn.LazyConv2d(out_c, kernel_size=conv_kernel, padding=2), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)) for out_c in out_channels\n        ] + [nn.Flatten()] + [\n            nn.Sequential(nn.LazyLinear(dim), nn.ReLU()) for dim in hidden_dims\n        ] + [nn.LazyLinear(num_classes)]\n    \n        self.net = nn.Sequential(*layers)\n\n    def forward(self, X):\n        return self.net(X)\n\nLet’s try adjusting the convolution window size \\(c_w \\in \\{3,..,7\\}\\). We’ll train 5 nets per size and report the average:\n\n\nChanging convolution window size\nn_avg = 5\nconv_sizes = range(3, 7 + 1)\ntest_accs = []\nfor conv_size in conv_sizes:\n    avg = 0\n    for _ in range(n_avg):\n        net = TweakableModernLeNet(10, conv_kernel = conv_size)\n        train(net, train_loader, verbose = False)\n        avg += eval_model(net, test_loader)\n    test_accs.append(avg / n_avg)\n\nplt.plot(conv_sizes, test_accs)\nplt.xlabel('Convolution kernel size'); plt.ylabel('Test Accuracy')\n\n\nText(0, 0.5, 'Test Accuracy')\n\n\n\n\n\n\n\n\n\nLooking at the y-axis, it seems the kernel size has only a small effect on performance in this case, and that 5 is a reasonable choice.\n\nAdjust the number of output channels.\n\nWe can try halving, doubling and tripling the original [6, 16] output channels:\n\n\nChanging output channel sizes\nn_avg = 3\nout_channels = [[3, 8], [6, 16], [12, 22], [24, 44]]\ndf = []\nfor out_c in out_channels:\n    avg = 0\n    for _ in range(n_avg):\n        net = TweakableModernLeNet(10, out_channels = out_c)\n        train(net, train_loader, verbose = False)\n        avg += eval_model(net, test_loader)\n    df.append({'Output channels': out_c, 'Test Accuracy': avg / n_avg})\n\ndf = pd.DataFrame(df)\ndf\n\n\n\n\n\n\n\n\n\nOutput channels\nTest Accuracy\n\n\n\n\n0\n[3, 8]\n0.885500\n\n\n1\n[6, 16]\n0.894867\n\n\n2\n[12, 22]\n0.901233\n\n\n3\n[24, 44]\n0.908233\n\n\n\n\n\n\n\nAgain, the gains seem marginal and increase compute so the original seems reasonable.\n\nAdjust the number of convolution layers.\n\nLets try adding up to 2 additional convolution layers:\n\n\nAdding convolution layers\nn_avg = 3\nout_channels = [[6, 16], [6, 16, 22], [6, 16, 22, 28]]\ntest_accs = []\nfor out_c in out_channels:\n    avg = 0\n    for _ in range(n_avg):\n        net = TweakableModernLeNet(10, out_channels = out_c)\n        train(net, train_loader, verbose = False)\n        avg += eval_model(net, test_loader)\n\n    test_accs.append(avg / n_avg)\n\nplt.plot([len(i) for i in out_channels], test_accs)\nplt.xlabel('# of convolution layers'); plt.ylabel('Test Accuracy')\n\n\nText(0, 0.5, 'Test Accuracy')\n\n\n\n\n\n\n\n\n\nSame as above (see y-axis).\n\nAdjust the number of fully connected layers.\n\nLets try 1, the original 2, 4, and 6 layers:\n\n\nAdding number of FL layers\nn_avg = 3\nhidden_dims = [[84], [120, 84], [120, 120, 84, 84], [120, 120, 120, 84, 84, 84]]\ntest_accs = []\nfor hid_dims in hidden_dims:\n    avg = 0\n    for _ in range(n_avg):\n        net = TweakableModernLeNet(10, hidden_dims = hid_dims)\n        train(net, train_loader, verbose = False)\n        avg += eval_model(net, test_loader)\n    test_accs.append(avg / n_avg)\n\nplt.plot([len(i) for i in hidden_dims], test_accs)\nplt.xlabel('# of FL layers'); plt.ylabel('Test Accuracy')\n\n\nText(0, 0.5, 'Test Accuracy')\n\n\n\n\n\n\n\n\n\nSame.\n\nAdjust the learning rates and other training details (e.g., initialization and number of epochs).\n\nLets first experiment with different learning rates and try \\(\\{0.01, 0.05, 0.1, 0.5\\}\\):\n\n\nDifferent learning rates\nlrs = [0.01, 0.05, 0.1, 0.5]\ntest_accs = []\nfor lr in lrs:\n    net = TweakableModernLeNet(10)\n    train(net, train_loader, verbose = False, lr = lr)\n    test_accs.append(eval_model(net, test_loader))\n\nplt.plot(lrs, test_accs)\nplt.xlabel('Learning Rate'); plt.ylabel('Test Accuracy')\nplt.xscale('log')\n\n\n\n\n\n\n\n\n\nIt seams only 0.5 is too high and that our original 0.1 performs well."
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html#q3",
    "href": "dl-playground/d2l-exercises/7-6.html#q3",
    "title": "LeNet exercises",
    "section": "Q3",
    "text": "Q3\nTry out the improved network on the original MNIST dataset\nWe can reuse code from above:\n\n\nModernLeNet of MNIST\n_, _, train_loader_MNIST, test_loader_MNIST = get_data(datasets.MNIST, 128)\nnet = ModernLeNet(10)\nlosses = train(net, train_loader, verbose = False, lr = 0.01)\ntest_acc = eval_model(net, test_loader) \nplot_results(losses, test_acc)\n\n\nEpoch: 1    Loss: 0.75\nEpoch: 2    Loss: 0.53\nEpoch: 3    Loss: 0.62\nEpoch: 4    Loss: 0.31\nEpoch: 5    Loss: 0.40\nEpoch: 6    Loss: 0.50\nEpoch: 7    Loss: 0.31\nEpoch: 8    Loss: 0.52\nEpoch: 9    Loss: 0.49\nEpoch: 10   Loss: 0.47\nEpoch: 11   Loss: 0.45\nEpoch: 12   Loss: 0.38\nEpoch: 13   Loss: 0.44\nEpoch: 14   Loss: 0.29\nEpoch: 15   Loss: 0.30\n\n\n\n\n\n\n\n\n\nAnd we get good performance."
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html#q4",
    "href": "dl-playground/d2l-exercises/7-6.html#q4",
    "title": "LeNet exercises",
    "section": "Q4",
    "text": "Q4\nDisplay the activations of the first and second layer of LeNet for different inputs (e.g., sweaters and coats)\nLets retrain the ModernLeNet on FashionMNIST:\n\n\nRetrain ModernLeNet on MNIST\nnet = ModernLeNet(10)\nlosses = train(net, train_loader, verbose = False)\ntest_acc = eval_model(net, test_loader) \nprint('Test acc: ', test_acc)\n\n\nTest acc:  0.8936\n\n\nAnd now get a few sweater and coat images:\n\n\nGet images of coats and sweaters\ntrain_dataset, _, train_loader, _ = get_data(datasets.FashionMNIST, 128)\nimgs, ys = next(iter(train_loader))\n\ncoat_ix = train_dataset.classes.index('Coat')\nsweater_ix = train_dataset.classes.index('Pullover')\n\ncoats_ix = [ix for ix, y in enumerate(ys) if y == coat_ix][:n]\nsweaters_ix = [ix for ix, y in enumerate(ys) if y == sweater_ix][:n]\n\n\nRemmember that the ReLUs are at indeces 1 and 4:\n\n\nCode\nnet\n\n\nModernLeNet(\n  (net): Sequential(\n    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=400, out_features=120, bias=True)\n    (8): ReLU()\n    (9): Linear(in_features=120, out_features=84, bias=True)\n    (10): ReLU()\n    (11): Linear(in_features=84, out_features=10, bias=True)\n  )\n)\n\n\nWe can subscript into Sequential to obtain the activations and each channel along with the input:\n\n\nFunction to display activations\n# Prepare img: add minibatch dim and move to device\ndef disp_acts(end_layer, images = None):\n\n    if images is None: images = [imgs[coats_ix[0]], imgs[sweaters_ix[0]]]\n    \n    for name, og_img in zip(['Coat', 'Sweater'], images):\n    \n        img = og_img[None, :].to(device)  \n        activation = net.net[:end_layer](img)\n        n_channels = activation.shape[1]\n        \n        f, axs = plt.subplots(1, n_channels + 1, figsize = (8,5))\n        axs[0].imshow(og_img.permute(1, 2, 0))\n        axs[0].set_xticks([]); axs[0].set_yticks([]); axs[0].set_title(name)\n        for ax, channel in zip(axs[1:], range(n_channels)):\n            ax.imshow(activation.squeeze().permute(1, 2, 0)[:, :, channel].detach().to('cpu').numpy())\n            ax.set_title(str(channel))\n            ax.set_xticks([])\n            ax.set_yticks([])\n        \n        f.tight_layout()\n        f.show()\n\ndisp_acts(1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndisp_acts(4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd we observe that the first activation layer seems to detect lines, edges, etc. - low-level features, while the second activation layer is slightly more abstract features (although they are hard to interpret)."
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html#q5",
    "href": "dl-playground/d2l-exercises/7-6.html#q5",
    "title": "LeNet exercises",
    "section": "Q5",
    "text": "Q5\nWhat happens to the activations when you feed significantly different images into the network (e.g., cats, cars, or even random noise)?\nLets try inputing random noise:\n\n\nCode\ndisp_acts(1, [torch.randn((1, 28, 28)), torch.randn((1, 28, 28))])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ndisp_acts(4, [torch.randn((1, 28, 28)), torch.randn((1, 28, 28))])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd the activations seem completely random, as we would expect."
  },
  {
    "objectID": "dl-playground/nn-memorization/index.html",
    "href": "dl-playground/nn-memorization/index.html",
    "title": "A Closer Look at Memorization in Deep Networks",
    "section": "",
    "text": "This paper argues that memorization is a behavior exhibited by networks trained on random data, as, in the absence of patterns, they can only rely on remembering examples. The authors investigate this phenomenon and make three key claims:\nHere we aim to reproduce Figures 1, 7, and 8 from the paper."
  },
  {
    "objectID": "dl-playground/nn-memorization/index.html#fig-1",
    "href": "dl-playground/nn-memorization/index.html#fig-1",
    "title": "A Closer Look at Memorization in Deep Networks",
    "section": "Fig 1",
    "text": "Fig 1\nTo support the first claim, the authors argue if networks simply memorize inputs they perform equally when on different training examples. However, if networks learn patterns, there should be points that are easy to learn because they fit these patterns better than others. To see if this is the case they train an MLP for a single epoch starting from 100 different initializations and data shufflings and log the percentage of times an example was correctly classified.\nThe experiment is performed with the CIFAR10 dataset, a noisy input version RandX, and a noisy label version RandY. We first define dataset wrappers to implement the noisy variants. Note that for epoch-to-epoch consistency we determine which examples to corrupt at initialization.\n\n\nRandom dataset wrappers\nclass RandX(Dataset):\n    \"\"\"Injects example noise into dataset by replacing x% of inputs with random gaussian N(0, 1) noise\"\"\"\n    def __init__(self, dataset, x = 1.0):\n        self.dataset = dataset\n        self.x = x\n\n        self.modified = {}\n        for idx, (img, _) in enumerate(self.dataset):\n            if np.random.rand() &lt;= x:\n                self.modified[idx] = torch.randn_like(img)\n        torch.save(self.modified, os.path.join(dataset.root, 'randX_modified'))\n\n    def __len__(self): return len(self.dataset)\n\n    def __getitem__(self, idx):\n        X, y = self.dataset[idx]\n        return self.modified.get(idx, X), y \n\nclass RandY(Dataset):\n    \"\"\"Injects example noise into dataset by replacing y% of labels with random labels\"\"\"\n    def __init__(self, dataset, y = 1.0):\n        self.dataset = dataset\n        self.y = y\n\n        self.modified = {}\n        for idx in range(len(self.dataset)):\n            if np.random.rand() &lt;= y:\n                self.modified[idx] = np.random.randint(0, len(self.dataset.classes))\n        torch.save(self.modified, os.path.join(dataset.root, 'randY_modified'))\n\n    def __len__(self): return len(self.dataset)\n\n    def __getitem__(self, idx):\n        X, y = self.dataset[idx]\n        return X, self.modified.get(idx, y)\n\n\nNow we define a standard training loop, initialization functions, and the MLP specified in the paper.\n\n\nModel initing and training functions\ndef train(model, train, val, optimizer, criterion = nn.CrossEntropyLoss(), epochs = 100, batch_size = 256, save_path = 'models/tmp'):\n    model.to(device)\n    train = DataLoader(train, batch_size = batch_size, shuffle = True)\n    val = DataLoader(val, batch_size = batch_size, shuffle = False)\n    best_loss = np.inf\n    for epoch in range(epochs):\n        model.train()\n        for images, labels in train:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(images), labels)\n            loss.backward()\n            optimizer.step()\n        val_loss, val_acc = eval_model(model, val)\n        if val_loss &lt; best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), save_path)\n        print(f'Epoch {epoch + 1}/{epochs}, Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}')\n\ndef _init_weights(m):\n    if 'Linear' in str(type(m)):\n        nn.init.xavier_uniform_(m.weight)\n        nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n    elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n\ndef initialize_model(model, data_loader, in_device = False):\n    with torch.no_grad():\n        imgs, _ = next(iter(data_loader))\n        if in_device:model.to(device); model(imgs.to(device))\n        _ = model(imgs)\n        model.apply(_init_weights)\n\n\n\n\nMLP definition\nclass MLP(nn.Module):\n    def __init__(self, n_classes = 10):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.LazyLinear(4096),\n            nn.ReLU(),\n            nn.LazyLinear(4096),\n            nn.ReLU(),\n            nn.LazyLinear(n_classes)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\n\nAnd run the experiment, training models for a single epoch as in the paper but also after 10 epochs to investigate how results vary.\n\n\nGet estimated P(correct)\ndef add_missclassified(missclassified, model, test_set, batch_size = 256):\n    model = model.to(device); model.eval()\n    test = DataLoader(test_set, batch_size = batch_size, shuffle = False)\n    i = 0\n    with torch.no_grad():\n        for images, labels in test:\n            images, labels = images.to(device), labels.to(device)\n            _, pred = torch.max(model(images), 1)\n            missclassified[i:i + test.batch_size] += (pred != labels).float()\n            i += test.batch_size\n\ndef gen_fig_1(epochs = 1, n_inits = 100):\n    training_sets = [train_set, RandX(train_set, x = 1.0), RandY(train_set, y = 1.0)]\n    for training_set in training_sets:\n        missclassified = torch.zeros(len(test_set)).to(device)\n        for _ in range(n_inits):\n            m = MLP()\n            initialize_model(m, DataLoader(train_set, batch_size = 256))\n            train(m, training_set, test_set, optim.SGD(m.parameters(), lr = 0.01), epochs = epochs)\n            add_missclassified(missclassified, m, test_set)\n        missclassified /= n_inits\n        torch.save(missclassified, f'logs/missclassified_epochs={epochs}_' + training_set.__class__.__name__)\n\ngen_fig_1(epochs = 1, n_inits = 100)\ngen_fig_1(epochs = 10, n_inits = 100)\n\n\n\n\nPlot results\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (9, 4), sharey = True)\n\nfor ax, epochs in zip([ax1, ax2], [1, 10]):\n\n    for fname in sorted([f for f in os.listdir('logs') if 'missclassified' in f and f'epochs={epochs}_' in f]):\n        missclassified = torch.load(os.path.join('logs', fname))\n        p = (1 - missclassified).sort().values.to('cpu').numpy()\n        ax.plot(p, label = fname.split('_')[-1])\n    \n    # Plot binomially sampled points\n    randX_mean = 1 - torch.load(os.path.join('logs', 'missclassified_RandX')).mean().item()\n    bin_data = np.random.binomial(n = 100, p = randX_mean, size = 10000) / 100\n    bin_data.sort()\n    ax.plot(bin_data, label = 'Binomial_X')\n    ax.set_title(f'Epoch = {epochs}')\n\nax1.legend()\nf.supylabel('P(correct)')\nf.supxlabel('Example(sorted by P(correct))')\nf.tight_layout()\n\n\n\n\n\n\n\n\n\nObserve that the left is a figure very similar to the paper’s. Whereas real data has easy patterns that can be learned in a single epoch, random data does not and networks must resort to memorization. After 10 epochs we observe that the networks trained on random data manage to improve the performance on a few points at the expense of the rest, whose performance becomes worse than random.\nOut of curiosity here are the 10 easiest and hardest examples.\n\n\nPlot hardest and easiest examples\nn = 10\nf, ax = plt.subplots(2, n, figsize = (n - 2, 2))\nfor i, idx in enumerate(torch.sort(p).indices[:n]):\n    img, label = test_set[idx]\n    ax[0][i].imshow(img.permute(1, 2, 0).numpy())\n    ax[0][i].axis('off')\n    ax[0][i].set_title(test_set.classes[label].replace('mobile', ''))\n\nfor i, idx in enumerate(torch.sort(p).indices[-n:]):\n    img, label = test_set[idx]\n    ax[1][i].imshow(img.permute(1, 2, 0).numpy())\n    ax[1][i].axis('off')\n    ax[1][i].set_title(test_set.classes[label].replace('mobile', ''))\n\nf.suptitle('Hardest (top) and easiest (bottom) examples')\nf.tight_layout()"
  },
  {
    "objectID": "dl-playground/nn-memorization/index.html#fig-2",
    "href": "dl-playground/nn-memorization/index.html#fig-2",
    "title": "A Closer Look at Memorization in Deep Networks",
    "section": "Fig 2",
    "text": "Fig 2\nThe fact that networks learn patterns when trained on real data and don’t when trained on noise can also be visualized by plotting the first layer weights of a convolutional network. We show the weights for networks trained for 10 epochs on real and random data.\n\n\nConvNet definition\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes):\n        super(ConvNet, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.LazyConv2d(out_channels=200, kernel_size=5),  # LazyConv2d to infer input channels\n            nn.BatchNorm2d(200),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=3),\n\n            nn.LazyConv2d(out_channels=200, kernel_size=5),  # Another LazyConv2d\n            nn.BatchNorm2d(200),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=3),\n\n            nn.Flatten(),  # Flatten for the fully connected layer\n            nn.LazyLinear(out_features=384),  # LazyLinear to infer input features\n            nn.BatchNorm1d(384),\n            nn.ReLU(),\n\n            nn.LazyLinear(out_features=192),  # Another LazyLinear\n            nn.BatchNorm1d(192),\n            nn.ReLU(),\n\n            nn.LazyLinear(out_features=num_classes)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n    \ndef train_conv(model, train, val, optimizer, criterion = nn.CrossEntropyLoss(), epochs = 100, batch_size = 256):\n    model.to(device)\n    train = DataLoader(train, batch_size = batch_size, shuffle = True)\n    val = DataLoader(val, batch_size = batch_size, shuffle = False)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 15, gamma = 0.5)\n    for epoch in range(epochs):\n        model.train()\n        for images, labels in train:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(images), labels)\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n        val_loss, val_acc = eval_model(model, val)\n        print(f'Epoch {epoch + 1}/{epochs}, Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}')\n\n\n\n\nTrain models\ncifar10_model = ConvNet(num_classes = n_classes)\nrandY_model = ConvNet(num_classes = n_classes)\n\nfor model, dataset in zip([cifar10_model, randY_model], [train_set, RandY(train_set)]):\n    initialize_model(model, DataLoader(dataset, batch_size = 256))\n    train_conv(model, dataset, test_set, optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9), epochs = 10)\n    torch.save(model.state_dict(), f'models/fig2_{model.__class__.__name__}')\n\n\n\n\nPlot filters\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (9, 4), sharey = True)\n\nfor name, model, ax in zip(['Cifar 10', 'RandY'], [cifar10_model, randY_model], [ax1, ax2]):\n\n    model.eval()\n    kernel_weights = model.model[0].weight[:7 * 12].detach().cpu().clone()\n    kernel_weights = (kernel_weights - kernel_weights.min()) / (kernel_weights.max() - kernel_weights.min())\n    filter_img = utils.make_grid(kernel_weights, nrow = 12, padding = 1)\n    ax.imshow(filter_img.permute(1, 2, 0))\n    ax.set_title(name)\n    ax.axis('off')\n\n\n\n\n\n\n\n\n\nAnd we are able to see that the filters learned by the network trained on real data are much more structured and seem useful in contrast to the ones learned by training on noise."
  },
  {
    "objectID": "dl-playground/nn-memorization/index.html#fig-9",
    "href": "dl-playground/nn-memorization/index.html#fig-9",
    "title": "A Closer Look at Memorization in Deep Networks",
    "section": "Fig 9",
    "text": "Fig 9\nTo attempt to show that networks trained on real data are simpler hypotheses because they learn patterns, the authors introduce Critical Sample Ratio as a way to measure complexity. The idea is to\n\n“estimate the complexity by measuring how densely points on the data manifold are present around the model’s decision boundaries. Intuitively, if we were to randomly sample points from the data distribution, a smaller fraction of points in the proximity of a decision boundary suggests that the learned hypothesis is simpler.”\n\nA simple sketch illustrates:\n\n\n\nCSR intuition sketch\n\n\nTo estimate the density of points close to decision boundaries we might perturb the original data points within a box of size \\(r\\) and see if we cross the boundary. If a point crosses a boundary we call it “critical”. The Critical Sample Ratio is then the proportion of points that are critical and we expect simpler networks to have lower CSRs.\nThe perturbation done to data points is not totally random. The technique used by the paper is presented in Algorithm 1, borrows ideas from adversarial attacks, and is called Langevin Adversarial Sample Search (LASS). Here is how I implemented it.\n\n\nLASS implementation\ndef standard_normal(shape):\n    r = torch.randn(shape)\n    r = r.to(device)\n    return r\n\ndef lass(model, x, alpha = 0.25 / 255, beta = 0.2 / 255, r = 0.3 / 255, eta = standard_normal, max_iter = 10):\n    \"\"\"\n    Langevin Adversarial Sample Search (LASS).\n    Finds a perturbation of x that changes the model's prediction.\n    \n        labels: Tensor of true labels corresponding to the input x.\n        alpha: Step size for the gradient sign method.\n        beta: Scaling factor for the noise.\n        r: Clipping radius for adversarial perturbations.\n        eta: Noise process.\n    \"\"\"\n    # Orignal prediction\n    with torch.no_grad():\n        pred_on_x = model(x).argmax(dim=1)\n    \n\n    x_adv = x.clone().detach().requires_grad_(True)\n    converged = False\n    iter_count = 0\n\n    while not converged and iter_count &lt; max_iter:\n        iter_count += 1\n\n        # Forward pass to get model output\n        x_adv.requires_grad_(True)\n        output = model(x_adv)\n\n        # Compute gradient of the output with respect to input\n        loss = F.cross_entropy(output, pred_on_x)  # Use actual labels\n        loss.backward()\n\n        # Compute the perturbation\n        gradient_sign = x_adv.grad.sign()\n        delta = alpha * gradient_sign + beta * eta(x_adv.shape)\n\n        with torch.no_grad():\n            \n            x_adv += delta\n\n            # Apply the clipping to each dimension so that each pixel is in the range [x - r, x + r]\n            x_adv = torch.clamp(x_adv, x - r, x + r)\n\n            # Check if the adversarial example has changed the model's prediction\n            new_output = model(x_adv)\n            if not torch.equal(output.argmax(dim=1), new_output.argmax(dim=1)):\n                converged = True\n                x_hat = x_adv.clone().detach()\n\n        # Zero the gradients for the next iteration\n        model.zero_grad()\n        if x_adv.grad is not None: x_adv.grad.zero_()\n\n    return converged, x_hat if converged else None\n\ndef compute_csr(model, test_set, n_examples = None, shuffle = False, **lass_kwargs):\n    if n_examples is None: n_examples = len(test_set)\n    model = model.to(device)\n    csr = 0\n    for i, (images, labels) in enumerate(DataLoader(test_set, batch_size = 1, shuffle = shuffle)):\n        if i == n_examples: break\n        images, labels = images.to(device), labels.to(device)\n        converged, _ = lass(model, images, **lass_kwargs)\n        if converged: csr += 1\n    return csr / n_examples\n\n\nThe paper sets the radius we search for adversarial examples to \\(r = 30/255\\) because it was small enough to not be noticed by a human evaluator. Here is an example.\n\n\nAdversarial example\nmodel = cifar10_model\nmodel.eval(); model.to(device)\nset_seed(5)\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (9, 4), sharey = True)\nfor i, (x, y) in enumerate(DataLoader(test_set, batch_size = 1, shuffle = True)):\n    x = x.to(device)\n    y = y.to(device)\n    with torch.no_grad():\n        if model(x).argmax() != y: continue\n    converged, adv = lass(model, x)\n    if converged:\n        ax1.imshow(x.squeeze().cpu().permute(1, 2, 0))\n        ax2.imshow(adv.squeeze().cpu().permute(1, 2, 0))\n        ax1.axis('off'); ax2.axis('off')\n        with torch.no_grad():\n            ax1.set_title(f'Original. Predicted: {test_set.classes[model(x).argmax().item()]}')\n            ax2.set_title(f'Adversarial. Predicted: {test_set.classes[model(adv).argmax().item()]}')\n        break\n\n\n\n\n\n\n\n\n\nAnd now try to compute the Critical Sample Ratio as we train models to reproduce Figure 9.\n\n\nConvNet training loop\ndef train_fig9(model, train, val, optimizer, criterion = nn.CrossEntropyLoss(), epochs = 100, batch_size = 256):\n    val_accs, csrs = [], []\n    model.to(device)\n    train = DataLoader(train, batch_size = batch_size, shuffle = True)\n    val = DataLoader(val, batch_size = batch_size, shuffle = False)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 15, gamma = 0.5)\n    for epoch in range(epochs):\n        model.train()\n        for images, labels in train:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(images), labels)\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n        val_loss, val_acc = eval_model(model, val)\n        val_accs.append(val_acc)\n        print(f'Epoch {epoch + 1}/{epochs}, Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}')\n\n        if epoch % 10 == 0:\n            csr = compute_csr(model, val.dataset, n_examples = 500, r = 40 / 255)\n            csrs.append(csr)\n            print(f'CSR: {csr:.4f}')\n    \n    return val_accs, csrs\n\nfor dataset in [train_set, RandX(train_set, x = 1.0), RandY(train_set, y = 1.0)]:\n    set_seed(seed  = 42)\n    print(dataset.__class__.__name__)\n    model = ConvNet(num_classes = n_classes)\n    initialize_model(model, DataLoader(dataset, batch_size = 256))\n    val_accs, csrs = train_fig9(model, dataset, test_set, optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9), epochs = 141)\n    torch.save({'val_accs': val_accs, 'csrs': csrs}, f'logs/fig9_r=45_{dataset.__class__.__name__}')\n\n\n\n\nPlot results\nplt.plot(np.arange(0, 141, 10), torch.load('logs/fig9_CIFAR10')['csrs'], 'b', label = 'CIFAR10')\nplt.plot(np.arange(0, 141, 10), [0] * 15, 'r--', label = 'RandX (?)')\nplt.plot(np.arange(0, 141, 10), torch.load('logs/fig9_RandY')['csrs'], 'g', label = 'RandY')\n\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Critical Sample Ratio (CSR)')\n\n\nText(0, 0.5, 'Critical Sample Ratio (CSR)')\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal Fig 9\n\n\nWhere we observe roughly the same trend as in the paper displayed above while the network trained on real data has a somewhat constant CSR, the one trained on random labels has a higher CSR as training progresses. However, I could reproduce RandX’s behavior and obtained a constant CSR of 0. I tried different seeds, \\(r\\), and datasets (training and validation) without luck. My suspicion is that the model’s capacity and thus performance were not high enough (around 10% validation accuracy). I decided to stick with the paper’s architecture and move on."
  },
  {
    "objectID": "dl-playground/nn-memorization/index.html#what-i-learned-practiced",
    "href": "dl-playground/nn-memorization/index.html#what-i-learned-practiced",
    "title": "A Closer Look at Memorization in Deep Networks",
    "section": "What I learned / practiced",
    "text": "What I learned / practiced\n\nHow to visualize 1st layer kernel weights\nA bit about adversarial attacks\nA creative proxy for model complexity (CSR)"
  },
  {
    "objectID": "dl-playground/dl-massive-label-noise/index.html",
    "href": "dl-playground/dl-massive-label-noise/index.html",
    "title": "Deep Learning is Robust to Massive Label Noise",
    "section": "",
    "text": "The paper shows that neural networks can keep generalizing when large numbers of (non-adversarially) incorrectly labeled examples are added to datasets (MNIST, CIFAR, and ImageNet). It also appears that larger networks are more robust and that higher noise levels lead to lower optimal (fixed) learning rates.\nWe’ll focus on the uniform label noise experiment and attempt to reproduce Figure 1:\n\n\n\nFigure 1. As we increase the amount of noise in the dataset the performance drops. However, note that even when there are 100 noisy labels per clean label performance is still acceptable. For example, the Convnet still achieves 91% accuracy.\n\n\nNote: As far as I can tell the paper has no accompanying code so I’ll be filling in the details to the best of my abilities.\n\n\nImports and model evaluation function\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os, itertools, time\n\nos.makedirs('logs', exist_ok = True)\nos.makedirs('models', exist_ok = True)\n\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\ndevice = torch.device(\n    'cuda' if torch.cuda.is_available() else\n    ('mps' if torch.backends.mps.is_available() else\n    'cpu')\n)\ndevice = 'cpu' # faster for the small models we are using\n\ndef eval_model(model, test, criterion = nn.CrossEntropyLoss()):\n    model.eval()\n    correct, loss = 0, 0.0\n    with torch.no_grad():\n        for images, labels in test:\n            images, labels = images.to(device), labels.to(device)\n            _, pred = torch.max(model(images), 1)\n            correct += (pred == labels).float().sum().item()\n            loss += criterion(model(images), labels).item()\n    return loss / len(test.dataset), correct / len(test.dataset)\n\n\nTo generate the uniform label noise the paper augments the original dataset with an additional \\(\\alpha\\) \\((X_i, Y')\\) pairs, where \\(Y'\\) is a class sampled uniformly at random with replacement.\nTo minimize disk use I opted for a custom dataset that wraps the original. Pytorch only requires we override __len__ and __getitem__. The length is simply the original size plus the amount of noisy labels. When queried for data we’ll generate the noisy labels for the original pairs immediately after it. For example when \\(\\alpha = 2\\):\n\\[\n(X_1, Y_1), (X_1, Y'), (X_1, Y'), (X_2, Y_2), ...\n\\]\nNote that to guarantee that noisy labels are consistent between epocs, i.e. data[1] returns the same class when called again, we can’t sample the labels at query time. To avoid storing all the randomly sampled labels (\\(60, 000 \\times 100\\) in the worst case), we simply return a shifted index’s label. We can do this with MNIST because its reasonably class-balanced and shuffled.\n\nclass NoisyLabelDataset(Dataset):\n    \"\"\"Adds alpha noisy labels per original example\"\"\"\n    \n    def __init__(self, dataset, alpha):\n        self.dataset = dataset\n        self.alpha = alpha\n        self.shift = np.random.randint(0, len(dataset))\n\n    def __len__(self):\n        n = len(self.dataset)\n        return n + (self.alpha * n)\n    \n    def __getitem__(self, idx):\n        x, y = self.dataset[idx // (self.alpha + 1)]\n        if idx % (self.alpha + 1) != 0:\n            y = self.dataset[(idx + self.shift) % len(self.dataset)][1]\n        return x, y\n\nAlthough the paper appears to only include a test set, we also include a validation set to perform early stopping with.\n\n\nDatasets and loaders\nbatch_size = 128\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = datasets.MNIST('data', download = True, train = True,   transform = transform)\ntest_dataset =  datasets.MNIST('data', download = True, train = False,  transform = transform)\n\nnoisy_train_dataset = NoisyLabelDataset(train_dataset, alpha = 5)\nval_dataset, test_dataset = torch.utils.data.random_split(test_dataset, (0.2, 0.8), generator = torch.Generator().manual_seed(seed))\n\ntrain_loader = DataLoader(noisy_train_dataset, batch_size = batch_size, shuffle = True)\nval_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n\n\nOur training loop is pretty standard. We use the Adadelta optimizer as per the paper. Although the paper does not mention it, we assume they used early stopping: stop training when the validation accuracy does not increase after patience epochs and return the model with the highest validation accuracy.\n\n\nTraining function\ndef train(model, train_loader, val_loader, lr = 0.01, patience = 3, max_epochs = 100, verbose = False):\n    \n    model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adadelta(model.parameters(), lr = lr)\n\n    log = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n\n    best_val_acc = float('inf')\n    best_model = None\n\n    for epoch in range(max_epochs):\n\n        model.train()\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(images), labels)\n            loss.backward()\n            optimizer.step()\n\n        val_loss, val_acc = eval_model(model, val_loader)\n        log['train_loss'].append(loss.item())\n        log['val_loss'].append(val_loss)\n        log['val_acc'].append(val_acc)\n\n        if verbose: print(', '.join([f'Epoch {epoch + 1}'] + [f'{k}: {v[-1]:.4f}' for k, v in log.items()]))\n\n        if val_acc &lt; best_val_acc:\n            best_val_acc = val_acc\n            best_model = model.state_dict()\n\n        # Early stopping: stop if val acc has not increased in the last `patience` epochs\n        if epoch &gt; patience and val_acc &lt;= max(log['val_acc'][-patience-1:-1]): break \n    \n    if best_model: model.load_state_dict(best_model)\n    return model, log\n\n\nWe train with learning rates \\(\\{0.01, 0.05, 0.1, 0.5\\}\\) as per the paper and \\(\\alpha \\in \\{0, 25, 50\\}\\) to save some compute (we should get the idea). Below we define our perceptron, MLPs with 1, 2, and 4 layers and a 4-layer Convnet. Again, since the paper does not specify hidden dims, activations, or the convnet architecture, we set it ourselves.\n\n\nHyperparam and model definitions\nlearning_rates = [0.01, 0.05, 0.1, 0.5]\nalphas = range(0, 75, 25)\n\nlin_relu = lambda n_in, n_out: nn.Sequential(nn.Linear(n_in, n_out), nn.ReLU())\nmodels = {\n    'perceptron':nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10)),\n    'MLP1':nn.Sequential(nn.Flatten(), lin_relu(28 * 28, 256), nn.Linear(256, 10)),\n    'MLP2':nn.Sequential(nn.Flatten(), lin_relu(28 * 28, 256), lin_relu(256, 128), nn.Linear(128, 10)),\n    'MLP4':nn.Sequential(nn.Flatten(), lin_relu(28 * 28, 256), lin_relu(256, 128), lin_relu(128, 64), nn.Linear(64, 10)),\n    'Conv4':nn.Sequential(\n        nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n        \n        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n        \n        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n        \n        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n        \n        nn.Flatten(),\n        nn.Linear(256 * 1 * 1, 1000),\n        nn.ReLU(),\n        nn.Linear(1000, 10)\n    )\n}\n\n\n\n\nTrain and save models\nfor alpha, (name, model), lr in itertools.product(alphas, models.items(), learning_rates):\n\n    noisy_train_dataset = NoisyLabelDataset(train_dataset, alpha = alpha)\n    train_loader = DataLoader(noisy_train_dataset, batch_size = batch_size, shuffle = True)\n    \n    start = time.time()\n    model, log = train(model, train_loader, val_loader, lr = lr, verbose = True)\n    test_loss, test_acc = eval_model(model, test_loader)\n    log['test_loss'] = test_loss\n    log['test_acc'] = test_acc\n\n    print(f'{name} - alpha: {alpha}, lr: {lr}, test acc: {test_acc:.4f}, took: {time.time() - start:.2f}s')\n    torch.save(log, f'logs/{name}_{alpha}_{lr}.pt')\n    torch.save(model, f'models/{name}_{alpha}_{lr}.pt')\n\n\nFinally, we plot the accuracies on both the validation and test sets:\n\n\nPlot results\n# Load results into dataframe\ndf = []\nfor fname in os.listdir('logs'):\n    name, alpha, lr = fname.split('_')\n    lr = float(lr.replace('.pt', ''))\n    alpha = int(alpha)\n    \n    log = torch.load('logs/' + fname)\n    tmp = {}\n    tmp['Model'] = name\n    tmp['Alpha'] = alpha\n    tmp['lr'] = lr\n    tmp['Prediction Accuracy'] = log['test_acc']\n    tmp['Validation Accuracy'] = max(log['val_acc'])\n    df.append(tmp)\ndf = pd.DataFrame(df)\n\nhue_order = ['perceptron', 'MLP1', 'MLP2', 'MLP4', 'Conv4']\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 4), sharey = True, sharex = True)\n\nsns.lineplot(\n    df.groupby(['Model', 'Alpha']).max(), x = 'Alpha', y = 'Prediction Accuracy',\n    hue = 'Model', hue_order = hue_order, ax = ax1\n)\nax1.set_title('On Test Set')\nax1.grid()\n\nsns.lineplot(\n    df.groupby(['Model', 'Alpha']).max(), x = 'Alpha', y = 'Validation Accuracy',\n    hue = 'Model', hue_order = hue_order, ax = ax2, legend = False\n)\nax2.set_title('On Validation Set')\nax2.grid()\nplt.tight_layout()\n\n\n\n\n\n\n\n\n\nWe observe that the general trends seem to hold. As we add noise the performance drops and larger models tend to be more robust.\nHowever, our models overall tend to perform worse than the paper’s. At \\(\\alpha = 50\\) most of our models have accuracies below \\(60\\%\\), whereas the paper’s are around the high \\(80\\%\\)’s. In addition, our Conv4 model is already below 90% when the paper archives 91% at \\(\\alpha = 100\\).\nThis might be due to differences in training (use and implementation of early stopping), architecture implementation, random seeds (we did not try multiple / averaging because of compute), etc.\nWe also observe the trend the paper points out (in Section 5) that higher noise levels lead to smaller effective batch sizes and thus lower optimal learning rates:\n\n\n\n\n\n\n\n\n\nA few closing thoughts: Although our results did not completely align with the paper, we still find the robustness to noise impressive.\nThe paper performs and studies the effect much further under non-uniform noise, with different datasets, batch sizes, etc. It is worth a read.\nI might revisit this notebook to perform further experiments and try to answer some lingering questions:\n\nDoes within epoch sample ordering matter? Intuitively, if we place all clean labels before the noisy ones, one expects worse performance (catastrophic forgetting?)\nWhat effect does early stopping have? We used a clean validation set to determine when to stop – which is not realistic. What happens if we use the loss or no early stopping?"
  },
  {
    "objectID": "dl-playground/d2l-exercises/9-5.html",
    "href": "dl-playground/d2l-exercises/9-5.html",
    "title": "RNNs from scratch exercises",
    "section": "",
    "text": "Where I build up intuition for RNNs and attempt to solve the exercises in section 9.5 of the d2l book from scratch in pytorch (without using the d2l library).\nFirst, a little context and intermediate implementations that helped me understand the one the book and Pytorch follow. For better explainers see [1],[2],[3].\n\nContext\nRNNs are layers that maintain a state \\(\\mathbf{H}_t\\) and update it every time you a forward pass. The new value at “time” \\(t\\) depends on the current input \\(\\mathbf{X}_t\\) and the previous value \\(\\mathbf{H}_{t-1}\\) according to\n\\[\n\\mathbf{H}_t = \\phi(\\mathbf{X}_t \\mathbf{W}_{\\textrm{xh}} + \\mathbf{H}_{t-1} \\mathbf{W}_{\\textrm{hh}}  + \\mathbf{b}_\\textrm{h})\n\\]\nAfter updating its state, the layer can then produce an output.\n\\[\n\\mathbf{O}_t = \\mathbf{H}_t \\mathbf{W}_{\\textrm{hq}} + \\mathbf{b}_\\textrm{q}\n\\]\nWhat about the dimensions? There are two choices we get to make. The first is the dimension \\(d\\) of the vectors we represent our words/characters/tokens with that will dictate the shape of our input \\(\\mathbf{X}_t \\in \\mathbb{R}^{n \\times d}\\), where \\(n\\) is the batch size. The second is the hidden dimension \\(h\\) we wish to transform our tokens to, which will dictate the shape of the hidden state as \\(\\mathbf{H}_t \\in \\mathbb{R}^{n \\times h}\\).\nWe can visualize the layer by displaying how it behaves across forward passes \\(t-1\\), \\(t\\), \\(t+1\\) by “unrolling time” horizontally.\n\n\n\nFig. 9.4.1 From the book.\n\n\nWe first sort out the data we will feed the RNN, mainly following sections 9.2 and 9.3 implemented from scratch.\n\n\nGet and preprocess data\n# url = 'http://d2l-data.s3-accelerate.amazonaws.com/' + 'timemachine.txt'\n# with open('data/timemachine.txt', 'w') as f:\n#     f.write(requests.get(url).text)\n\nwith open('data/timemachine.txt', 'r') as f:\n    text = f.read()\n\ntext = re.sub('[^A-Za-z]+', ' ', text).lower() # ignore punctuation: 'the time machine by h g wells i the time traveller ...'\ntokens = list(text) # character-level tokens: ['t', 'h', 'e', ' ', 't', 'i', 'm', 'e', ...]\nvocab = set(tokens) # unique characters: {' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', ...}\nchar_to_ix = {c:i for i, c in enumerate(vocab)} # character to index: {' ': 0, 'a': 1, 'b': 2, 'c': 3, ...}\nix_to_char = {i:c for i, c in enumerate(vocab)} # index to character: {0: ' ', 1: 'a', 2: 'b', 3: 'c', ...}\n\n\n# Just so we can use torch's DataLoaders\nclass OffsetSequences(Dataset):\n    \n    def __init__(self, tokens, seq_len): self.tokens = tokens; self.seq_len = seq_len\n    def __len__(self): return len(self.tokens) - self.seq_len\n\n    def __getitem__(self, idx):\n        if idx &gt;= len(self): raise IndexError\n        t = lambda l: torch.Tensor(l).long()\n        return t(self.tokens[idx:idx + self.seq_len]), t(self.tokens[idx + 1:idx + 1 + self.seq_len])\n\n# Hyperparams\ndef get_data(batch_size, num_steps, train_prop = 0.7):\n    train_cutoff = int(train_prop * len(tokens))\n    train_dataset = OffsetSequences([char_to_ix[t] for t in tokens[:train_cutoff]], num_steps)\n    test_dataset  = OffsetSequences([char_to_ix[t] for t in tokens[train_cutoff:]], num_steps)\n    train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, drop_last = True)\n    test_loader   = DataLoader(test_dataset,  batch_size = batch_size, shuffle = False, drop_last = True)\n    return train_dataset, test_dataset, train_loader, test_loader\n\nbatch_size, num_steps = 1024, 16\ntrain_dataset, test_dataset, train_loader, test_loader = get_data(batch_size, num_steps)\ntrain_dataset_demo, test_dataset_demo, train_loader_demo, test_loader_demo = get_data(2, 3)\n\n\nWe can now obtain sequences of our text data offset by one to feed as input and targets. Let’s display the first few tokenized characters of the text and the first (input, target) pair when we set the sequence length num_steps to 3:\n\nX, Y = train_dataset_demo[0]\n[char_to_ix[i] for i in tokens[:5]], X, Y\n\n([13, 12, 8, 25, 13], tensor([13, 12,  8]), tensor([12,  8, 25]))\n\n\nAnd can use data loaders to chunk several such pairs. For example, if we set the batch_size=2:\n\nX, Y = next(iter(DataLoader(train_dataset_demo, batch_size = 2, shuffle = False)))\nprint(f'X: {X}\\nY: {Y}\\n\\nBoth shaped as: {X.shape}')\n\nX: tensor([[13, 12,  8],\n        [12,  8, 25]])\nY: tensor([[12,  8, 25],\n        [ 8, 25, 13]])\n\nBoth shaped as: torch.Size([2, 3])\n\n\nIn practice, we’ll have much larger sequence lengths, and batch sizes and will set shuffle=True to grab random sequences from the corpus. Finally, we need to one-hot encode the tokens (Sec. 9.5.2.1) and end up with the shape (sequence length, batch size, vocab size):\n\nX = F.one_hot(X.T, len(vocab)).type(torch.float32)\nX.shape # seq_length, batch_size, vocab_size\n\ntorch.Size([3, 2, 27])\n\n\nSince we are now ready to start feeding in data let’s start with a literal implementation of the above description of the RNN layer:\n\n\nA literal implementation\nclass RNNScratch(nn.Module):\n    \"\"\"The RNN model implemented from scratch.\"\"\"\n    def __init__(self, num_inputs, num_hiddens, num_outputs, sigma = 0.01):\n        super().__init__()\n        self.num_inputs = num_inputs; self.num_hiddens = num_hiddens; self.num_outputs = num_outputs\n\n        self.W_xh = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)\n        self.W_hh = nn.Parameter(torch.randn(num_hiddens, num_hiddens) * sigma)\n        self.b_h = nn.Parameter(torch.zeros(num_hiddens))\n\n        self.W_hq = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)\n        self.b_q = nn.Parameter(torch.zeros(num_outputs))\n    \n    def forward(self, X, H = None):\n        # X: (batch_size, vocab_size)\n        if H is None: H = torch.zeros(X.shape[0], self.num_hiddens)\n        H = torch.tanh(torch.matmul(X, self.W_xh) + torch.matmul(H, self.W_hh) + self.b_h)\n        O = torch.matmul(H, self.W_hq) + self.b_q\n        return O, H\n\n\nRemember we feed RNNs tokens sequentially. Let’s first feed in the first tokens and inspect the output shapes.\n\nm = RNNScratch(num_inputs = len(vocab), num_hiddens = 64, num_outputs = len(vocab))\noutput, hidden_state = m(X[0])\nX[0].shape, output.shape, hidden_state.shape\n\n(torch.Size([2, 27]), torch.Size([2, 27]), torch.Size([2, 64]))\n\n\nWhich makes sense since the layer must output, for each example in the batch, (the logits for) which of the 27 characters is more likely to follow the input. We also have the layer output the updated hidden state H to pass in the next forward pass(es):\n\noutput, hidden_state = m(X[1], hidden_state)\noutput, hidden_state = m(X[2], hidden_state)\n\nSince we’ve now reached the end of the sequence we can compare the predicted and actual last word to compute a loss:\n\nF.cross_entropy(output, Y.T[-1])\n\ntensor(3.2961, grad_fn=&lt;NllLossBackward0&gt;)\n\n\nWith this simple setup, we can already train a simple language model:\n\n\nTrain small lm\n# train on train_loader, not train_loader_demo\nm = RNNScratch(num_inputs = len(vocab), num_hiddens = 32, num_outputs = len(vocab))\nopt = optim.SGD(m.parameters(), lr = 1)\nlosses = []\n\nfor X, Y in train_loader:\n\n    X = F.one_hot(X.T, len(vocab)).type(torch.float32)\n\n    opt.zero_grad()\n\n    hidden_state = None\n    for i in range(num_steps):\n        output, hidden_state = m(X[i], hidden_state)\n\n    loss = F.cross_entropy(output, Y.T[-1])\n    loss.backward()\n    opt.step()\n    \n    losses.append(np.exp(loss.item()))\n\nplt.plot(losses); plt.ylabel('perplexity'); plt.xlabel('batch'); plt.title(f'Final perplexity: {losses[-1]:.2f}')\n\n\nText(0.5, 1.0, 'Final perplexity: 12.89')\n\n\n\n\n\n\n\n\n\nNow instead of only teaching the network to predict the last character of the sequence using all the previous ones \\((x_t | x_{t-1}, ~\\dots~x_1)\\) we could “increase de signal” in our loss by evaluating all intermediate predictions \\((x_2 | x_1)\\), \\((x_3|x_2, x_1)\\), …, \\((x_t | x_{t-1}, ~\\dots~x_1)\\). We can do so by accumulating the intermediate logits in a list, stacking them, and then making sure our shapes make sense before passing to cross_entropy. Here we assume all intermediate predictions are equally weighted in the loss, but you could play around with this.\n\n\nTrain with intermediate predictions\n# train on train_loader, not train_loader_demo\nm = RNNScratch(num_inputs = len(vocab), num_hiddens = 32, num_outputs = len(vocab))\nopt = optim.SGD(m.parameters(), lr = 1)\nlosses = []\n\nfor X, Y in train_loader:\n\n    X = F.one_hot(X.T, len(vocab)).type(torch.float32)\n\n    opt.zero_grad()\n\n    outputs = [] # accumulate intermediate predictions\n    hidden_state = None\n    for i in range(num_steps):\n        output, hidden_state = m(X[i], hidden_state)\n        outputs.append(output)\n\n    outputs = torch.stack(outputs) # (seq_len, batch_size, vocab_size)\n    outputs = outputs.view(-1, len(vocab)) # (seq_len * batch_size, vocab_size)\n    targets = Y.view(-1) # (batch_size, seq_len) -&gt; (seq_len * batch_size)\n\n    loss = F.cross_entropy(outputs, targets)\n    loss.backward()\n    opt.step()\n    \n    losses.append(np.exp(loss.item()))\n\nplt.plot(losses); plt.ylabel('perplexity'); plt.xlabel('batch'); plt.title(f'Final perplexity: {losses[-1]:.2f}')\n\n\nText(0.5, 1.0, 'Final perplexity: 17.08')\n\n\n\n\n\n\n\n\n\nWhy do we get a higher loss? Well, asking the RNN to make correct intermediate predictions is a harder task than just predicting the last character. Thus we probably require more training, capacity, and tuning.\nAnyway, our training loop is getting pretty messy. To match the book’s implementation and tidy up our loop we can first let the RNN loop though the sequence itself:\n\n\nCode\nclass RNNScratch(nn.Module):\n    \"\"\"The RNN model implemented from scratch.\"\"\"\n    def __init__(self, num_inputs, num_hiddens, num_outputs, sigma = 0.01):\n        super().__init__()\n        self.num_inputs = num_inputs; self.num_hiddens = num_hiddens; self.num_outputs = num_outputs\n\n        self.W_xh = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)\n        self.W_hh = nn.Parameter(torch.randn(num_hiddens, num_hiddens) * sigma)\n        self.b_h = nn.Parameter(torch.zeros(num_hiddens))\n\n        self.W_hq = nn.Parameter(torch.randn(num_hiddens, num_outputs) * sigma)\n        self.b_q = nn.Parameter(torch.zeros(num_outputs))\n    \n    def forward(self, X, H = None):\n        # Expects X: (seq_length, batch_size, num_inputs i.e. vocab_size)\n        # Outputs X: (seq_len, batch_size, num_outputs i.e. vocab_size)\n        if H is None: H = torch.zeros(X.shape[1], self.num_hiddens) # (batch_size, num_hiddens)\n        outputs = []\n        for X_i in X: # loop over first dim\n            H = torch.tanh(torch.matmul(X_i, self.W_xh) + torch.matmul(H, self.W_hh) + self.b_h)\n            O = torch.matmul(H, self.W_hq) + self.b_q\n            outputs.append(O)\n        return torch.stack(outputs), H\n\n\nAnd make sure the output shapes still make sense:\n\n\nCode\nm = RNNScratch(num_inputs = len(vocab), num_hiddens = 32, num_outputs = len(vocab))\nX, Y = next(iter(train_loader_demo))\nX = F.one_hot(X.T, len(vocab)).type(torch.float32)\noutputs, H = m(X)\noutputs.shape, H.shape\n\n\n(torch.Size([3, 2, 27]), torch.Size([2, 32]))\n\n\nFinally, the book and Pytorch don’t include the output linear layer in the RNN layer. This way we can have the RNN layer focus only on updating the hidden state and could use it to generate output using a linear layer or something more complicated like a whole decoder module.\n\n\nRNN layer without output layer\nclass RNNScratch(nn.Module):\n    \"\"\"The RNN model implemented from scratch.\"\"\"\n    def __init__(self, num_inputs, num_hiddens, sigma = 0.01):\n        super().__init__()\n        self.num_inputs = num_inputs; self.num_hiddens = num_hiddens\n\n        self.W_xh = nn.Parameter(torch.randn(num_inputs, num_hiddens) * sigma)\n        self.W_hh = nn.Parameter(torch.randn(num_hiddens, num_hiddens) * sigma)\n        self.b_h = nn.Parameter(torch.zeros(num_hiddens))\n    \n    def forward(self, X, H = None):\n        # Expects X: (seq_length, batch_size, num_inputs i.e. vocab_size)\n        # Outputs: (seq_len, batch_size, num_outputs i.e. vocab_size)\n        if H is None: H = torch.zeros(X.shape[1], self.num_hiddens)\n        hidden_states = [] # we now return all hidden states\n        for X_i in X:\n            H = torch.tanh(torch.matmul(X_i, self.W_xh) + torch.matmul(H, self.W_hh) + self.b_h)\n            hidden_states.append(H)\n        return torch.stack(hidden_states)\n\n\nWe can now define a language model module to deal with generating the output, embedding the input, and sampling sequences from the model.\n\n\nRNN Language Model\nclass RNNLM(nn.Module):\n    def __init__(self, vocab_size, num_hiddens, sigma = 0.01):\n        super().__init__()\n        self.vocab_size = vocab_size\n\n        self.rnn = RNNScratch(vocab_size, num_hiddens, sigma)\n        self.W_hq = nn.Parameter(torch.randn(num_hiddens, vocab_size) * sigma)\n        self.b_q = nn.Parameter(torch.zeros(vocab_size))\n\n    def embed(self, X): return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n    \n    def output_layer(self, hidden_states):\n        return torch.stack([torch.matmul(H, self.W_hq) + self.b_q for H in hidden_states])\n    \n    def forward(self, X, H = None):\n        hidden_states = self.rnn(self.embed(X), H)\n        return self.output_layer(hidden_states)\n    \n    @torch.no_grad()\n    def generate(self, preamble, num_chars, char_to_ix):\n    \n        generation = preamble\n        prepare_X = lambda char: self.embed(torch.Tensor([[char_to_ix[char]]]).long())\n        hidden_state = None\n\n        for char in preamble: # warm-up\n            hidden_state = self.rnn(prepare_X(char), hidden_state)[-1] # only the last hidden state\n\n        for _ in range(num_chars): # generation\n            hidden_state = self.rnn(prepare_X(generation[-1]), hidden_state)[-1]\n            output = self.output_layer([hidden_state])\n            generation += ix_to_char[output.argmax(dim = 2).item()]\n\n        return generation\n\n\nLet’s test it out. We first sample a sequence continuation before training the model.\n\nm = RNNLM(vocab_size = len(vocab), num_hiddens = 32)\nm.generate('it has', 20, char_to_ix)\n\n'it hasxhkzwytyqdmuvngggggg'\n\n\nAnd train the model, now with the simplified training loop. To imitate Section 9.5.4. of the book, we do not use intermediate predictions and clip the gradients to magnitude 1 using nn.utils.clip_grad_norm_.\n\n\nEvaluation function\n@torch.no_grad()\ndef eval(m, test_loader):\n    m.eval()\n    loss = 0\n    for X, Y in test_loader:\n        outputs = m(X)\n        outputs = outputs[-1]\n        targets = Y[:, -1]\n        loss += F.cross_entropy(outputs, targets).item()\n    return np.exp(loss / len(test_loader)) # perplexity\n\n\n\n\nTrain the model\nopt = optim.SGD(m.parameters(), lr = 1)\ntrain_perplexities, test_perplexities = [], []\nfor _ in range(100):\n\n    for X, Y in train_loader:\n\n        opt.zero_grad()\n\n        outputs = m(X)\n        outputs = outputs[-1]\n        targets = Y[:, -1]\n\n        loss = F.cross_entropy(outputs, targets)\n        loss.backward()\n        nn.utils.clip_grad_norm_(m.parameters(), 1) # Clip the gradient\n        opt.step()\n        \n        train_perplexities.append(np.exp(loss.item()))\n    test_perplexities.append(eval(m, test_loader))\n\nplt.plot(train_perplexities, label = 'train')\nplt.plot(list(range(0, len(train_perplexities), len(train_loader))),test_perplexities, label = 'test')\nplt.legend(); plt.xlabel('batch'); plt.ylabel('perplexity')\n\n\nText(0, 0.5, 'perplexity')\n\n\n\n\n\n\n\n\n\nFinally, let’s see what the trained model generates:\n\nm.generate('it has', 20, char_to_ix)\n\n'it hase the stound the sto'\n\n\nI think that is enough context. Let’s get to some of the exercises I found interesting.\n\n\n1\n\nDoes the implemented language model predict the next token based on all the past tokens up to the very first token in The Time Machine?\n\nIn general no. The model is trained to predict the next token based only on the previous num_steps tokens. However, after generation, you could pass in the whole of The Time Machine to the model in the warm-up phase and then ask it to predict the next token. In principle the prediction would be based on all tokens up to the very first token but since the model has to compress every token it has seen into the hidden state it’s likely to not remember much of the beginning of the text.\n\n\n2\n\nWhich hyperparameter controls the length of history used for prediction?\n\nnum_steps during training and the length of prefix during inference.\n\n\n3\n\nShow that one-hot encoding is equivalent to picking a different embedding for each object.\n\nOne-hot embedding assigns each object a vector of zeros that is of size # of objects with a 1 in the unique entry that corresponds to the object. Thus every object has a distinct embedding.\n\n\n5\n\nReplace one-hot encoding with learnable embeddings. Does this lead to better performance?\n\nWe can simply add an embedding module to the language model:\n\n\nRNN LM with learnable embeddings\nclass RNNLMLearnableEmbs(RNNLM):\n    def __init__(self, vocab_size, num_hiddens, sigma = 0.01):\n        super().__init__(vocab_size, num_hiddens, sigma)\n        self.emb = nn.Embedding(num_embeddings = len(vocab), embedding_dim = len(vocab))\n\n    def embed(self, X): return self.emb(X.T)\n\n\nNow the learnable parameters of the model are:\n\nm = RNNLMLearnableEmbs(vocab_size = len(vocab), num_hiddens = 32)\nfor name, param in m.named_parameters(): print(name, param.shape)\n\nW_hq torch.Size([32, 27])\nb_q torch.Size([27])\nrnn.W_xh torch.Size([27, 32])\nrnn.W_hh torch.Size([32, 32])\nrnn.b_h torch.Size([32])\nemb.weight torch.Size([27, 27])\n\n\n\n\nTrain the model\nopt = optim.SGD(m.parameters(), lr = 1)\n\ntrain_perplexities, test_perplexities = [], []\nfor _ in range(100):\n\n    for X, Y in train_loader:\n\n        opt.zero_grad()\n\n        outputs = m(X)\n        outputs = outputs[-1]\n        targets = Y[:, -1]\n\n        loss = F.cross_entropy(outputs, targets)\n        loss.backward()\n        nn.utils.clip_grad_norm_(m.parameters(), 1) # Clip the gradient\n        opt.step()\n        \n        train_perplexities.append(np.exp(loss.item()))\n    test_perplexities.append(eval(m, test_loader))\n\nplt.plot(train_perplexities, label = 'train')\nplt.plot(list(range(0, len(train_perplexities), len(train_loader))),test_perplexities, label = 'test')\nplt.legend(); plt.xlabel('batch'); plt.ylabel('perplexity')\n\n\nText(0, 0.5, 'perplexity')\n\n\n\n\n\n\n\n\n\nI.e. we got comparable results using one-hot encoding and learnable embeddings did not seem to improve performance.\n\n\n6\n\nConduct an experiment to determine how well this language model trained on The Time Machine works on other books by H. G. Wells, e.g., The War of the Worlds.\n\n\n\nEvaluate on other texts\ndef eval_on_other_text(m, url):\n    text = requests.get(url).text\n    text = re.sub('[^A-Za-z]+', ' ', text).lower()\n    tokens = list(text)\n    test_dataset = OffsetSequences([char_to_ix[t] for t in tokens[:10000]], num_steps)\n    test_loader  = DataLoader(test_dataset,  batch_size = batch_size, shuffle = False, drop_last = True)\n    return eval(m, test_loader)\n\nperplexities = {\n    'time machine': eval(m, test_loader),\n    'the sleeper awakes': eval_on_other_text(m, 'https://gutenberg.org/cache/epub/12163/pg12163.txt'),\n    'the war of the worlds': eval_on_other_text(m, 'https://gutenberg.org/cache/epub/36/pg36.txt'),\n    'britling sees it through': eval_on_other_text(m, 'https://gutenberg.org/cache/epub/14060/pg14060.txt'),\n    'certain personal matters': eval_on_other_text(m, 'https://gutenberg.org/cache/epub/17508/pg17508.txt'),\n}\n\nperplexities\n\n\n{'time machine': 5.590489799933571,\n 'the sleeper awakes': 7.463832230616033,\n 'the war of the worlds': 6.850138662194433,\n 'britling sees it through': 8.232215427642316,\n 'certain personal matters': 7.385997756118561}\n\n\nWe get increased losses on texts by the same author, presumably (and hopefully) because of changes in the stories, plot lines, etc. What about texts from other authors?\n\n\n7\n\nConduct another experiment to evaluate the perplexity of this model on books written by other authors.\n\n\n\nCode\nother_perplexities = {\n    'The Mysterious Affair (Agatha)': eval_on_other_text(m, 'https://gutenberg.org/cache/epub/12163/pg12163.txt'),\n    'Odessy (Homer)': eval_on_other_text(m, 'https://gutenberg.org/cache/epub/1727/pg1727.txt'),\n    'Candice (Voltaire)': eval_on_other_text(m, 'https://gutenberg.org/cache/epub/19942/pg19942.txt'),\n    'Don Quixote (Cervantes)': eval_on_other_text(m, 'https://gutenberg.org/cache/epub/2000/pg2000.txt'),\n}\nother_perplexities\n\n\n{'The Mysterious Affair (Agatha)': 7.463832230616033,\n 'Odessy (Homer)': 7.94175036576811,\n 'Candice (Voltaire)': 8.60364584900993,\n 'Don Quixote (Cervantes)': 28.923269995282546}\n\n\nWith the first 3 texts I tried at first, there wasn’t that much of a jump in perplexities. Evaluating the model in the last text in Spanish served as a sanity check.\n\n\n8\n\nModify the prediction method so as to use sampling rather than picking the most likely next character.\n\nWhat happens?\nBias the model towards more likely outputs, e.g., by sampling from \\(q(x_t | x_{t-1}, ~\\dots~, x_1) \\propto P(x_t | x_{t-1}, ~\\dots~, x_1)^\\alpha\\) for $&gt; 1 $\n\n\nWe can do so by multiplying the logits by \\(\\alpha\\) (inverse temperature) before applying softmax. Notice that as \\(\\alpha \\rightarrow 0\\) we sample tokens uniformly at random, and as \\(\\alpha \\rightarrow +\\infty\\) we’ll select the most probable token.\n\n\nCode\nf, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (8, 3), sharey = True)\nwith torch.no_grad():\n    X, Y = next(iter(test_loader))\n    logits = m(X)[-1][0]\n    for ax, alpha in zip([ax1, ax2, ax3], [0.1, 1, 20]):\n        probs = F.softmax(logits * alpha, dim = 0).numpy()\n        ax.bar(list(range(len(vocab))), probs)\n        ax.set_title(r'$\\alpha = $' + str(alpha))\n\nax2.set_xlabel('Character Index'); ax1.set_ylabel(r'Predicted probability $q(x_{t})$'); f.tight_layout()\n\n\n\n\n\n\n\n\n\nLet’s observe qualitatively how generation is affected. We should expect to see less diverse but more probable sequences as we increase \\(\\alpha\\).\n\n\nCode\n@torch.no_grad()\ndef generate_by_sampling(model, preamble, num_chars, alpha, char_to_ix):\n\n    generation = preamble\n    prepare_X = lambda char: model.embed(torch.Tensor([[char_to_ix[char]]]).long())\n    hidden_state = None\n\n    for char in preamble: # warm-up\n        hidden_state = model.rnn(prepare_X(char), hidden_state)[-1] # only the last hidden state\n\n    for _ in range(num_chars): # generation\n        hidden_state = model.rnn(prepare_X(generation[-1]), hidden_state)[-1]\n        output = model.output_layer([hidden_state])\n        weights = F.softmax(output.squeeze() * alpha, dim = 0)\n        generation += ix_to_char[torch.multinomial(weights, 1).item()]\n\n    return generation\n\nfor alpha in [0.1, 1, 2, 10]:\n    print(f'alpha: {alpha}\\tgeneration:{generate_by_sampling(m, \"friday\", 20, alpha, char_to_ix)}')\n\n\nalpha: 0.1  generation:friday furzziis   ushxthfr\nalpha: 1    generation:friday thought filltise to\nalpha: 2    generation:friday to little the prese\nalpha: 10   generation:friday the shate in the si\n\n\n\n\n9\n\nRun the code in this section without clipping the gradient. What happens?\n\nI was expecting exploding gradients and, thus, for the network to be untrainable, but surprisingly I did not observe it. I tried increasing the sequence length from 32 to 256, adding hooks to debug, etc. Nothing.\n\n\nTrain with and without graddient clipping\nact_means, grad_norms = [], []\ndef log_activations(module, input, output): act_means.append(output[-1].mean().item())\ndef log_grad_norms(module, grad_input, grad_output): grad_norms.append(np.linalg.norm(grad_output[0]))\n\ndef nine(clipping):\n\n    global act_means, grad_norms\n    act_means, grad_norms = [], []\n\n    batch_size, num_steps = 1024, 256\n    train_dataset = OffsetSequences([char_to_ix[t] for t in tokens[:train_cutoff]], num_steps)\n    test_dataset  = OffsetSequences([char_to_ix[t] for t in tokens[train_cutoff:]], num_steps)\n    train_loader  = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, drop_last = True)\n    test_loader   = DataLoader(test_dataset,  batch_size = batch_size, shuffle = False, drop_last = True)\n\n    m = RNNLM(vocab_size = len(vocab), num_hiddens = 32)\n    opt = optim.SGD(m.parameters(), lr = 1)\n\n    hooks = [m.register_forward_hook(log_activations), m.register_backward_hook(log_grad_norms)]\n\n    train_perplexities, test_perplexities = [], []\n    for _ in range(10):\n\n        for X, Y in train_loader:\n\n            opt.zero_grad()\n\n            outputs = m(X)\n            outputs = outputs[-1]\n            targets = Y[:, -1]\n\n            loss = F.cross_entropy(outputs, targets)\n            loss.backward()\n            if clipping: nn.utils.clip_grad_norm_(m.parameters(), 1)\n            opt.step()\n            \n            train_perplexities.append(np.exp(loss.item()))\n\n        test_perplexities.append(eval(m, test_loader))\n\n    for hook in hooks: hook.remove()\n\n    return train_perplexities, test_perplexities\n\nf, axs = plt.subplots(1, 3, figsize = (12, 5))\n\nfor clipping in [False, True]:\n\n    train_perplexities, _ = nine(clipping = clipping)\n    axs[0].plot(train_perplexities)\n    #axs[0].plot(list(range(0, len(train_perplexities), len(train_loader))), test_perplexities, label = 'test')\n    axs[0].set_title('Perplexity')#; axs[0].set_yscale('log')\n    axs[1].plot(act_means, label = f'With{\"out\" if not clipping else \"\"} clipping'); axs[1].set_title('Mean Activation')\n    axs[2].plot(grad_norms); axs[2].set_title('Mean Gradient Norm')\n\naxs[1].set_xlabel('batch'); axs[1].legend()\nf.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\nRNN Language Model\nclass RNNLM(nn.Module):\n    def __init__(self, vocab_size, num_hiddens):\n        super().__init__()\n        self.vocab_size = vocab_size\n\n        self.rnn = nn.RNN(vocab_size, num_hiddens)\n        self.linear = nn.Linear(num_hiddens, vocab_size)\n        \n    def embed(self, X): return F.one_hot(X.T, self.vocab_size).type(torch.float32)\n    \n    def output_layer(self, hidden_states):\n        return torch.stack([self.linear(H) for H in hidden_states])\n        # return torch.stack([torch.matmul(H, self.W_hq) + self.b_q for H in hidden_states])\n    def forward(self, X, H = None):\n        return self.rnn(self.embed(X), H)\n    \n    @torch.no_grad()\n    def generate(self, preamble, num_chars, char_to_ix):\n    \n        generation = preamble\n        prepare_X = lambda char: self.embed(torch.Tensor([[char_to_ix[char]]]).long())\n        hidden_state = None\n\n        for char in preamble: # warm-up\n            hidden_state = self.rnn(prepare_X(char), hidden_state)[-1] # only the last hidden state\n\n        for _ in range(num_chars): # generation\n            hidden_state = self.rnn(prepare_X(generation[-1]), hidden_state)[-1]\n            output = self.output_layer([hidden_state])\n            generation += ix_to_char[output.argmax(dim = 2).item()]\n\n        return generation"
  },
  {
    "objectID": "dl-playground/understanding-bn/index.html",
    "href": "dl-playground/understanding-bn/index.html",
    "title": "Understanding Batch Normalization",
    "section": "",
    "text": "The paper investigates the cause of batch norm’s benefits experimentally. The authors show that its main benefit is allowing for larger learning rates during training. In particular:\n\n“We show that the activations and gradients in deep neural networks without BN tend to be heavy-tailed. In particular, during an early on-set of divergence, a small subset of activations (typically in deep layer) “explode”. The typical practice to avoid such divergence is to set the learning rate to be sufficiently small such that no steep gradient direction can lead to divergence. However, small learning rates yield little progress along flat directions of the optimization landscape and may be more prone to convergence to sharp local minima with possibly worse generalization performance.”\n\nWe attempt to reproduce figures 1-3, 5, and 6.\n\nConvolutional BN Layer\nAs a reminder, the input \\(I\\) and output \\(O\\) tensors to a batch norm layer are 4 dimensional. The dimensions \\((b, c, x, y)\\) correspond to the batch example, channel, and spatial \\(x\\), \\(y\\) dimensions respectively. Batch norm (BN) applies a channel-wise normalization:\n\\[\nO_{b, c, x, y} \\leftarrow \\gamma_c \\frac{I_{b, c, x, y} - \\hat \\mu_c}{\\sqrt{\\hat \\sigma_c^2 + \\epsilon}} + \\beta_c\n\\]\nWhere \\(\\hat \\mu_c\\) and \\(\\hat \\sigma_c^2\\) are estimates channel \\(c\\)’s mean and standard deviation computed on the minibatch \\(\\mathcal B\\):\n\\[\n\\hat \\mu_c = \\frac{1}{|\\mathcal B|}\\sum_{b, x, y} I_{b, c, x, y}\n\\]\n\\[\n\\hat \\sigma_c^2 = \\frac{1}{\\mathcal |B|} \\sum_{b, x, y} (I_{b, c, x, y} - \\hat \\mu_c) ^ 2\n\\]\nTo make sure the layer does not lose expressive power we introduce learned parameters \\(\\gamma_c\\) and \\(\\beta_c\\). \\(\\epsilon\\) is a small constant added for numerical stability. In pytorch, we can simply use the BatchNorm2d layer.\n\n\nExperimental setup\nLet’s set up our data loaders, model, and training loop as described in Appendix B of the paper.\n\n\nImports and model evaluation function\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os, itertools, time\n\nos.makedirs('logs', exist_ok = True)\nos.makedirs('models', exist_ok = True)\n\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\ndevice = torch.device(\n    'cuda' if torch.cuda.is_available() else\n    ('mps' if torch.backends.mps.is_available() else\n    'cpu')\n)\n\ndef eval_model(model, test, criterion = nn.CrossEntropyLoss()):\n    model.eval()\n    correct, loss = 0, 0.0\n    with torch.no_grad():\n        for images, labels in test:\n            images, labels = images.to(device), labels.to(device)\n            _, pred = torch.max(model(images), 1)\n            correct += (pred == labels).float().sum().item()\n            loss += criterion(model(images), labels).item()\n    return loss / len(test.dataset), correct / len(test.dataset)\n\ndevice\n\n\nThe paper trains ResNet-110s on CIFAR-10, with channel-wise normalization, random horizontal flipping, and 32-by-32 cropping with 4-pixel zero padding. We’ll train the ResNet-101 included in torchvision but keep everything the same.\nWe first get the datasets and compute the channel-wise means and variances. Note: both the training and validation set have the same values.\n\n\nDatasets and channel-wise means and stds\ntrain_set = datasets.CIFAR10('./data', download = True, train = True, transform = transforms.ToTensor())\nval_set = datasets.CIFAR10('./data', download = True, train = False, transform = transforms.ToTensor())\n\ndef channel_means_stds(dataset):\n    imgs = torch.stack([img for img, _ in train_set])\n    return imgs.mean(dim = [0, 2, 3]), imgs.std(dim = [0, 2, 3])\n\nmeans, stds = channel_means_stds(train_set)\nprint(f'Training channel-wise\\n\\tmeans: {means}\\n\\tstds: {stds}')\n\nmeans, stds = channel_means_stds(val_set)\nprint(f'Validation channel-wise\\n\\tmeans: {means}\\n\\tstds: {stds}')\n\n\nWe now define the transforms with data augmentation and data loaders with batch size \\(128\\).\n\n\nData transforms and data loaders\ntrain_transform = transforms.Compose([\n    transforms.RandomCrop(32, padding = 4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(means, stds),\n])\n\n# We do not perform data augmentation on the validation set\nval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(means, stds),\n])\n\ntrain_set.transform = train_transform\nval_set.transform = val_transform\n\ntrain_loader = DataLoader(train_set, batch_size = 128, shuffle = True)\nval_loader = DataLoader(val_set, batch_size = 128, shuffle = False)\n\n\nWe’ll use torchvision’s implementation of ResNet-101, Xavier initialization, SGD with momentum \\(0.9\\) and weight decay \\(5\\times 10^{-4}\\), and cross-entropy loss. We try to implement the training details and learning rate scheduling as mentioned in the paper:\n\n“Initially, all models are trained for 165 epochs and as in [17] we divide the learning rate by 10 after epoch 50% and 75%, at which point learning has typically plateued. If learning doesn’t plateu for some number of epochs, we roughly double the number of epochs until it does”.\n\n\n\nInit, and train functions\ndef xavier_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight)\n\ndef train_epoch(model, train, optimizer, criterion):\n    # Trains the model for one epoch\n    model.train()\n    train_loss, correct = 0.0, 0\n    for images, labels in train:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        output = model(images)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        _, pred = torch.max(output, 1)\n        correct += (pred == labels).float().sum().item()\n    return train_loss / len(train.dataset), correct / len(train.dataset)\n\n\ndef train(model, train, val, init_lr, plateau_patience = 20):\n\n    optimizer = optim.SGD(model.parameters(), lr = init_lr, momentum = 0.9, weight_decay = 5e-4)\n    scheduler  = optim.lr_scheduler.MultiStepLR(optimizer, milestones = [82, 123], gamma = 0.1)\n    criterion = nn.CrossEntropyLoss()\n\n    model.to(device)\n\n    init_epochs = 165\n\n    epoch = 0\n    plateau_count = 0\n    best_loss = None\n\n    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n\n    while epoch &lt; init_epochs and plateau_count &lt; plateau_patience:\n\n        # Train the model for an epoch\n        loss, acc = train_epoch(model, train, optimizer, criterion)\n        train_losses.append(loss)\n        train_accs.append(acc)\n\n        # Evaluate the model on the validation set\n        val_loss, val_acc = eval_model(model, val, criterion)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        # Update the learning rate\n        scheduler.step(val_loss)\n\n        # Check for a plateau\n        if best_loss is None or val_loss &lt; best_loss:\n            best_loss = val_loss\n            plateau_count = 0\n        else:\n            plateau_count += 1\n        \n        epoch += 1\n\n        # \"If learning doesn’t plateu for some number of epochs,\n        # we roughly double the number of epochs until it does.\"\n        if epoch == init_epochs and plateau_count &lt; plateau_patience:\n            init_epochs *= 2\n\n        print(f'Epoch {epoch}/{init_epochs} | Learning Rate: {optimizer.param_groups[0][\"lr\"]} | '\n              f'Training loss: {train_losses[-1]:.4f} | '\n              f'Validation loss: {val_losses[-1]:.4f} | '\n              f'Validation accuracy: {val_accs[-1]:.4f}')\n        \n    return train_losses, train_accs, val_losses, val_accs\n\n\nAnd we define a function to disable batch norm layers in a model by replacing them with identity layers:\n\ndef disable_bn(model):\n    for name, module in model.named_children():\n        if isinstance(module, nn.BatchNorm2d):\n            setattr(model, name, nn.Identity())\n        else:\n            disable_bn(module)  # Recursively replace in child modules\n\n\n\nFig 1\nFigure 1 aims to demonstrate that batch norm’s primary benefit is that it allows training with larger learning rates.\nThe authors find the highest (initial) learning rate with which they can train an unnormalized model (\\(\\alpha = 0.0001\\)) and compare its performance with normalized models trained with \\(\\alpha \\in \\{0.0001, 0.003, 0.1\\}\\). We train each model once (instead of five times to save on compute) and present train and test accuracy curves.\n\n\nTrain models\nMODELS_DIR = 'models'\nLOGS_DIR = 'logs'\n\nfor lr, bn in [(0.0001, False), (0.0001, True), (0.003, True), (0.1, True)]:\n\n    s = f'lr={lr}' + ('_bn' if bn else '')\n    print(s)\n\n    model = models.resnet101(num_classes = 10)\n    model.apply(xavier_init)\n\n    if not bn: disable_bn(model)\n\n    torch.save(model, f'{MODELS_DIR}/{s}_init.pth')\n    data = train(model, train_loader, val_loader, init_lr = lr)\n    torch.save(model, f'{MODELS_DIR}/{s}_end.pth')\n    torch.save(data, f'{LOGS_DIR}/{s}.pth')\n\n\n\n\nCode\n# code-summary: Plot results\n\nget_x = lambda x: 100 * np.arange(1, len(x) + 1) / len(x)\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (9, 4), sharey = True)\n\nfor fname in os.listdir(LOGS_DIR):\n\n    train_losses, train_accs, val_losses, val_accs = torch.load(f'{LOGS_DIR}/{fname}')\n    ax1.plot(get_x(train_accs), train_accs, label = fname[:-4])\n    ax2.plot(get_x(val_accs), val_accs, label = fname[:-4])\n    print(f'{fname[:-4]} took {len(train_accs)} epochs')\n\n\nax1.legend(); ax2.legend()\nax1.set_ylabel('Training accuracy'); ax2.set_ylabel('Validation accuracy')\nax1.set_xlabel('% of training'); ax2.set_xlabel('% of training')\nf.tight_layout()\n\n\nlr=0.1_bn took 83 epochs\nlr=0.0001 took 263 epochs\nlr=0.003_bn took 69 epochs\nlr=0.0001_bn took 211 epochs\n\n\n\n\n\n\n\n\n\nAnd observe the same general trends found in the original paper: similar learning rates result in about the same performance (red and orange) while increasing the rate yields better performance for normalized networks (blue) and training diverges for non-normalized ones (not shown).\n\n\nFig 2\nIn Figure 2 the authors begin to investigate “why BN facilitates training with higher learning rates in the first place”. The authors claim that batch norm (BN) prevents divergence during training, which usually occurs because of large gradients in the first mini-batches.\nSo, the authors analyze the gradients at initialization of a midpoint layer (55) with and without batch norm. They find that gradients in unnormalized networks are consistently larger and distributed with heavier tails.\nI had trouble replicating this figure. I could not obtain the general shape and scale of the histograms they did:\n\n\n\n\n\nAt first, I thought because I was\n\nlooking at the wrong layer, +- 1 (then found it made little difference)\nlogging the gradient magnitudes incorrectly - why does the plot have negative values? (then found the authors plot the raw gradient)\nmisunderstanding the whole process\n\nAs I understood it, we initialize the model (using Xavier’s initialization), do a forward and backward pass on a single batch, and log the gradients at roughly the middle layer:\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3))\nimages, labels = next(iter(val_loader))\nimages, labels = images.to(device), labels.to(device)\n\nfor bn, ax in zip([True, False], [ax1, ax2]):\n\n    # Init\n    model = models.resnet101(num_classes = 10)\n    model.to(device)\n    if not bn: disable_bn(model)\n    model.apply(xavier_init)\n\n    # Forward and backward pass\n    model.train()\n    output = model(images)\n    loss = nn.CrossEntropyLoss()(output, labels)\n    loss.backward()\n\n    model.eval()\n    grads = model.layer3[9].conv1.weight.grad.view(-1).cpu().detach().numpy()\n    sns.histplot(grads, ax = ax)\n\nax1.set_title('With Batch Normalization')\nax2.set_title('Without Batch Normalization')\nax2.set_ylim((0, 2500)); ax2.set_ylabel('')\nf.tight_layout()\n\n\n\n\n\n\n\n\nAlthough the unnormalized gradients are heavy-tailed, they are still much smaller than the normalized ones. I was stuck on this issue for a few days until I experimented with different initializations:\n\n\nTrying other inits\ndef init_func(f, also_linear = True):\n    def init(m):\n        if isinstance(m, nn.Conv2d):\n            f(m.weight)\n        if also_linear and isinstance(m, nn.Linear):\n            f(m.weight)\n    return init\n\ncriterion = nn.CrossEntropyLoss()\nimages, labels = next(iter(val_loader))\nimages, labels = images.to(device), labels.to(device)\n\nwith_bn, without_bn = {}, {}\n\nfor init_f in [\n    nn.init.xavier_uniform_, nn.init.kaiming_uniform_,\n    nn.init.kaiming_normal_, lambda x: nn.init.kaiming_normal_(x, mode = 'fan_out', nonlinearity='relu')]:\n\n    init_name = init_f.__name__[:-1]\n    if 'lambda' in init_name: init_name = '(default) kaiming_normal fan_out'\n    \n    for linear in (True, False):\n\n        lin_name = ' w/linear lyrs' if linear else ''\n        \n        for bn, d in zip((True, False), (with_bn, without_bn)):\n                \n                model = models.resnet101(num_classes = 10)\n                model.to(device)\n                if not bn: disable_bn(model)\n                model.apply(init_func(init_f, linear))\n\n                model.train()\n                output = model(images)\n                loss = criterion(output, labels)\n                loss.backward()\n\n                model.eval()\n                d[init_name + lin_name] = model.layer3[9].conv1.weight.grad.view(-1).cpu().detach().numpy()\n\n\n\n\nPlotting the results\nfor init_name, v in with_bn.items():\n    f, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3))\n    sns.histplot(v, ax = ax1)\n    ax1.set_title('with BN')\n    sns.histplot(without_bn[init_name], ax = ax2)\n    ax2.set_title('without BN')\n    ax2.set_ylabel('')\n    ax2.set_ylim(0, 1500)\n    f.suptitle(init_name)\n    f.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see the general shapes, normal vs heavy-tailed don’t depend that much on the initialization scheme but the scales do. We could only achieve the same scale of the gradients presented in the paper by using the kaiming_normal scheme with fan=out (to preserve the magnitudes of the variance of the weights in the backward pass instead of the forward one) and applying it only to Conv2 layers. This is the default used by torchvision’s resnets.\nNote: xavier_normal produced very similar shapes/scales as xavier_uniform so we don’t show it.\nFor the rest of the figures we’ll use the default init scheme:\n\ndef init_net(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode = 'fan_out', nonlinearity = 'relu')\n\n\n\nFig 3\nThe authors then investigate the loss landscape along the gradient direction for the first few mini-batches for models with BN (trained with \\(\\alpha = 0.1\\)) and without BN (\\(\\alpha = 0.0001\\)). For each network and mini-batch they compute the gradient and plot the relative change in the loss (new_loss/old_loss).\nWe save the model’s and optimizer’s states (state_dict) before taking the tentative steps to explore the landscape and restore them before taking the actual step between batches.\n\n\nExplore loss landscape\ndef fig3(model, init_lr, log_lrs, log_batches = [1, 4, 7]):\n\n    # batch -&gt; list of relative losses for each lr\n    out = {}\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr = init_lr, momentum = 0.9, weight_decay = 5e-4)\n\n    for i, (images, labels) in enumerate(train_loader):\n\n        images, labels = images.to(device), labels.to(device)\n\n        if i in log_batches:\n            \n            torch.save(model.state_dict(), f'models/model_state.tmp')\n            torch.save(optimizer.state_dict(), f'models/optimizer_state.tmp')\n\n            rel_losses = []\n            for lr in log_lrs:\n\n                for param_group in optimizer.param_groups: param_group['lr'] = lr\n\n                optimizer.zero_grad()\n                output = model(images)\n                current_loss = criterion(output, labels)\n                current_loss.backward()\n                optimizer.step()\n\n                with torch.no_grad():\n                    output = model(images)\n                    tmp_loss = criterion(output, labels)\n                    rel_loss = (tmp_loss / current_loss).item()\n                \n                    # print learning rate, current loss, tmp loss, relative loss (at 4 decimal places)\n                    print(f'{lr:.5f} {current_loss:.5f} {tmp_loss:.5f} {rel_loss:.5f}')\n                    rel_losses.append(rel_loss)\n            \n                model.load_state_dict(torch.load('models/model_state.tmp'))\n                optimizer.load_state_dict(torch.load('models/optimizer_state.tmp'))   \n\n                # If loss is nan of int, break. Unlikely to recover.\n                if torch.isnan(tmp_loss).item() or torch.isinf(tmp_loss).item():\n                    rel_losses.pop()\n                    print('breaking')\n                    break\n\n            out[i] = rel_losses\n        \n        if i == max(log_batches): break\n\n        # take the actual step\n        optimizer.zero_grad()\n        output = model(images)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n    \n    return out\n\n\nlrs = np.logspace(-5.5, 0.5, 80)\n\n# With BN\nmodel = models.resnet101(num_classes = 10)\nmodel.to(device)\nmodel.apply(init_net)\nwith_bn = fig3(model, init_lr = 0.1, log_lrs = lrs)\n\n# Without BN\nmodel = models.resnet101(num_classes = 10)\nmodel.to(device)\ndisable_bn(model)\nmodel.apply(init_net)\nwithout_bn = fig3(model, init_lr = 0.0001, log_lrs = lrs)\n\n\n\n\nPlotting the results\nf, axs = plt.subplots(2, 3, figsize = (12, 6))\n\nfor (batch, rel_losses), ax in zip(with_bn.items(), axs[0]):\n    ax.plot(lrs, rel_losses)\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n    ax.set_ylim(0, 1.5)\n    ax.set_xlim(10**-5.5, 0.4)\n    ax.set_title(f'Batch {batch} with BN')\n    ax.axhline(1, color = 'grey', linestyle = '--')\n\nfor (batch, rel_losses), ax in zip(without_bn.items(), axs[1]):\n    ax.plot(lrs[:len(rel_losses)], rel_losses)\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n    ax.set_ylim(10**-2, 10**3)\n    ax.set_xlim(10**-5.5, 0.4)\n    ax.set_title(f'Batch {batch} w/o BN')\n    ax.axhline(1, color = 'grey', linestyle = '--')\n\naxs[0, 0].set_ylabel('Relative loss')\naxs[1, 0].set_ylabel('Relative loss')\nf.tight_layout()\n\n\n\n\n\n\n\n\n\nAlthough we get roughly different scales, we observe that unnormalized networks reduce the loss only with small steps while normalized ones can improve with a much larger range, as in the paper.\n\n\nFig 5\nFigures 5 and 6 explore the behavior of networks at initialization. Figure 5 displays the mean and variances of channels in the network as a function of depth at initialization. We initialize \\(10\\) networks and use forward hooks to log their channel mean and standard deviations.\n\n\nLog activation stats\ndf = []\n\ndef log_activation_stats(layer_name, key):\n    def hook(module, input, output):\n        with torch.no_grad():\n            df.append({\n                'bn': key,\n                'layer': layer_name,\n                'mean': output[:, 0, :, :].mean().abs().item(),\n                'std': output[:, 0, :, :].std().abs().item()\n            })\n    return hook\n\n\nfor _ in range(10):\n\n    for bn in [True, False]:\n\n        model = models.resnet101(num_classes = 10)\n        model.to(device)\n        if not bn: disable_bn(model)\n        model.apply(init_net)\n\n        # Layers to log activations from\n        log_layers = [] # (n, name, layer)\n        n = 1\n        for name, layer in model.named_modules():\n            if 'conv' in name:\n                n += 1\n                if n in list(range(5, 101 + 12, 12)):\n                    log_layers.append((n, name, layer))\n\n        for n, _, layer in log_layers: layer.register_forward_hook(log_activation_stats(n, str(bn)))\n        for images, _ in val_loader: model(images.to(device))\n\ndf = pd.DataFrame(df)\n\n\n\n\nCode\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (9, 4))\n\nsns.lineplot(data = df, x = 'layer', y = 'mean', hue = 'bn', ax = ax1)\nax1.set_yscale('log')\n\nsns.lineplot(data = df, x = 'layer', y = 'std', hue = 'bn', ax = ax2)\nax2.set_yscale('log')\n\nf.tight_layout()\n\n\n\n\n\n\n\n\n\nWe observe, consistent with the findings in the paper, that activation means and standard deviations increase almost exponentially in non-normalized networks, whereas they remain nearly constant in normalized networks.\n\n\nFig 6\nThe large activations in the final layers for unnormalized networks in the previous figure make us suspect that networks are biased towards a class. The authors investigate whether this is the case by looking at the gradients in the final (output) layer across images in a mini-batch and classes.\nNote: Don’t confuse this with the last fully connected layer of the network. We are looking at the gradients of the output logits themselves. We need to use retain_grad on the output (non-leaf node) to calculate its gradient on the backward pass.\n\n\nGenerate heatmaps\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3), sharey = True, sharex = True)\n\nimages, labels = next(iter(val_loader))\nimages, labels = images.to(device), labels.to(device)\n\nfor bn, ax in zip([False, True], [ax1, ax2]):\n\n    model = models.resnet101(num_classes = 10)\n    model.to(device)\n    if not bn: disable_bn(model)\n    model.apply(init_net)\n\n    out = model(images)\n    out.retain_grad()\n    loss = nn.CrossEntropyLoss()(out, labels)\n    loss.backward()\n\n    ax = sns.heatmap(out.grad.cpu().detach().numpy(), cmap = 'viridis', ax = ax)\n    ax.set_xticks([]); ax.set_yticks([0, 40, 80, 120]); ax.set_yticklabels([0, 40, 80, 120])\n    ax.set_xlabel('Classes')\n    ax.set_ylabel('Images in batch' if not bn else '')\n    ax.set_title('With BN' if bn else 'Without BN')\n\nf.tight_layout()\n\n\n\n\n\n\n\n\n\nAnd basically observe the same results as the paper:\n\n“A yellow entry indicates that the gradient is positive, and the step along the negative gradient would decrease the prediction strength of this class for this particular image. A dark blue entry indicates a negative gradient, indicating that this particular class prediction should be strengthened. Each row contains one dark blue entry, which corresponds to the true class of this particular image (as initially all predictions are arbitrary). A striking observation is the distinctly yellow column in the left heatmap (network without BN). This indicates that after initialization the network tends to almost always predict the same (typically wrong) class, which is then corrected with a strong gradient update. In contrast, the network with BN does not exhibit the same behavior, instead positive gradients are distributed throughout all classes.”\n\nRunning the above code multiple times, however, sometimes results in two or three yellow columns. We think this is because different mini-batches behave slightly differently or due to initialization randomness. Below, we log and average the gradients for a whole epoch and find much more consistent behavior.\n\n\nCode\ndef fig6(init_func = init_net):\n\n    f, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3), sharex = True, sharey = True)\n\n    for bn, ax in zip([False, True], [ax1, ax2]):\n\n        model = models.resnet101(num_classes = 10)\n        model.to(device)\n        if not bn: disable_bn(model)\n        model.apply(init_func)\n\n        avg_grad = torch.zeros((128, 10), device = device)\n\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            out = model(images)\n            out.retain_grad()\n            loss = nn.CrossEntropyLoss()(out, labels)\n            loss.backward()\n            if out.grad.shape == avg_grad.shape: avg_grad += out.grad\n        \n        avg_grad /= len(val_loader)\n\n        ax = sns.heatmap(avg_grad.cpu().detach().numpy(), cmap = 'viridis', ax = ax)\n        ax.set_xlabel('Classes')\n        ax.set_ylabel('Images in batch')\n        ax.set_title('With BN' if bn else 'Without BN')\n\n    f.tight_layout()\n\nfig6()\n\n\n\n\n\n\n\n\n\nAnd that’s about it.\n\n\nWhat I learned / practiced\nI gained a better understanding and intuition of why Batch Normalization (BN) works. More importantly, I got comfortable with PyTorch and debugging training, etc.\nPytorch specific:\n\nBasics of image augmentation: basically use transforms and compose them.\nLearning rate schedulers: they exist, are really useful, and pytorch has a good assortment of them.\nstate_dict preserves optimizer’s param groups and args (learning rates, etc.) but also momentum buffers.\nhooks as useful debugging and visualization tools.\nretain_grad is required to get gradients of non-leaf nodes like the output logits.\n\nFor the large training runs, I also experimented with jarvislabs.ai as a provider. In-browser notebooks and VS Code, and direct SSH/FTP access were pretty nice. I could not work out funkiness with VS Code remote windows. Used\nnohup jupyter nbconvert --execute --to notebook --inplace --allow-errors main.ipynb &\nto run the notebook, write results, and be able to close the Jupyter / VS Code tab."
  },
  {
    "objectID": "posts/lsh/index.html",
    "href": "posts/lsh/index.html",
    "title": "Approximate Nearest Cosine Neighbors",
    "section": "",
    "text": "Suppose you have some vectors and wish to find, for each point, the \\(k\\) nearest points. While you could compute pairwise distances using a naïve quadratic algorithm for small datasets, this approach becomes infeasible with millions or billions of points. If the points are in a low-dimensional space, clever data structures like kd-trees, ball trees, and M-trees can achieve substantial speedups. However, in high dimensions, performance degrades, and you may need to sacrifice exactness and turn to Approximate Nearest Neighbors (ANN) techniques.\nLocality-sensitive hashing (LSH) is a family of ANN algorithms that aim to group similar points into the same (or nearby) buckets efficiently using specialized hash functions. Recall that traditional hashing tries to map items to a set of buckets uniformly and minimize collisions. Thus, traditionally, slightly changing a point results in a vastly different hash and assigned bucket. LSH uses different hashing functions that often depend on the distance metric employed. Here, we explore a simple approach using cosine distance based on Random Projection.\n\nHow it works\nThe mechanics are not very complicated. We first generate \\(N_h\\) random hyperplanes. Let’s visualize this in two dimensions with \\(n=5\\) points and \\(N_h=2\\):\n\n\nRandom data and visualisation\n# Note that we sample from a standard normal distribution\nX = torch.randn((5, 2))\nrand_planes = torch.randn((2, 2)) - 0.5\n\ndef lims(X, ax, eps = 0.1):\n    m = X[:, ax].abs().max().item()\n    m += eps\n    return (-m, m)\n\nplt.scatter(X[:, 0], X[:, 1])\nplt.xlim(lims(X, 0)); plt.ylim(lims(X, 1))\n\n# Axes\nplt.axline((0,0), (0,1), c = '0', linewidth = 0.8)\nplt.axline((0,0), (1,0), c = '0', linewidth = 0.8)\n\n# Random planes\nfor v in rand_planes:\n    x, y = v.tolist()\n    plt.axline((0,0), (y, -x), c = 'g', linewidth = 0.8)\n\n\n\n\n\n\n\n\n\nNote that the plane is effectively partitioned into four regions. The main idea is to treat the regions as buckets and assign all the points in a region to the same bucket. When a new point comes along, we simply find out which region it belongs to and then search for points in that region and nearby regions until we accumulate \\(k\\) of them.\nGreat! How do we do it computationally? We mainly need to remember that a hyperplane is characterized by its normal vector \\(\\vec{v}\\) and that we can determine on which side of it a point \\(x\\) lies by the sign of their dot product, \\(\\text{sign}(\\vec{v} \\cdot \\vec{x})\\). For example, points 0 and 2 are on opposite sides of hyperplane 0:\n\nv = rand_planes[0]\nv @ X[0] &gt;= 0, v @ X[2] &gt;= 0\n\n(tensor(True), tensor(False))\n\n\n\n\nDot product example\nplt.scatter(*X[0].tolist())\nplt.scatter(*X[2].tolist())\nplt.xlim(lims(X, 0)); plt.ylim(lims(X, 1))\nplt.axline((0,0), (0,1), c = '0', linewidth = 0.8)\nplt.axline((0,0), (1,0), c = '0', linewidth = 0.8)\n\nx, y = rand_planes[0].tolist()\nplt.axline((0,0), (y, -x), c = 'g', linewidth = 0.8, label = 'hyperplane')\nplt.quiver(*([0], [0]), x, y, color = '0.5', label = 'v')\nplt.legend()\n\n\n\n\n\n\n\n\n\nThus, to find a point’s region, we can repeat this process for all (in our case, two) of the hyperplanes. For example:\n\nX[0] @ rand_planes[0] &gt;= 0, X[0] @ rand_planes[1] &gt;= 0\n\n(tensor(True), tensor(False))\n\n\n\nX[2] @ rand_planes[0] &gt;= 0, X[2] @ rand_planes[1] &gt;= 0\n\n(tensor(False), tensor(False))\n\n\nI.e., points 0 and 2 are in different regions. Using matrix notation, we can obtain every point’s region succinctly:\n\nregions = X @ rand_planes.T &gt;= 0\nregions\n\ntensor([[ True, False],\n        [ True, False],\n        [False, False],\n        [False,  True],\n        [ True, False]])\n\n\nWe can now place each point in a region into a bucket:\n\nbuckets = {}\nfor i, reg in enumerate(regions):\n    reg = tuple(reg.tolist())\n    if reg not in buckets: buckets[reg] = []\n    buckets[reg].append(i)\nbuckets\n\n{(True, False): [0, 1, 4], (False, False): [2], (False, True): [3]}\n\n\nAnd that’s all the preprocessing we need to do. Now, to find the nearest neighbors of a query point \\(\\vec{q}\\), we find its region:\n\nq = torch.randn((2,))\nq_reg = tuple((q @ rand_planes.T &gt;= 0).tolist())\nq, q_reg, buckets.get(q_reg, [])\n\n(tensor([-0.9890,  0.9580]), (True, False), [0, 1, 4])\n\n\nIn this case, we had three points in the query’s bucket. When the number of elements in the bucket is less than \\(k\\), the common approach is to look to nearby buckets in terms of Hamming distance. I.e., we first add points in buckets one bit flip away, then two flips away, and so on until we accumulate \\(k\\) (or slightly more) points.\n\n\nConsiderations\nTo help root out any false positives—points that are not in the top \\(k\\) nearest but happened to land in or near the buckets—we can calculate the actual cosine distance from the candidate points we retrieved and return only those below a specified threshold.\nYou can imagine that the number of hyperplanes presents an accuracy-speed tradeoff: more hyperplanes imply (exponentially) more buckets, which means fewer false positives but also more computation to calculate regions. You can find details in the references.\nIf you get unlucky with the random generation of the hyperplanes, you might have a high false negative rate—actually close points that end up in far-away buckets. In this case, we could generate other sets of hyperplanes, obtain their respective candidates, and obtain a candidate pool by taking their union. See the LSH Forest paper.\nAs a final note, how we generate hyperplanes is somewhat important. You can imagine that we generally want them to be evenly distributed on the unit sphere, which is why we sample from a normal distribution. We’d also like them to be spaced out, which is why some methods generate (expensive) orthogonal random matrices. Or you might not care in practice, remembering that in high dimensions, pairs of sampled points are almost surely orthogonal.\n\n\nResources\n\nBlog post w/some probabilty details\nMining of Massive Datasets (book and course, chapter 3)\nSimilarity Estimation Techniques from Rounding Algorithms"
  }
]