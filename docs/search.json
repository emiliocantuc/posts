[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "posts",
    "section": "",
    "text": "A Closer Look at Memorization in Deep Networks\n\n\nAn attempted (partial) paper reproduction\n\n\n\ndeep learning\n\n\npaper\n\n\n\n\n\n\n\n\n\n9/7/24\n\n\nE. Cantu\n\n\n\n\n\n\n\n\n\n\n\n\nApproximate Nearest Cosine Neighbors\n\n\n\n\n\n\ncs\n\n\nquick intro\n\n\n\nUsing Random Hyperplane LSH\n\n\n\n\n\n8/9/24\n\n\nE. Cantu\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Batch Normalization\n\n\nAn attempted (partial) paper reproduction\n\n\n\ndeep learning\n\n\npaper\n\n\n\n\n\n\n\n\n\n7/17/24\n\n\nE. Cantu\n\n\n\n\n\n\n\n\n\n\n\n\nBatch Norm exercises\n\n\n\n\n\n\ndeep learning\n\n\nexercises\n\n\n\n\n\n\n\n\n\n7/2/24\n\n\nE. Cantu\n\n\n\n\n\n\n\n\n\n\n\n\nLeNet exercises\n\n\n\n\n\n\ndeep learning\n\n\nexercises\n\n\n\n\n\n\n\n\n\n6/21/24\n\n\nE. Cantu\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Learning is Robust to Massive Label Noise\n\n\nAn attempted (partial) paper reproduction\n\n\n\ndeep learning\n\n\npaper\n\n\n\n\n\n\n\n\n\n6/18/24\n\n\nE. Cantu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "dl-playground/understanding-bn/index.html",
    "href": "dl-playground/understanding-bn/index.html",
    "title": "Understanding Batch Normalization",
    "section": "",
    "text": "The paper investigates the cause of batch norm’s benefits experimentally. The authors show that its main benefit is allowing for larger learning rates during training. In particular:\n\n“We show that the activations and gradients in deep neural networks without BN tend to be heavy-tailed. In particular, during an early on-set of divergence, a small subset of activations (typically in deep layer) “explode”. The typical practice to avoid such divergence is to set the learning rate to be sufficiently small such that no steep gradient direction can lead to divergence. However, small learning rates yield little progress along flat directions of the optimization landscape and may be more prone to convergence to sharp local minima with possibly worse generalization performance.”\n\nWe attempt to reproduce figures 1-3, 5, and 6.\n\nConvolutional BN Layer\nAs a reminder, the input \\(I\\) and output \\(O\\) tensors to a batch norm layer are 4 dimensional. The dimensions \\((b, c, x, y)\\) correspond to the batch example, channel, and spatial \\(x\\), \\(y\\) dimensions respectively. Batch norm (BN) applies a channel-wise normalization:\n\\[\nO_{b, c, x, y} \\leftarrow \\gamma_c \\frac{I_{b, c, x, y} - \\hat \\mu_c}{\\sqrt{\\hat \\sigma_c^2 + \\epsilon}} + \\beta_c\n\\]\nWhere \\(\\hat \\mu_c\\) and \\(\\hat \\sigma_c^2\\) are estimates channel \\(c\\)’s mean and standard deviation computed on the minibatch \\(\\mathcal B\\):\n\\[\n\\hat \\mu_c = \\frac{1}{|\\mathcal B|}\\sum_{b, x, y} I_{b, c, x, y}\n\\]\n\\[\n\\hat \\sigma_c^2 = \\frac{1}{\\mathcal |B|} \\sum_{b, x, y} (I_{b, c, x, y} - \\hat \\mu_c) ^ 2\n\\]\nTo make sure the layer does not lose expressive power we introduce learned parameters \\(\\gamma_c\\) and \\(\\beta_c\\). \\(\\epsilon\\) is a small constant added for numerical stability. In pytorch, we can simply use the BatchNorm2d layer.\n\n\nExperimental setup\nLet’s set up our data loaders, model, and training loop as described in Appendix B of the paper.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os, itertools, time\n\nos.makedirs('logs', exist_ok = True)\nos.makedirs('models', exist_ok = True)\n\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\ndevice = torch.device(\n    'cuda' if torch.cuda.is_available() else\n    ('mps' if torch.backends.mps.is_available() else\n    'cpu')\n)\n\ndef eval_model(model, test, criterion = nn.CrossEntropyLoss()):\n    model.eval()\n    correct, loss = 0, 0.0\n    with torch.no_grad():\n        for images, labels in test:\n            images, labels = images.to(device), labels.to(device)\n            _, pred = torch.max(model(images), 1)\n            correct += (pred == labels).float().sum().item()\n            loss += criterion(model(images), labels).item()\n    return loss / len(test.dataset), correct / len(test.dataset)\n\ndevice\n\nThe paper trains ResNet-110s on CIFAR-10, with channel-wise normalization, random horizontal flipping, and 32-by-32 cropping with 4-pixel zero padding. We’ll train the ResNet-101 included in torchvision but keep everything the same.\nWe first get the datasets and compute the channel-wise means and variances. Note: both the training and validation set have the same values.\n\ntrain_set = datasets.CIFAR10('./data', download = True, train = True, transform = transforms.ToTensor())\nval_set = datasets.CIFAR10('./data', download = True, train = False, transform = transforms.ToTensor())\n\ndef channel_means_stds(dataset):\n    imgs = torch.stack([img for img, _ in train_set])\n    return imgs.mean(dim = [0, 2, 3]), imgs.std(dim = [0, 2, 3])\n\nmeans, stds = channel_means_stds(train_set)\nprint(f'Training channel-wise\\n\\tmeans: {means}\\n\\tstds: {stds}')\n\nmeans, stds = channel_means_stds(val_set)\nprint(f'Validation channel-wise\\n\\tmeans: {means}\\n\\tstds: {stds}')\n\nWe now define the transforms with data augmentation and data loaders with batch size \\(128\\).\n\ntrain_transform = transforms.Compose([\n    transforms.RandomCrop(32, padding = 4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(means, stds),\n])\n\n# We do not perform data augmentation on the validation set\nval_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize(means, stds),\n])\n\ntrain_set.transform = train_transform\nval_set.transform = val_transform\n\ntrain_loader = DataLoader(train_set, batch_size = 128, shuffle = True)\nval_loader = DataLoader(val_set, batch_size = 128, shuffle = False)\n\nWe’ll use torchvision’s implementation of ResNet-101, Xavier initialization, SGD with momentum \\(0.9\\) and weight decay \\(5\\times 10^{-4}\\), and cross-entropy loss. We try to implement the training details and learning rate scheduling as mentioned in the paper:\n\n“Initially, all models are trained for 165 epochs and as in [17] we divide the learning rate by 10 after epoch 50% and 75%, at which point learning has typically plateued. If learning doesn’t plateu for some number of epochs, we roughly double the number of epochs until it does”.\n\n\ndef xavier_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        nn.init.xavier_uniform_(m.weight)\n\ndef train_epoch(model, train, optimizer, criterion):\n    # Trains the model for one epoch\n    model.train()\n    train_loss, correct = 0.0, 0\n    for images, labels in train:\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n        output = model(images)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n        _, pred = torch.max(output, 1)\n        correct += (pred == labels).float().sum().item()\n    return train_loss / len(train.dataset), correct / len(train.dataset)\n\n\ndef train(model, train, val, init_lr, plateau_patience = 20):\n\n    optimizer = optim.SGD(model.parameters(), lr = init_lr, momentum = 0.9, weight_decay = 5e-4)\n    scheduler  = optim.lr_scheduler.MultiStepLR(optimizer, milestones = [82, 123], gamma = 0.1)\n    criterion = nn.CrossEntropyLoss()\n\n    model.to(device)\n\n    init_epochs = 165\n\n    epoch = 0\n    plateau_count = 0\n    best_loss = None\n\n    train_losses, train_accs, val_losses, val_accs = [], [], [], []\n\n    while epoch &lt; init_epochs and plateau_count &lt; plateau_patience:\n\n        # Train the model for an epoch\n        loss, acc = train_epoch(model, train, optimizer, criterion)\n        train_losses.append(loss)\n        train_accs.append(acc)\n\n        # Evaluate the model on the validation set\n        val_loss, val_acc = eval_model(model, val, criterion)\n        val_losses.append(val_loss)\n        val_accs.append(val_acc)\n\n        # Update the learning rate\n        scheduler.step(val_loss)\n\n        # Check for a plateau\n        if best_loss is None or val_loss &lt; best_loss:\n            best_loss = val_loss\n            plateau_count = 0\n        else:\n            plateau_count += 1\n        \n        epoch += 1\n\n        # \"If learning doesn’t plateu for some number of epochs,\n        # we roughly double the number of epochs until it does.\"\n        if epoch == init_epochs and plateau_count &lt; plateau_patience:\n            init_epochs *= 2\n\n        print(f'Epoch {epoch}/{init_epochs} | Learning Rate: {optimizer.param_groups[0][\"lr\"]} | '\n              f'Training loss: {train_losses[-1]:.4f} | '\n              f'Validation loss: {val_losses[-1]:.4f} | '\n              f'Validation accuracy: {val_accs[-1]:.4f}')\n        \n    return train_losses, train_accs, val_losses, val_accs\n\nAnd we define a function to disable batch norm layers in a model by replacing them with identity layers:\n\ndef disable_bn(model):\n    for name, module in model.named_children():\n        if isinstance(module, nn.BatchNorm2d):\n            setattr(model, name, nn.Identity())\n        else:\n            disable_bn(module)  # Recursively replace in child modules\n\n\n\nFig 1\nFigure 1 aims to demonstrate that batch norm’s primary benefit is that it allows training with larger learning rates.\nThe authors find the highest (initial) learning rate with which they can train an unnormalized model (\\(\\alpha = 0.0001\\)) and compare its performance with normalized models trained with \\(\\alpha \\in \\{0.0001, 0.003, 0.1\\}\\). We train each model once (instead of five times to save on compute) and present train and test accuracy curves.\n\nMODELS_DIR = 'models'\nLOGS_DIR = 'logs'\n\nfor lr, bn in [(0.0001, False), (0.0001, True), (0.003, True), (0.1, True)]:\n\n    s = f'lr={lr}' + ('_bn' if bn else '')\n    print(s)\n\n    model = models.resnet101(num_classes = 10)\n    model.apply(xavier_init)\n\n    if not bn: disable_bn(model)\n\n    torch.save(model, f'{MODELS_DIR}/{s}_init.pth')\n    data = train(model, train_loader, val_loader, init_lr = lr)\n    torch.save(model, f'{MODELS_DIR}/{s}_end.pth')\n    torch.save(data, f'{LOGS_DIR}/{s}.pth')\n\n\n# code-summary: Plot results\n\nget_x = lambda x: 100 * np.arange(1, len(x) + 1) / len(x)\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (9, 4), sharey = True)\n\nfor fname in os.listdir(LOGS_DIR):\n\n    train_losses, train_accs, val_losses, val_accs = torch.load(f'{LOGS_DIR}/{fname}')\n    ax1.plot(get_x(train_accs), train_accs, label = fname[:-4])\n    ax2.plot(get_x(val_accs), val_accs, label = fname[:-4])\n    print(f'{fname[:-4]} took {len(train_accs)} epochs')\n\n\nax1.legend(); ax2.legend()\nax1.set_ylabel('Training accuracy'); ax2.set_ylabel('Validation accuracy')\nax1.set_xlabel('% of training'); ax2.set_xlabel('% of training')\nf.tight_layout()\n\nlr=0.1_bn took 83 epochs\nlr=0.0001 took 263 epochs\nlr=0.003_bn took 69 epochs\nlr=0.0001_bn took 211 epochs\n\n\n\n\n\n\n\n\n\nAnd observe the same general trends found in the original paper: similar learning rates result in about the same performance (red and orange) while increasing the rate yields better performance for normalized networks (blue) and training diverges for non-normalized ones (not shown).\n\n\nFig 2\nIn Figure 2 the authors begin to investigate “why BN facilitates training with higher learning rates in the first place”. The authors claim that batch norm (BN) prevents divergence during training, which usually occurs because of large gradients in the first mini-batches.\nSo, the authors analyze the gradients at initialization of a midpoint layer (55) with and without batch norm. They find that gradients in unnormalized networks are consistently larger and distributed with heavier tails.\nI had trouble replicating this figure. I could not obtain the general shape and scale of the histograms they did:\n\n\n\n\n\nAt first, I thought because I was\n\nlooking at the wrong layer, +- 1 (then found it made little difference)\nlogging the gradient magnitudes incorrectly - why does the plot have negative values? (then found the authors plot the raw gradient)\nmisunderstanding the whole process\n\nAs I understood it, we initialize the model (using Xavier’s initialization), do a forward and backward pass on a single batch, and log the gradients at roughly the middle layer:\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3))\nimages, labels = next(iter(val_loader))\nimages, labels = images.to(device), labels.to(device)\n\nfor bn, ax in zip([True, False], [ax1, ax2]):\n\n    # Init\n    model = models.resnet101(num_classes = 10)\n    model.to(device)\n    if not bn: disable_bn(model)\n    model.apply(xavier_init)\n\n    # Forward and backward pass\n    model.train()\n    output = model(images)\n    loss = nn.CrossEntropyLoss()(output, labels)\n    loss.backward()\n\n    model.eval()\n    grads = model.layer3[9].conv1.weight.grad.view(-1).cpu().detach().numpy()\n    sns.histplot(grads, ax = ax)\n\nax1.set_title('With Batch Normalization')\nax2.set_title('Without Batch Normalization')\nax2.set_ylim((0, 2500)); ax2.set_ylabel('')\nf.tight_layout()\n\n\n\n\n\n\n\n\nAlthough the unnormalized gradients are heavy-tailed, they are still much smaller than the normalized ones. I was stuck on this issue for a few days until I experimented with different initializations:\n\ndef init_func(f, also_linear = True):\n    def init(m):\n        if isinstance(m, nn.Conv2d):\n            f(m.weight)\n        if also_linear and isinstance(m, nn.Linear):\n            f(m.weight)\n    return init\n\ncriterion = nn.CrossEntropyLoss()\nimages, labels = next(iter(val_loader))\nimages, labels = images.to(device), labels.to(device)\n\nwith_bn, without_bn = {}, {}\n\nfor init_f in [\n    nn.init.xavier_uniform_, nn.init.kaiming_uniform_,\n    nn.init.kaiming_normal_, lambda x: nn.init.kaiming_normal_(x, mode = 'fan_out', nonlinearity='relu')]:\n\n    init_name = init_f.__name__[:-1]\n    if 'lambda' in init_name: init_name = '(default) kaiming_normal fan_out'\n    \n    for linear in (True, False):\n\n        lin_name = ' w/linear lyrs' if linear else ''\n        \n        for bn, d in zip((True, False), (with_bn, without_bn)):\n                \n                model = models.resnet101(num_classes = 10)\n                model.to(device)\n                if not bn: disable_bn(model)\n                model.apply(init_func(init_f, linear))\n\n                model.train()\n                output = model(images)\n                loss = criterion(output, labels)\n                loss.backward()\n\n                model.eval()\n                d[init_name + lin_name] = model.layer3[9].conv1.weight.grad.view(-1).cpu().detach().numpy()\n\n\nfor init_name, v in with_bn.items():\n    f, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3))\n    sns.histplot(v, ax = ax1)\n    ax1.set_title('with BN')\n    sns.histplot(without_bn[init_name], ax = ax2)\n    ax2.set_title('without BN')\n    ax2.set_ylabel('')\n    ax2.set_ylim(0, 1500)\n    f.suptitle(init_name)\n    f.tight_layout()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAs you can see the general shapes, normal vs heavy-tailed don’t depend that much on the initialization scheme but the scales do. We could only achieve the same scale of the gradients presented in the paper by using the kaiming_normal scheme with fan=out (to preserve the magnitudes of the variance of the weights in the backward pass instead of the forward one) and applying it only to Conv2 layers. This is the default used by torchvision’s resnets.\nNote: xavier_normal produced very similar shapes/scales as xavier_uniform so we don’t show it.\nFor the rest of the figures we’ll use the default init scheme:\n\ndef init_net(m):\n    if isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode = 'fan_out', nonlinearity = 'relu')\n\n\n\nFig 3\nThe authors then investigate the loss landscape along the gradient direction for the first few mini-batches for models with BN (trained with \\(\\alpha = 0.1\\)) and without BN (\\(\\alpha = 0.0001\\)). For each network and mini-batch they compute the gradient and plot the relative change in the loss (new_loss/old_loss).\nWe save the model’s and optimizer’s states (state_dict) before taking the tentative steps to explore the landscape and restore them before taking the actual step between batches.\n\ndef fig3(model, init_lr, log_lrs, log_batches = [1, 4, 7]):\n\n    # batch -&gt; list of relative losses for each lr\n    out = {}\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(model.parameters(), lr = init_lr, momentum = 0.9, weight_decay = 5e-4)\n\n    for i, (images, labels) in enumerate(train_loader):\n\n        images, labels = images.to(device), labels.to(device)\n\n        if i in log_batches:\n            \n            torch.save(model.state_dict(), f'models/model_state.tmp')\n            torch.save(optimizer.state_dict(), f'models/optimizer_state.tmp')\n\n            rel_losses = []\n            for lr in log_lrs:\n\n                for param_group in optimizer.param_groups: param_group['lr'] = lr\n\n                optimizer.zero_grad()\n                output = model(images)\n                current_loss = criterion(output, labels)\n                current_loss.backward()\n                optimizer.step()\n\n                with torch.no_grad():\n                    output = model(images)\n                    tmp_loss = criterion(output, labels)\n                    rel_loss = (tmp_loss / current_loss).item()\n                \n                    # print learning rate, current loss, tmp loss, relative loss (at 4 decimal places)\n                    print(f'{lr:.5f} {current_loss:.5f} {tmp_loss:.5f} {rel_loss:.5f}')\n                    rel_losses.append(rel_loss)\n            \n                model.load_state_dict(torch.load('models/model_state.tmp'))\n                optimizer.load_state_dict(torch.load('models/optimizer_state.tmp'))   \n\n                # If loss is nan of int, break. Unlikely to recover.\n                if torch.isnan(tmp_loss).item() or torch.isinf(tmp_loss).item():\n                    rel_losses.pop()\n                    print('breaking')\n                    break\n\n            out[i] = rel_losses\n        \n        if i == max(log_batches): break\n\n        # take the actual step\n        optimizer.zero_grad()\n        output = model(images)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n    \n    return out\n\n\nlrs = np.logspace(-5.5, 0.5, 80)\n\n# With BN\nmodel = models.resnet101(num_classes = 10)\nmodel.to(device)\nmodel.apply(init_net)\nwith_bn = fig3(model, init_lr = 0.1, log_lrs = lrs)\n\n# Without BN\nmodel = models.resnet101(num_classes = 10)\nmodel.to(device)\ndisable_bn(model)\nmodel.apply(init_net)\nwithout_bn = fig3(model, init_lr = 0.0001, log_lrs = lrs)\n\n\nf, axs = plt.subplots(2, 3, figsize = (12, 6))\n\nfor (batch, rel_losses), ax in zip(with_bn.items(), axs[0]):\n    ax.plot(lrs, rel_losses)\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n    ax.set_ylim(0, 1.5)\n    ax.set_xlim(10**-5.5, 0.4)\n    ax.set_title(f'Batch {batch} with BN')\n    ax.axhline(1, color = 'grey', linestyle = '--')\n\nfor (batch, rel_losses), ax in zip(without_bn.items(), axs[1]):\n    ax.plot(lrs[:len(rel_losses)], rel_losses)\n    ax.set_xscale('log')\n    ax.set_yscale('log')\n    ax.set_ylim(10**-2, 10**3)\n    ax.set_xlim(10**-5.5, 0.4)\n    ax.set_title(f'Batch {batch} w/o BN')\n    ax.axhline(1, color = 'grey', linestyle = '--')\n\naxs[0, 0].set_ylabel('Relative loss')\naxs[1, 0].set_ylabel('Relative loss')\nf.tight_layout()\n\n\n\n\n\n\n\n\nAlthough we get roughly different scales, we observe that unnormalized networks reduce the loss only with small steps while normalized ones can improve with a much larger range, as in the paper.\n\n\nFig 5\nFigures 5 and 6 explore the behavior of networks at initialization. Figure 5 displays the mean and variances of channels in the network as a function of depth at initialization. We initialize \\(10\\) networks and use forward hooks to log their channel mean and standard deviations.\n\ndf = []\n\ndef log_activation_stats(layer_name, key):\n    def hook(module, input, output):\n        with torch.no_grad():\n            df.append({\n                'bn': key,\n                'layer': layer_name,\n                'mean': output[:, 0, :, :].mean().abs().item(),\n                'std': output[:, 0, :, :].std().abs().item()\n            })\n    return hook\n\n\nfor _ in range(10):\n\n    for bn in [True, False]:\n\n        model = models.resnet101(num_classes = 10)\n        model.to(device)\n        if not bn: disable_bn(model)\n        model.apply(init_net)\n\n        # Layers to log activations from\n        log_layers = [] # (n, name, layer)\n        n = 1\n        for name, layer in model.named_modules():\n            if 'conv' in name:\n                n += 1\n                if n in list(range(5, 101 + 12, 12)):\n                    log_layers.append((n, name, layer))\n\n        for n, _, layer in log_layers: layer.register_forward_hook(log_activation_stats(n, str(bn)))\n        for images, _ in val_loader: model(images.to(device))\n\ndf = pd.DataFrame(df)\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (9, 4))\n\nsns.lineplot(data = df, x = 'layer', y = 'mean', hue = 'bn', ax = ax1)\nax1.set_yscale('log')\n\nsns.lineplot(data = df, x = 'layer', y = 'std', hue = 'bn', ax = ax2)\nax2.set_yscale('log')\n\nf.tight_layout()\n\n\n\n\n\n\n\n\nWe observe, consistent with the findings in the paper, that activation means and standard deviations increase almost exponentially in non-normalized networks, whereas they remain nearly constant in normalized networks.\n\n\nFig 6\nThe large activations in the final layers for unnormalized networks in the previous figure make us suspect that networks are biased towards a class. The authors investigate whether this is the case by looking at the gradients in the final (output) layer across images in a mini-batch and classes.\nNote: Don’t confuse this with the last fully connected layer of the network. We are looking at the gradients of the output logits themselves. We need to use retain_grad on the output (non-leaf node) to calculate its gradient on the backward pass.\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3), sharey = True, sharex = True)\n\nimages, labels = next(iter(val_loader))\nimages, labels = images.to(device), labels.to(device)\n\nfor bn, ax in zip([False, True], [ax1, ax2]):\n\n    model = models.resnet101(num_classes = 10)\n    model.to(device)\n    if not bn: disable_bn(model)\n    model.apply(init_net)\n\n    out = model(images)\n    out.retain_grad()\n    loss = nn.CrossEntropyLoss()(out, labels)\n    loss.backward()\n\n    ax = sns.heatmap(out.grad.cpu().detach().numpy(), cmap = 'viridis', ax = ax)\n    ax.set_xticks([]); ax.set_yticks([0, 40, 80, 120]); ax.set_yticklabels([0, 40, 80, 120])\n    ax.set_xlabel('Classes')\n    ax.set_ylabel('Images in batch' if not bn else '')\n    ax.set_title('With BN' if bn else 'Without BN')\n\nf.tight_layout()\n\n\n\n\n\n\n\n\nAnd basically observe the same results as the paper:\n\n“A yellow entry indicates that the gradient is positive, and the step along the negative gradient would decrease the prediction strength of this class for this particular image. A dark blue entry indicates a negative gradient, indicating that this particular class prediction should be strengthened. Each row contains one dark blue entry, which corresponds to the true class of this particular image (as initially all predictions are arbitrary). A striking observation is the distinctly yellow column in the left heatmap (network without BN). This indicates that after initialization the network tends to almost always predict the same (typically wrong) class, which is then corrected with a strong gradient update. In contrast, the network with BN does not exhibit the same behavior, instead positive gradients are distributed throughout all classes.”\n\nRunning the above code multiple times, however, sometimes results in two or three yellow columns. We think this is because different mini-batches behave slightly differently or due to initialization randomness. Below, we log and average the gradients for a whole epoch and find much more consistent behavior.\n\ndef fig6(init_func = init_net):\n\n    f, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 3), sharex = True, sharey = True)\n\n    for bn, ax in zip([False, True], [ax1, ax2]):\n\n        model = models.resnet101(num_classes = 10)\n        model.to(device)\n        if not bn: disable_bn(model)\n        model.apply(init_func)\n\n        avg_grad = torch.zeros((128, 10), device = device)\n\n        for images, labels in val_loader:\n            images, labels = images.to(device), labels.to(device)\n            out = model(images)\n            out.retain_grad()\n            loss = nn.CrossEntropyLoss()(out, labels)\n            loss.backward()\n            if out.grad.shape == avg_grad.shape: avg_grad += out.grad\n        \n        avg_grad /= len(val_loader)\n\n        ax = sns.heatmap(avg_grad.cpu().detach().numpy(), cmap = 'viridis', ax = ax)\n        ax.set_xlabel('Classes')\n        ax.set_ylabel('Images in batch')\n        ax.set_title('With BN' if bn else 'Without BN')\n\n    f.tight_layout()\n\nfig6()\n\n\n\n\n\n\n\n\nAnd that’s about it.\n\n\nWhat I learned / practiced\nI gained a better understanding and intuition of why Batch Normalization (BN) works. More importantly, I got comfortable with PyTorch and debugging training, etc.\nPytorch specific:\n\nBasics of image augmentation: basically use transforms and compose them.\nLearning rate schedulers: they exist, are really useful, and pytorch has a good assortment of them.\nstate_dict preserves optimizer’s param groups and args (learning rates, etc.) but also momentum buffers.\nhooks as useful debugging and visualization tools.\nretain_grad is required to get gradients of non-leaf nodes like the output logits.\n\nFor the large training runs, I also experimented with jarvislabs.ai as a provider. In-browser notebooks and VS Code, and direct SSH/FTP access were pretty nice. I could not work out funkiness with VS Code remote windows. Used\nnohup jupyter nbconvert --execute --to notebook --inplace --allow-errors main.ipynb &\nto run the notebook, write results, and be able to close the Jupyter / VS Code tab."
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html",
    "href": "dl-playground/d2l-exercises/7-6.html",
    "title": "LeNet exercises",
    "section": "",
    "text": "Where I attempt to solve the exercises in section 7.6 of the d2l book from scratch in pytorch (without using the d2l library).\n!pip3 install matplotlib\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os, itertools, time\n\ndevice = 'cuda'"
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html#context",
    "href": "dl-playground/d2l-exercises/7-6.html#context",
    "title": "LeNet exercises",
    "section": "Context",
    "text": "Context\nThe book section ties together convolution/pooling layer concepts seen in previous sections and introduces the original LeNet architecture:\nWhich is succinctly defined in PyTorch as\nnn.Sequential(\n    nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n    nn.AvgPool2d(kernel_size=2, stride=2),\n    nn.Flatten(),\n    nn.LazyLinear(120), nn.Sigmoid(),\n    nn.LazyLinear(84), nn.Sigmoid(),\n    nn.LazyLinear(num_classes)\n)\nWithout forgetting to use uniform Xavier initialization. Let’s define the net:\n\nclass LeNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.LazyLinear(120), nn.Sigmoid(),\n            nn.LazyLinear(84), nn.Sigmoid(),\n            nn.LazyLinear(num_classes)\n        )\n\n    def forward(self, X):\n        return self.net(X)\n\n# Used later on to initialize the weights\ndef init_weights(m):\n    if isinstance(m, nn.Linear) or type(m) == nn.Conv2d:\n        torch.nn.init.xavier_uniform_(m.weight)\n\nAnd try to train it without using the d2l library.\nWe first get our dataset and dataloaders. We’ll wrap it in a function for convinience later.\n\ndef get_data(dataset, batch_size):\n    \n    data_params = {'root': 'data', 'transform': transforms.ToTensor(), 'download': True}\n    train_dataset = dataset(train = True, **data_params)\n    test_dataset = dataset(train = False, **data_params)\n    \n    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n\n    return train_dataset, test_dataset, train_loader, test_loader\n\n_, _, train_loader, test_loader = get_data(datasets.FashionMNIST, 128)\n\nAnd functions to evaluate our models and plot losses.\n\ndef eval_model(model, test_loader):  \n    model.eval()\n    correct = 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            _, pred = torch.max(model(images), 1)\n            correct += (pred == labels).float().sum().item()\n    return correct / len(test_loader.dataset)\n\ndef plot_results(losses, test_acc):\n    plt.figure()\n    plt.plot(losses)\n    plt.xlabel('Epoch'); plt.ylabel('Cross Entropy Loss')\n    plt.title(f'Test Accuracy: {test_acc:.2f}')\n\nAnd finally train it using SGD with learning rate 0.1 for 15 epochs\n\ndef train(net, train_loader, lr = 0.1, epochs = 15, verbose = True):\n\n    # Infer input shapes, initialize weights and move to device\n    _ = net(next(iter(train_loader))[0]) # Necessary before initing weights\n    net.apply(init_weights)\n    net.to(device)\n    net.train()\n    \n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr = lr)\n\n    losses = []\n    for epoch in range(epochs):\n    \n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(net(images), labels)\n            loss.backward()\n            optimizer.step()\n            \n        losses.append(loss.item())\n        if verbose: print(f'Epoch: {epoch + 1} \\tLoss: {loss.item():.2f}')\n\n    return losses\n        \nnet = LeNet(10)\nlosses = train(net, train_loader, verbose = False)\ntest_acc = eval_model(net, test_loader) \nplot_results(losses, test_acc)\n\n\n\n\n\n\n\n\nAnd we achieve reasonable performance. Let’s now attempt the section questions."
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html#q1",
    "href": "dl-playground/d2l-exercises/7-6.html#q1",
    "title": "LeNet exercises",
    "section": "Q1",
    "text": "Q1\nLet’s modernize LeNet. Implement and test the following changes:\n\nReplace average pooling with max-pooling.\nReplace the softmax layer with ReLU.\n\nWe define another module and replace nn.Sigmoid’s for nn.ReLu’s and nn.AvgPool2d’s for nn.MaxPool2d’s:\n\nclass ModernLeNet(nn.Module):\n\n    def __init__(self, num_classes):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.LazyLinear(120), nn.ReLU(),\n            nn.LazyLinear(84), nn.ReLU(),\n            nn.LazyLinear(num_classes)\n        )\n\n    def forward(self, X):\n        return self.net(X)\n\n\nnet = ModernLeNet(10)\nlosses = train(net, train_loader, verbose = False)\ntest_acc = eval_model(net, test_loader) \nplot_results(losses, test_acc) \n\n\n\n\n\n\n\n\nAnd we achieve a non-trivial improvement in performance. We even observed the loss increase slightly, indicating that our learning rate it too high (or we should decrease it on a schedule)."
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html#q2",
    "href": "dl-playground/d2l-exercises/7-6.html#q2",
    "title": "LeNet exercises",
    "section": "Q2",
    "text": "Q2\nTry to change the size of the LeNet style network to improve its accuracy in addition to max-pooling and ReLU.\n\nAdjust the convolution window size.\n\nLets make everything a parameter:\n\nclass TweakableModernLeNet(nn.Module):\n\n    def __init__(\n            self, num_classes, conv_kernel = 5, out_channels = [6, 16],\n            hidden_dims = [120, 84]):\n        \n        super().__init__()\n        layers = [\n            nn.Sequential(\n            nn.LazyConv2d(out_c, kernel_size=conv_kernel, padding=2), nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)) for out_c in out_channels\n        ] + [nn.Flatten()] + [\n            nn.Sequential(nn.LazyLinear(dim), nn.ReLU()) for dim in hidden_dims\n        ] + [nn.LazyLinear(num_classes)]\n    \n        self.net = nn.Sequential(*layers)\n\n    def forward(self, X):\n        return self.net(X)\n\nLet’s try adjusting the convolution window size \\(c_w \\in \\{3,..,7\\}\\). We’ll train 5 nets per size and report the average:\n\nn_avg = 5\nconv_sizes = range(3, 7 + 1)\ntest_accs = []\nfor conv_size in conv_sizes:\n    avg = 0\n    for _ in range(n_avg):\n        net = TweakableModernLeNet(10, conv_kernel = conv_size)\n        train(net, train_loader, verbose = False)\n        avg += eval_model(net, test_loader)\n    test_accs.append(avg / n_avg)\n\nplt.plot(conv_sizes, test_accs)\nplt.xlabel('Convolution kernel size'); plt.ylabel('Test Accuracy')\n\nText(0, 0.5, 'Test Accuracy')\n\n\n\n\n\n\n\n\n\nLooking at the y-axis, it seems the kernel size has only a small effect on performance in this case, and that 5 is a reasonable choice.\n\nAdjust the number of output channels.\n\nWe can try halving, doubling and tripling the original [6, 16] output channels:\n\nn_avg = 3\nout_channels = [[3, 8], [6, 16], [12, 22], [24, 44]]\ndf = []\nfor out_c in out_channels:\n    avg = 0\n    for _ in range(n_avg):\n        net = TweakableModernLeNet(10, out_channels = out_c)\n        train(net, train_loader, verbose = False)\n        avg += eval_model(net, test_loader)\n    df.append({'Output channels': out_c, 'Test Accuracy': avg / n_avg})\n\ndf = pd.DataFrame(df)\ndf\n\n\n\n\n\n\n\n\nOutput channels\nTest Accuracy\n\n\n\n\n0\n[3, 8]\n0.885500\n\n\n1\n[6, 16]\n0.894867\n\n\n2\n[12, 22]\n0.901233\n\n\n3\n[24, 44]\n0.908233\n\n\n\n\n\n\n\nAgain, the gains seem marginal and increase compute so the original seems reasonable.\n\nAdjust the number of convolution layers.\n\nLets try adding up to 2 additional convolution layers:\n\nn_avg = 3\nout_channels = [[6, 16], [6, 16, 22], [6, 16, 22, 28]]\ntest_accs = []\nfor out_c in out_channels:\n    avg = 0\n    for _ in range(n_avg):\n        net = TweakableModernLeNet(10, out_channels = out_c)\n        train(net, train_loader, verbose = False)\n        avg += eval_model(net, test_loader)\n\n    test_accs.append(avg / n_avg)\n\nplt.plot([len(i) for i in out_channels], test_accs)\nplt.xlabel('# of convolution layers'); plt.ylabel('Test Accuracy')\n\nText(0, 0.5, 'Test Accuracy')\n\n\n\n\n\n\n\n\n\nSame as above (see y-axis).\n\nAdjust the number of fully connected layers.\n\nLets try 1, the original 2, 4, and 6 layers:\n\nn_avg = 3\nhidden_dims = [[84], [120, 84], [120, 120, 84, 84], [120, 120, 120, 84, 84, 84]]\ntest_accs = []\nfor hid_dims in hidden_dims:\n    avg = 0\n    for _ in range(n_avg):\n        net = TweakableModernLeNet(10, hidden_dims = hid_dims)\n        train(net, train_loader, verbose = False)\n        avg += eval_model(net, test_loader)\n    test_accs.append(avg / n_avg)\n\nplt.plot([len(i) for i in hidden_dims], test_accs)\nplt.xlabel('# of FL layers'); plt.ylabel('Test Accuracy')\n\nText(0, 0.5, 'Test Accuracy')\n\n\n\n\n\n\n\n\n\nSame.\n\nAdjust the learning rates and other training details (e.g., initialization and number of epochs).\n\nLets first experiment with different learning rates and try \\(\\{0.01, 0.05, 0.1, 0.5\\}\\):\n\nlrs = [0.01, 0.05, 0.1, 0.5]\ntest_accs = []\nfor lr in lrs:\n    net = TweakableModernLeNet(10)\n    train(net, train_loader, verbose = False, lr = lr)\n    test_accs.append(eval_model(net, test_loader))\n\nplt.plot(lrs, test_accs)\nplt.xlabel('Learning Rate'); plt.ylabel('Test Accuracy')\nplt.xscale('log')\n\n\n\n\n\n\n\n\nIt seams only 0.5 is too high and that our original 0.1 performs well."
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html#q3",
    "href": "dl-playground/d2l-exercises/7-6.html#q3",
    "title": "LeNet exercises",
    "section": "Q3",
    "text": "Q3\nTry out the improved network on the original MNIST dataset\nWe can reuse code from above:\n\n_, _, train_loader_MNIST, test_loader_MNIST = get_data(datasets.MNIST, 128)\nnet = ModernLeNet(10)\nlosses = train(net, train_loader, verbose = False, lr = 0.01)\ntest_acc = eval_model(net, test_loader) \nplot_results(losses, test_acc)\n\nEpoch: 1    Loss: 0.75\nEpoch: 2    Loss: 0.53\nEpoch: 3    Loss: 0.62\nEpoch: 4    Loss: 0.31\nEpoch: 5    Loss: 0.40\nEpoch: 6    Loss: 0.50\nEpoch: 7    Loss: 0.31\nEpoch: 8    Loss: 0.52\nEpoch: 9    Loss: 0.49\nEpoch: 10   Loss: 0.47\nEpoch: 11   Loss: 0.45\nEpoch: 12   Loss: 0.38\nEpoch: 13   Loss: 0.44\nEpoch: 14   Loss: 0.29\nEpoch: 15   Loss: 0.30\n\n\n\n\n\n\n\n\n\nAnd we get good performance."
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html#q4",
    "href": "dl-playground/d2l-exercises/7-6.html#q4",
    "title": "LeNet exercises",
    "section": "Q4",
    "text": "Q4\nDisplay the activations of the first and second layer of LeNet for different inputs (e.g., sweaters and coats)\nLets retrain the ModernLeNet on FashionMNIST:\n\nnet = ModernLeNet(10)\nlosses = train(net, train_loader, verbose = False)\ntest_acc = eval_model(net, test_loader) \nprint('Test acc: ', test_acc)\n\nTest acc:  0.8936\n\n\nAnd now get a few sweater and coat images:\n\ntrain_dataset, _, train_loader, _ = get_data(datasets.FashionMNIST, 128)\nimgs, ys = next(iter(train_loader))\n\ncoat_ix = train_dataset.classes.index('Coat')\nsweater_ix = train_dataset.classes.index('Pullover')\n\ncoats_ix = [ix for ix, y in enumerate(ys) if y == coat_ix][:n]\nsweaters_ix = [ix for ix, y in enumerate(ys) if y == sweater_ix][:n]\n\nRemmember that the ReLUs are at indeces 1 and 4:\n\nnet\n\nModernLeNet(\n  (net): Sequential(\n    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): ReLU()\n    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (3): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n    (4): ReLU()\n    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (6): Flatten(start_dim=1, end_dim=-1)\n    (7): Linear(in_features=400, out_features=120, bias=True)\n    (8): ReLU()\n    (9): Linear(in_features=120, out_features=84, bias=True)\n    (10): ReLU()\n    (11): Linear(in_features=84, out_features=10, bias=True)\n  )\n)\n\n\nWe can subscript into Sequential to obtain the activations and each channel along with the input:\n\n# Prepare img: add minibatch dim and move to device\ndef disp_acts(end_layer, images = None):\n\n    if images is None: images = [imgs[coats_ix[0]], imgs[sweaters_ix[0]]]\n    \n    for name, og_img in zip(['Coat', 'Sweater'], images):\n    \n        img = og_img[None, :].to(device)  \n        activation = net.net[:end_layer](img)\n        n_channels = activation.shape[1]\n        \n        f, axs = plt.subplots(1, n_channels + 1, figsize = (8,5))\n        axs[0].imshow(og_img.permute(1, 2, 0))\n        axs[0].set_xticks([]); axs[0].set_yticks([]); axs[0].set_title(name)\n        for ax, channel in zip(axs[1:], range(n_channels)):\n            ax.imshow(activation.squeeze().permute(1, 2, 0)[:, :, channel].detach().to('cpu').numpy())\n            ax.set_title(str(channel))\n            ax.set_xticks([])\n            ax.set_yticks([])\n        \n        f.tight_layout()\n        f.show()\n\ndisp_acts(1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndisp_acts(4)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd we observe that the first activation layer seems to detect lines, edges, etc. - low-level features, while the second activation layer is slightly more abstract features (although they are hard to interpret)."
  },
  {
    "objectID": "dl-playground/d2l-exercises/7-6.html#q5",
    "href": "dl-playground/d2l-exercises/7-6.html#q5",
    "title": "LeNet exercises",
    "section": "Q5",
    "text": "Q5\nWhat happens to the activations when you feed significantly different images into the network (e.g., cats, cars, or even random noise)?\nLets try inputing random noise:\n\ndisp_acts(1, [torch.randn((1, 28, 28)), torch.randn((1, 28, 28))])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndisp_acts(4, [torch.randn((1, 28, 28)), torch.randn((1, 28, 28))])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnd the activations seem completely random, as we would expect."
  },
  {
    "objectID": "dl-playground/nn-memorization/index.html",
    "href": "dl-playground/nn-memorization/index.html",
    "title": "A Closer Look at Memorization in Deep Networks",
    "section": "",
    "text": "This paper argues that memorization is a behavior exhibited by networks trained on random data, as, in the absence of patterns, they can only rely on remembering examples. The authors investigate this phenomenon and make three key claims:\nHere we aim to reproduce Figures 1, 7, and 8 from the paper."
  },
  {
    "objectID": "dl-playground/nn-memorization/index.html#fig-1",
    "href": "dl-playground/nn-memorization/index.html#fig-1",
    "title": "A Closer Look at Memorization in Deep Networks",
    "section": "Fig 1",
    "text": "Fig 1\nTo support the first claim, the authors argue if networks simply memorize inputs they perform equally when on different training examples. However, if networks learn patterns, there should be points that are easy to learn because they fit these patterns better than others. To see if this is the case they train an MLP for a single epoch starting from 100 different initializations and data shufflings and log the percentage of times an example was correctly classified.\nThe experiment is performed with the CIFAR10 dataset, a noisy input version RandX, and a noisy label version RandY. We first define dataset wrappers to implement the noisy variants. Note that for epoch-to-epoch consistency we determine which examples to corrupt at initialization.\n\nclass RandX(Dataset):\n    \"\"\"Injects example noise into dataset by replacing x% of inputs with random gaussian N(0, 1) noise\"\"\"\n    def __init__(self, dataset, x = 1.0):\n        self.dataset = dataset\n        self.x = x\n\n        self.modified = {}\n        for idx, (img, _) in enumerate(self.dataset):\n            if np.random.rand() &lt;= x:\n                self.modified[idx] = torch.randn_like(img)\n        torch.save(self.modified, os.path.join(dataset.root, 'randX_modified'))\n\n    def __len__(self): return len(self.dataset)\n\n    def __getitem__(self, idx):\n        X, y = self.dataset[idx]\n        return self.modified.get(idx, X), y \n\nclass RandY(Dataset):\n    \"\"\"Injects example noise into dataset by replacing y% of labels with random labels\"\"\"\n    def __init__(self, dataset, y = 1.0):\n        self.dataset = dataset\n        self.y = y\n\n        self.modified = {}\n        for idx in range(len(self.dataset)):\n            if np.random.rand() &lt;= y:\n                self.modified[idx] = np.random.randint(0, len(self.dataset.classes))\n        torch.save(self.modified, os.path.join(dataset.root, 'randY_modified'))\n\n    def __len__(self): return len(self.dataset)\n\n    def __getitem__(self, idx):\n        X, y = self.dataset[idx]\n        return X, self.modified.get(idx, y)\n\nNow we define a standard training loop, initialization functions, and the MLP specified in the paper.\n\ndef train(model, train, val, optimizer, criterion = nn.CrossEntropyLoss(), epochs = 100, batch_size = 256, save_path = 'models/tmp'):\n    model.to(device)\n    train = DataLoader(train, batch_size = batch_size, shuffle = True)\n    val = DataLoader(val, batch_size = batch_size, shuffle = False)\n    best_loss = np.inf\n    for epoch in range(epochs):\n        model.train()\n        for images, labels in train:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(images), labels)\n            loss.backward()\n            optimizer.step()\n        val_loss, val_acc = eval_model(model, val)\n        if val_loss &lt; best_loss:\n            best_loss = val_loss\n            torch.save(model.state_dict(), save_path)\n        print(f'Epoch {epoch + 1}/{epochs}, Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}')\n\ndef _init_weights(m):\n    if 'Linear' in str(type(m)):\n        nn.init.xavier_uniform_(m.weight)\n        nn.init.zeros_(m.bias)\n    elif isinstance(m, nn.Conv2d):\n        nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n    elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n        nn.init.constant_(m.weight, 1)\n        nn.init.constant_(m.bias, 0)\n\ndef initialize_model(model, data_loader, in_device = False):\n    with torch.no_grad():\n        imgs, _ = next(iter(data_loader))\n        if in_device:model.to(device); model(imgs.to(device))\n        _ = model(imgs)\n        model.apply(_init_weights)\n\n\nclass MLP(nn.Module):\n    def __init__(self, n_classes = 10):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Flatten(),\n            nn.LazyLinear(4096),\n            nn.ReLU(),\n            nn.LazyLinear(4096),\n            nn.ReLU(),\n            nn.LazyLinear(n_classes)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n\nAnd run the experiment, training models for a single epoch as in the paper but also after 10 epochs to investigate how results vary.\n\ndef add_missclassified(missclassified, model, test_set, batch_size = 256):\n    model = model.to(device); model.eval()\n    test = DataLoader(test_set, batch_size = batch_size, shuffle = False)\n    i = 0\n    with torch.no_grad():\n        for images, labels in test:\n            images, labels = images.to(device), labels.to(device)\n            _, pred = torch.max(model(images), 1)\n            missclassified[i:i + test.batch_size] += (pred != labels).float()\n            i += test.batch_size\n\ndef gen_fig_1(epochs = 1, n_inits = 100):\n    training_sets = [train_set, RandX(train_set, x = 1.0), RandY(train_set, y = 1.0)]\n    for training_set in training_sets:\n        missclassified = torch.zeros(len(test_set)).to(device)\n        for _ in range(n_inits):\n            m = MLP()\n            initialize_model(m, DataLoader(train_set, batch_size = 256))\n            train(m, training_set, test_set, optim.SGD(m.parameters(), lr = 0.01), epochs = epochs)\n            add_missclassified(missclassified, m, test_set)\n        missclassified /= n_inits\n        torch.save(missclassified, f'logs/missclassified_epochs={epochs}_' + training_set.__class__.__name__)\n\ngen_fig_1(epochs = 1, n_inits = 100)\ngen_fig_1(epochs = 10, n_inits = 100)\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (9, 4), sharey = True)\n\nfor ax, epochs in zip([ax1, ax2], [1, 10]):\n\n    for fname in sorted([f for f in os.listdir('logs') if 'missclassified' in f and f'epochs={epochs}_' in f]):\n        missclassified = torch.load(os.path.join('logs', fname))\n        p = (1 - missclassified).sort().values.to('cpu').numpy()\n        ax.plot(p, label = fname.split('_')[-1])\n    \n    # Plot binomially sampled points\n    randX_mean = 1 - torch.load(os.path.join('logs', 'missclassified_RandX')).mean().item()\n    bin_data = np.random.binomial(n = 100, p = randX_mean, size = 10000) / 100\n    bin_data.sort()\n    ax.plot(bin_data, label = 'Binomial_X')\n    ax.set_title(f'Epoch = {epochs}')\n\nax1.legend()\nf.supylabel('P(correct)')\nf.supxlabel('Example(sorted by P(correct))')\nf.tight_layout()\n\n\n\n\n\n\n\n\nObserve that the left is a figure very similar to the paper’s. Whereas real data has easy patterns that can be learned in a single epoch, random data does not and networks must resort to memorization. After 10 epochs we observe that the networks trained on random data manage to improve the performance on a few points at the expense of the rest, whose performance becomes worse than random.\nOut of curiosity here are the 10 easiest and hardest examples.\n\nn = 10\nf, ax = plt.subplots(2, n, figsize = (n - 2, 2))\nfor i, idx in enumerate(torch.sort(p).indices[:n]):\n    img, label = test_set[idx]\n    ax[0][i].imshow(img.permute(1, 2, 0).numpy())\n    ax[0][i].axis('off')\n    ax[0][i].set_title(test_set.classes[label].replace('mobile', ''))\n\nfor i, idx in enumerate(torch.sort(p).indices[-n:]):\n    img, label = test_set[idx]\n    ax[1][i].imshow(img.permute(1, 2, 0).numpy())\n    ax[1][i].axis('off')\n    ax[1][i].set_title(test_set.classes[label].replace('mobile', ''))\n\nf.suptitle('Hardest (top) and easiest (bottom) examples')\nf.tight_layout()"
  },
  {
    "objectID": "dl-playground/nn-memorization/index.html#fig-2",
    "href": "dl-playground/nn-memorization/index.html#fig-2",
    "title": "A Closer Look at Memorization in Deep Networks",
    "section": "Fig 2",
    "text": "Fig 2\nThe fact that networks learn patterns when trained on real data and don’t when trained on noise can also be visualized by plotting the first layer weights of a convolutional network. We show the weights for networks trained for 10 epochs on real and random data.\n\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes):\n        super(ConvNet, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.LazyConv2d(out_channels=200, kernel_size=5),  # LazyConv2d to infer input channels\n            nn.BatchNorm2d(200),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=3),\n\n            nn.LazyConv2d(out_channels=200, kernel_size=5),  # Another LazyConv2d\n            nn.BatchNorm2d(200),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=3, stride=3),\n\n            nn.Flatten(),  # Flatten for the fully connected layer\n            nn.LazyLinear(out_features=384),  # LazyLinear to infer input features\n            nn.BatchNorm1d(384),\n            nn.ReLU(),\n\n            nn.LazyLinear(out_features=192),  # Another LazyLinear\n            nn.BatchNorm1d(192),\n            nn.ReLU(),\n\n            nn.LazyLinear(out_features=num_classes)\n        )\n\n    def forward(self, x):\n        return self.model(x)\n    \ndef train_conv(model, train, val, optimizer, criterion = nn.CrossEntropyLoss(), epochs = 100, batch_size = 256):\n    model.to(device)\n    train = DataLoader(train, batch_size = batch_size, shuffle = True)\n    val = DataLoader(val, batch_size = batch_size, shuffle = False)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 15, gamma = 0.5)\n    for epoch in range(epochs):\n        model.train()\n        for images, labels in train:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(images), labels)\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n        val_loss, val_acc = eval_model(model, val)\n        print(f'Epoch {epoch + 1}/{epochs}, Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}')\n\n\ncifar10_model = ConvNet(num_classes = n_classes)\nrandY_model = ConvNet(num_classes = n_classes)\n\nfor model, dataset in zip([cifar10_model, randY_model], [train_set, RandY(train_set)]):\n    initialize_model(model, DataLoader(dataset, batch_size = 256))\n    train_conv(model, dataset, test_set, optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9), epochs = 10)\n    torch.save(model.state_dict(), f'models/fig2_{model.__class__.__name__}')\n\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (9, 4), sharey = True)\n\nfor name, model, ax in zip(['Cifar 10', 'RandY'], [cifar10_model, randY_model], [ax1, ax2]):\n\n    model.eval()\n    kernel_weights = model.model[0].weight[:7 * 12].detach().cpu().clone()\n    kernel_weights = (kernel_weights - kernel_weights.min()) / (kernel_weights.max() - kernel_weights.min())\n    filter_img = utils.make_grid(kernel_weights, nrow = 12, padding = 1)\n    ax.imshow(filter_img.permute(1, 2, 0))\n    ax.set_title(name)\n    ax.axis('off')\n\n\n\n\n\n\n\n\nAnd we are able to see that the filters learned by the network trained on real data are much more structured and seem useful in contrast to the ones learned by training on noise."
  },
  {
    "objectID": "dl-playground/nn-memorization/index.html#fig-9",
    "href": "dl-playground/nn-memorization/index.html#fig-9",
    "title": "A Closer Look at Memorization in Deep Networks",
    "section": "Fig 9",
    "text": "Fig 9\nTo attempt to show that networks trained on real data are simpler hypotheses because they learn patterns, the authors introduce Critical Sample Ratio as a way to measure complexity. The idea is to\n\n“estimate the complexity by measuring how densely points on the data manifold are present around the model’s decision boundaries. Intuitively, if we were to randomly sample points from the data distribution, a smaller fraction of points in the proximity of a decision boundary suggests that the learned hypothesis is simpler.”\n\nA simple sketch illustrates:\n\n\n\nCSR intuition sketch\n\n\nTo estimate the density of points close to decision boundaries we might perturb the original data points within a box of size \\(r\\) and see if we cross the boundary. If a point crosses a boundary we call it “critical”. The Critical Sample Ratio is then the proportion of points that are critical and we expect simpler networks to have lower CSRs.\nThe perturbation done to data points is not totally random. The technique used by the paper is presented in Algorithm 1, borrows ideas from adversarial attacks, and is called Langevin Adversarial Sample Search (LASS). Here is how I implemented it.\n\ndef standard_normal(shape):\n    r = torch.randn(shape)\n    r = r.to(device)\n    return r\n\ndef lass(model, x, alpha = 0.25 / 255, beta = 0.2 / 255, r = 0.3 / 255, eta = standard_normal, max_iter = 10):\n    \"\"\"\n    Langevin Adversarial Sample Search (LASS).\n    Finds a perturbation of x that changes the model's prediction.\n    \n        labels: Tensor of true labels corresponding to the input x.\n        alpha: Step size for the gradient sign method.\n        beta: Scaling factor for the noise.\n        r: Clipping radius for adversarial perturbations.\n        eta: Noise process.\n    \"\"\"\n    # Orignal prediction\n    with torch.no_grad():\n        pred_on_x = model(x).argmax(dim=1)\n    \n\n    x_adv = x.clone().detach().requires_grad_(True)\n    converged = False\n    iter_count = 0\n\n    while not converged and iter_count &lt; max_iter:\n        iter_count += 1\n\n        # Forward pass to get model output\n        x_adv.requires_grad_(True)\n        output = model(x_adv)\n\n        # Compute gradient of the output with respect to input\n        loss = F.cross_entropy(output, pred_on_x)  # Use actual labels\n        loss.backward()\n\n        # Compute the perturbation\n        gradient_sign = x_adv.grad.sign()\n        delta = alpha * gradient_sign + beta * eta(x_adv.shape)\n\n        with torch.no_grad():\n            \n            x_adv += delta\n\n            # Apply the clipping to each dimension so that each pixel is in the range [x - r, x + r]\n            x_adv = torch.clamp(x_adv, x - r, x + r)\n\n            # Check if the adversarial example has changed the model's prediction\n            new_output = model(x_adv)\n            if not torch.equal(output.argmax(dim=1), new_output.argmax(dim=1)):\n                converged = True\n                x_hat = x_adv.clone().detach()\n\n        # Zero the gradients for the next iteration\n        model.zero_grad()\n        if x_adv.grad is not None: x_adv.grad.zero_()\n\n    return converged, x_hat if converged else None\n\ndef compute_csr(model, test_set, n_examples = None, shuffle = False, **lass_kwargs):\n    if n_examples is None: n_examples = len(test_set)\n    model = model.to(device)\n    csr = 0\n    for i, (images, labels) in enumerate(DataLoader(test_set, batch_size = 1, shuffle = shuffle)):\n        if i == n_examples: break\n        images, labels = images.to(device), labels.to(device)\n        converged, _ = lass(model, images, **lass_kwargs)\n        if converged: csr += 1\n    return csr / n_examples\n\nThe paper sets the radius we search for adversarial examples to \\(r = 30/255\\) because it was small enough to not be noticed by a human evaluator. Here is an example.\n\nmodel = cifar10_model\nmodel.eval(); model.to(device)\nset_seed(5)\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (9, 4), sharey = True)\nfor i, (x, y) in enumerate(DataLoader(test_set, batch_size = 1, shuffle = True)):\n    x = x.to(device)\n    y = y.to(device)\n    with torch.no_grad():\n        if model(x).argmax() != y: continue\n    converged, adv = lass(model, x)\n    if converged:\n        ax1.imshow(x.squeeze().cpu().permute(1, 2, 0))\n        ax2.imshow(adv.squeeze().cpu().permute(1, 2, 0))\n        ax1.axis('off'); ax2.axis('off')\n        with torch.no_grad():\n            ax1.set_title(f'Original. Predicted: {test_set.classes[model(x).argmax().item()]}')\n            ax2.set_title(f'Adversarial. Predicted: {test_set.classes[model(adv).argmax().item()]}')\n        break\n\n\n\n\n\n\n\n\nAnd now try to compute the Critical Sample Ratio as we train models to reproduce Figure 9.\n\ndef train_fig9(model, train, val, optimizer, criterion = nn.CrossEntropyLoss(), epochs = 100, batch_size = 256):\n    val_accs, csrs = [], []\n    model.to(device)\n    train = DataLoader(train, batch_size = batch_size, shuffle = True)\n    val = DataLoader(val, batch_size = batch_size, shuffle = False)\n    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 15, gamma = 0.5)\n    for epoch in range(epochs):\n        model.train()\n        for images, labels in train:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(images), labels)\n            loss.backward()\n            optimizer.step()\n        scheduler.step()\n        val_loss, val_acc = eval_model(model, val)\n        val_accs.append(val_acc)\n        print(f'Epoch {epoch + 1}/{epochs}, Loss: {val_loss:.4f}, Accuracy: {val_acc:.4f}')\n\n        if epoch % 10 == 0:\n            csr = compute_csr(model, val.dataset, n_examples = 500, r = 40 / 255)\n            csrs.append(csr)\n            print(f'CSR: {csr:.4f}')\n    \n    return val_accs, csrs\n\nfor dataset in [train_set, RandX(train_set, x = 1.0), RandY(train_set, y = 1.0)]:\n    set_seed(seed  = 42)\n    print(dataset.__class__.__name__)\n    model = ConvNet(num_classes = n_classes)\n    initialize_model(model, DataLoader(dataset, batch_size = 256))\n    val_accs, csrs = train_fig9(model, dataset, test_set, optim.SGD(model.parameters(), lr = 0.01, momentum = 0.9), epochs = 141)\n    torch.save({'val_accs': val_accs, 'csrs': csrs}, f'logs/fig9_r=45_{dataset.__class__.__name__}')\n\n\nplt.plot(np.arange(0, 141, 10), torch.load('logs/fig9_CIFAR10')['csrs'], 'b', label = 'CIFAR10')\nplt.plot(np.arange(0, 141, 10), [0] * 15, 'r--', label = 'RandX (?)')\nplt.plot(np.arange(0, 141, 10), torch.load('logs/fig9_RandY')['csrs'], 'g', label = 'RandY')\n\nplt.legend()\nplt.xlabel('Epoch')\nplt.ylabel('Critical Sample Ratio (CSR)')\n\nText(0, 0.5, 'Critical Sample Ratio (CSR)')\n\n\n\n\n\n\n\n\n\n\n\n\nOriginal Fig 9\n\n\nWhere we observe roughly the same trend as in the paper displayed above while the network trained on real data has a somewhat constant CSR, the one trained on random labels has a higher CSR as training progresses. However, I could reproduce RandX’s behavior and obtained a constant CSR of 0. I tried different seeds, \\(r\\), and datasets (training and validation) without luck. My suspicion is that the model’s capacity and thus performance were not high enough (around 10% validation accuracy). I decided to stick with the paper’s architecture and move on."
  },
  {
    "objectID": "dl-playground/nn-memorization/index.html#what-i-learned-practiced",
    "href": "dl-playground/nn-memorization/index.html#what-i-learned-practiced",
    "title": "A Closer Look at Memorization in Deep Networks",
    "section": "What I learned / practiced",
    "text": "What I learned / practiced\n\nHow to visualize 1st layer kernel weights\nA bit about adversarial attacks\nA creative proxy for model complexity (CSR)"
  },
  {
    "objectID": "dl-playground/dl-massive-label-noise/index.html",
    "href": "dl-playground/dl-massive-label-noise/index.html",
    "title": "Deep Learning is Robust to Massive Label Noise",
    "section": "",
    "text": "The paper shows that neural networks can keep generalizing when large numbers of (non-adversarially) incorrectly labeled examples are added to datasets (MNIST, CIFAR, and ImageNet). It also appears that larger networks are more robust and that higher noise levels lead to lower optimal (fixed) learning rates.\nWe’ll focus on the uniform label noise experiment and attempt to reproduce Figure 1:\n\n\n\nFigure 1. As we increase the amount of noise in the dataset the performance drops. However, note that even when there are 100 noisy labels per clean label performance is still acceptable. For example, the Convnet still achieves 91% accuracy.\n\n\nNote: As far as I can tell the paper has no accompanying code so I’ll be filling in the details to the best of my abilities.\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os, itertools, time\n\nos.makedirs('logs', exist_ok = True)\nos.makedirs('models', exist_ok = True)\n\nseed = 42\nnp.random.seed(seed)\ntorch.manual_seed(seed)\n\ndevice = torch.device(\n    'cuda' if torch.cuda.is_available() else\n    ('mps' if torch.backends.mps.is_available() else\n    'cpu')\n)\ndevice = 'cpu' # faster for the small models we are using\n\ndef eval_model(model, test, criterion = nn.CrossEntropyLoss()):\n    model.eval()\n    correct, loss = 0, 0.0\n    with torch.no_grad():\n        for images, labels in test:\n            images, labels = images.to(device), labels.to(device)\n            _, pred = torch.max(model(images), 1)\n            correct += (pred == labels).float().sum().item()\n            loss += criterion(model(images), labels).item()\n    return loss / len(test.dataset), correct / len(test.dataset)\n\nTo generate the uniform label noise the paper augments the original dataset with an additional \\(\\alpha\\) \\((X_i, Y')\\) pairs, where \\(Y'\\) is a class sampled uniformly at random with replacement.\nTo minimize disk use I opted for a custom dataset that wraps the original. Pytorch only requires we override __len__ and __getitem__. The length is simply the original size plus the amount of noisy labels. When queried for data we’ll generate the noisy labels for the original pairs immediately after it. For example when \\(\\alpha = 2\\):\n\\[\n(X_1, Y_1), (X_1, Y'), (X_1, Y'), (X_2, Y_2), ...\n\\]\nNote that to guarantee that noisy labels are consistent between epocs, i.e. data[1] returns the same class when called again, we can’t sample the labels at query time. To avoid storing all the randomly sampled labels (\\(60, 000 \\times 100\\) in the worst case), we simply return a shifted index’s label. We can do this with MNIST because its reasonably class-balanced and shuffled.\n\nclass NoisyLabelDataset(Dataset):\n    \"\"\"Adds alpha noisy labels per original example\"\"\"\n    \n    def __init__(self, dataset, alpha):\n        self.dataset = dataset\n        self.alpha = alpha\n        self.shift = np.random.randint(0, len(dataset))\n\n    def __len__(self):\n        n = len(self.dataset)\n        return n + (self.alpha * n)\n    \n    def __getitem__(self, idx):\n        x, y = self.dataset[idx // (self.alpha + 1)]\n        if idx % (self.alpha + 1) != 0:\n            y = self.dataset[(idx + self.shift) % len(self.dataset)][1]\n        return x, y\n\nAlthough the paper appears to only include a test set, we also include a validation set to perform early stopping with.\n\nbatch_size = 128\n\ntransform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\ntrain_dataset = datasets.MNIST('data', download = True, train = True,   transform = transform)\ntest_dataset =  datasets.MNIST('data', download = True, train = False,  transform = transform)\n\nnoisy_train_dataset = NoisyLabelDataset(train_dataset, alpha = 5)\nval_dataset, test_dataset = torch.utils.data.random_split(test_dataset, (0.2, 0.8), generator = torch.Generator().manual_seed(seed))\n\ntrain_loader = DataLoader(noisy_train_dataset, batch_size = batch_size, shuffle = True)\nval_loader = DataLoader(val_dataset, batch_size = batch_size, shuffle = False)\ntest_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n\nOur training loop is pretty standard. We use the Adadelta optimizer as per the paper. Although the paper does not mention it, we assume they used early stopping: stop training when the validation accuracy does not increase after patience epochs and return the model with the highest validation accuracy.\n\ndef train(model, train_loader, val_loader, lr = 0.01, patience = 3, max_epochs = 100, verbose = False):\n    \n    model.to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adadelta(model.parameters(), lr = lr)\n\n    log = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n\n    best_val_acc = float('inf')\n    best_model = None\n\n    for epoch in range(max_epochs):\n\n        model.train()\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            loss = criterion(model(images), labels)\n            loss.backward()\n            optimizer.step()\n\n        val_loss, val_acc = eval_model(model, val_loader)\n        log['train_loss'].append(loss.item())\n        log['val_loss'].append(val_loss)\n        log['val_acc'].append(val_acc)\n\n        if verbose: print(', '.join([f'Epoch {epoch + 1}'] + [f'{k}: {v[-1]:.4f}' for k, v in log.items()]))\n\n        if val_acc &lt; best_val_acc:\n            best_val_acc = val_acc\n            best_model = model.state_dict()\n\n        # Early stopping: stop if val acc has not increased in the last `patience` epochs\n        if epoch &gt; patience and val_acc &lt;= max(log['val_acc'][-patience-1:-1]): break \n    \n    if best_model: model.load_state_dict(best_model)\n    return model, log\n\nWe train with learning rates \\(\\{0.01, 0.05, 0.1, 0.5\\}\\) as per the paper and \\(\\alpha \\in \\{0, 25, 50\\}\\) to save some compute (we should get the idea). Below we define our perceptron, MLPs with 1, 2, and 4 layers and a 4-layer Convnet. Again, since the paper does not specify hidden dims, activations, or the convnet architecture, we set it ourselves.\n\nlearning_rates = [0.01, 0.05, 0.1, 0.5]\nalphas = range(0, 75, 25)\n\nlin_relu = lambda n_in, n_out: nn.Sequential(nn.Linear(n_in, n_out), nn.ReLU())\nmodels = {\n    'perceptron':nn.Sequential(nn.Flatten(), nn.Linear(28 * 28, 10)),\n    'MLP1':nn.Sequential(nn.Flatten(), lin_relu(28 * 28, 256), nn.Linear(256, 10)),\n    'MLP2':nn.Sequential(nn.Flatten(), lin_relu(28 * 28, 256), lin_relu(256, 128), nn.Linear(128, 10)),\n    'MLP4':nn.Sequential(nn.Flatten(), lin_relu(28 * 28, 256), lin_relu(256, 128), lin_relu(128, 64), nn.Linear(64, 10)),\n    'Conv4':nn.Sequential(\n        nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n        \n        nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n        \n        nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n        \n        nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n        nn.ReLU(),\n        nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n        \n        nn.Flatten(),\n        nn.Linear(256 * 1 * 1, 1000),\n        nn.ReLU(),\n        nn.Linear(1000, 10)\n    )\n}\n\n\nfor alpha, (name, model), lr in itertools.product(alphas, models.items(), learning_rates):\n\n    noisy_train_dataset = NoisyLabelDataset(train_dataset, alpha = alpha)\n    train_loader = DataLoader(noisy_train_dataset, batch_size = batch_size, shuffle = True)\n    \n    start = time.time()\n    model, log = train(model, train_loader, val_loader, lr = lr, verbose = True)\n    test_loss, test_acc = eval_model(model, test_loader)\n    log['test_loss'] = test_loss\n    log['test_acc'] = test_acc\n\n    print(f'{name} - alpha: {alpha}, lr: {lr}, test acc: {test_acc:.4f}, took: {time.time() - start:.2f}s')\n    torch.save(log, f'logs/{name}_{alpha}_{lr}.pt')\n    torch.save(model, f'models/{name}_{alpha}_{lr}.pt')\n\nFinally, we plot the accuracies on both the validation and test sets:\n\n# Load results into dataframe\ndf = []\nfor fname in os.listdir('logs'):\n    name, alpha, lr = fname.split('_')\n    lr = float(lr.replace('.pt', ''))\n    alpha = int(alpha)\n    \n    log = torch.load('logs/' + fname)\n    tmp = {}\n    tmp['Model'] = name\n    tmp['Alpha'] = alpha\n    tmp['lr'] = lr\n    tmp['Prediction Accuracy'] = log['test_acc']\n    tmp['Validation Accuracy'] = max(log['val_acc'])\n    df.append(tmp)\ndf = pd.DataFrame(df)\n\nhue_order = ['perceptron', 'MLP1', 'MLP2', 'MLP4', 'Conv4']\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize = (8, 4), sharey = True, sharex = True)\n\nsns.lineplot(\n    df.groupby(['Model', 'Alpha']).max(), x = 'Alpha', y = 'Prediction Accuracy',\n    hue = 'Model', hue_order = hue_order, ax = ax1\n)\nax1.set_title('On Test Set')\nax1.grid()\n\nsns.lineplot(\n    df.groupby(['Model', 'Alpha']).max(), x = 'Alpha', y = 'Validation Accuracy',\n    hue = 'Model', hue_order = hue_order, ax = ax2, legend = False\n)\nax2.set_title('On Validation Set')\nax2.grid()\nplt.tight_layout()\n\n\n\n\n\n\n\n\nWe observe that the general trends seem to hold. As we add noise the performance drops and larger models tend to be more robust.\nHowever, our models overall tend to perform worse than the paper’s. At \\(\\alpha = 50\\) most of our models have accuracies below \\(60\\%\\), whereas the paper’s are around the high \\(80\\%\\)’s. In addition, our Conv4 model is already below 90% when the paper archives 91% at \\(\\alpha = 100\\).\nThis might be due to differences in training (use and implementation of early stopping), architecture implementation, random seeds (we did not try multiple / averaging because of compute), etc.\nWe also observe the trend the paper points out (in Section 5) that higher noise levels lead to smaller effective batch sizes and thus lower optimal learning rates:\n\n\n\n\n\n\n\n\n\nA few closing thoughts: Although our results did not completely align with the paper, we still find the robustness to noise impressive.\nThe paper performs and studies the effect much further under non-uniform noise, with different datasets, batch sizes, etc. It is worth a read.\nI might revisit this notebook to perform further experiments and try to answer some lingering questions:\n\nDoes within epoch sample ordering matter? Intuitively, if we place all clean labels before the noisy ones, one expects worse performance (catastrophic forgetting?)\nWhat effect does early stopping have? We used a clean validation set to determine when to stop – which is not realistic. What happens if we use the loss or no early stopping?"
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html",
    "href": "dl-playground/d2l-exercises/8-5.html",
    "title": "Batch Norm exercises",
    "section": "",
    "text": "Where I attempt to solve the exercises in section 8.5 of the d2l book from scratch in pytorch (without using the d2l library).\ntry:\n    import matplotlib.pyplot as plt\nexcept:\n    !pip3 install matplotlib\n    import matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom PIL import Image\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport os, itertools, time, random\n\ndevice = 'cpu'"
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#context",
    "href": "dl-playground/d2l-exercises/8-5.html#context",
    "title": "Batch Norm exercises",
    "section": "Context",
    "text": "Context\nThe section introduces how to implement batch norm (BN) and some of the intuitions behind its effectiveness.\nAs a quick recap, batch norm layers apply the following transform to their inputs:\n\\[\nBN(x) = \\gamma \\odot \\frac{x-\\hat \\mu_B}{\\hat \\sigma_B} + \\beta\n\\]\nWhere \\(\\gamma\\), \\(\\beta\\) are learned and \\(\\mu_B\\), \\(\\sigma_B\\) are estimated using the input’s minibatch \\(B\\) during training.\nI.e. batch norm first normalizes the input to have mean \\(0\\) and std \\(1\\), facilitating convergence during optimization.\nHowever, since BN is typically applied before activation (at least traditionally), doing so will reduce the expressive power of the layer. For instance, as pointed out by the original paper, “normalizing the inputs of a sigmoid would constrain them to the linear regime of the nonlinearity.” Below, we plot a sigmoid and note that in the [-1, 1] range (where most of the normalized data would fall) it is essentially linear.\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(6, 3))\nt_constrained = np.linspace(-1, 1, 100)\nt_full = np.linspace(-4, 4, 100)\nsigmoid = lambda t:1/(1 + np.exp(-t))\nax1.plot(t_full, sigmoid(t_full))\nax1.plot(t_constrained, sigmoid(t_constrained), c = 'r')\nax2.plot(t_constrained, sigmoid(t_constrained), c = 'r')\nax1.set_ylabel('sigmoid(x)')\nax1.set_xlabel('x')\nax2.set_xlabel('x')\nf.tight_layout()\n\n\n\n\n\n\n\n\nSo to maintain the layer’s expressive power (degrees of freedom), “we make sure that the transformation inserted in the network can represent the identity transform” and introduce \\(\\gamma\\) and \\(\\beta\\). So if it is optimal to leave the input unchanged, the network can learn to do so by setting \\(\\gamma = \\sigma_B(x)\\) and \\(\\beta = \\bar x_B\\). (This part was confusing as the section only mentions: “Next, we apply a scale coefficient and an offset to recover the lost degrees of freedom” - the paper provided clarification).\nFinally, the second reason batch norm seems to help is the implicit regularization it provides by injecting noise into the training process. What noise? \\(\\hat \\mu_B\\) and \\(\\hat \\sigma_B\\) are (noisy) estimates calculated on a sample (the minibatch). Thus, the size of the minibatch \\(|B|\\) plays an important role: too small and the estimates are too high variance; too big and the estimates become too stable (noiseless).\nAnyway, let’s get to the exercises."
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q1",
    "href": "dl-playground/d2l-exercises/8-5.html#q1",
    "title": "Batch Norm exercises",
    "section": "Q1",
    "text": "Q1\nShould we remove the bias parameter from the fully connected layer or the convolutional layer before the batch normalization? Why?\nI believe we could remove the bias parameter from both the fully connected and convolution layers if BN is applied as described in the section: right after the fully connected / convolution layer but before the activation \\(\\phi\\). Why? essentially BN is location invariant because it centers the minibatch at 0:\n\\[\nBN_{\\gamma, \\beta}(x + \\alpha) =  BN_{\\gamma, \\beta'}(x)\n\\]\nThus in the fully connected layer case: \\[\n\\boldsymbol h = \\phi(BN_{\\gamma, \\beta}(\\boldsymbol{Wx + b})) = \\phi(BN_{\\gamma, \\beta'}(\\boldsymbol{Wx}))\n\\]\nIn convolution layers, we apply BN per channel, across all locations. I.e. “each channel has its own scale and shift parameters, both of which are scalars”. And since the convolution layer also outputs a scalar bias per channel, a similar argument applies.\nBut should we remove the biases? Yes. We get the same expressive power with fewer params.\nLet’s try it out empirically on MNIST by training the BNLeNet network defined in the section, removing bias on linear and convolution layers, in turn. Although we’ll not get the same learned parameters (\\(\\beta \\to \\beta'\\)), we should get comparable performance.\n\ndef init_cnn(module):\n    \"\"\"Initialize weights for CNNs.\"\"\"\n    if type(module) in [nn.Linear, nn.Conv2d]:\n        nn.init.xavier_uniform_(module.weight)\n\n\nclass BNLeNet(nn.Module):\n    def __init__(self, num_classes=10, removed_bias = 'linear'):\n        super().__init__()\n        assert removed_bias in ['linear', 'conv', 'none']\n        self.net = nn.Sequential(\n            nn.LazyConv2d(6, kernel_size=5, bias = removed_bias == 'conv'), nn.LazyBatchNorm2d(),\n            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5, bias = removed_bias == 'conv'), nn.LazyBatchNorm2d(),\n            nn.Sigmoid(), nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(), nn.LazyLinear(120, bias = removed_bias == 'linear'), nn.LazyBatchNorm1d(),\n            nn.Sigmoid(), nn.LazyLinear(84, bias = removed_bias == 'linear'), nn.LazyBatchNorm1d(),\n            nn.Sigmoid(), nn.LazyLinear(num_classes))\n        \n    def forward(self, x):\n        return self.net(x)\n\n\ndef train(net, train_loader, test_loader, num_epochs = 5, lr = 0.1, verbose = True):\n    # Infer input shapes, initialize weights and move to device\n    _ = net(next(iter(train_loader))[0]) # Necessary before initing weights\n    net.apply(init_cnn)\n    net.to(device)\n\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.SGD(net.parameters(), lr=lr)\n    \n    test_accs = []\n    for epoch in range(num_epochs):\n        net.train()\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = net(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n        \n        eval_acc = eval_net(net, test_loader)\n        test_accs.append(eval_acc)\n        if verbose: print(f'Epoch {epoch + 1}/{num_epochs}, Test acc: {eval_acc}')\n        \n    return test_accs\n\n\n# Set data loader seed for reproducibility\ndef seed_worker(worker_id):\n    worker_seed = torch.initial_seed() % 2**32\n    np.random.seed(worker_seed)\n    random.seed(worker_seed)\n\ntest_loader = DataLoader(testing_data, batch_size = 128, shuffle = False)\n\nfor removed_bias in ['none', 'linear', 'conv']:\n\n    torch.manual_seed(0); np.random.seed(0); random.seed(0)\n    g = torch.Generator()\n    g.manual_seed(0)\n    net = BNLeNet(removed_bias = removed_bias)\n    train_loader = DataLoader(training_data, batch_size = 128, shuffle = True, worker_init_fn = seed_worker, generator = g)\n\n    train(net, train_loader, test_loader, num_epochs = 10, lr = 0.1, verbose = False)\n    print(f'{removed_bias} final test acc: \\t{eval_net(net, test_loader)}')\n\nnone final test acc:    0.9821\nlinear final test acc:  0.9869\nconv final test acc:    0.9824\n\n\nClose enough."
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q2",
    "href": "dl-playground/d2l-exercises/8-5.html#q2",
    "title": "Batch Norm exercises",
    "section": "Q2",
    "text": "Q2\nCompare the learning rates for LeNet with and without batch normalization.\n\nPlot the increase in validation accuracy.\nHow large can you make the learning rate before the optimization fails in both cases?\n\n\nclass LeNet(nn.Module):\n\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LazyConv2d(6, kernel_size=5, padding=2), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.LazyLinear(120), nn.Sigmoid(),\n            nn.LazyLinear(84), nn.Sigmoid(),\n            nn.LazyLinear(num_classes)\n        )\n\n    def forward(self, X):\n        return self.net(X)\n        \nclass BNLeNet(nn.Module):\n    def __init__(self, num_classes=10):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.LazyLinear(120), nn.LazyBatchNorm1d(), nn.Sigmoid(),\n            nn.LazyLinear(84), nn.LazyBatchNorm1d(), nn.Sigmoid(),\n            nn.LazyLinear(num_classes))\n        \n    def forward(self, x):\n        return self.net(x)\n\nWe tried \\(lr \\in \\{4, 2, 1, 0.5, 0.1, 0.05\\}\\) as \\(0.05\\) and \\(4\\) are the lowest and highest rates where LeNet still trains (in 10 epochs).\n\nf, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 4), sharey = True)\n\nfor ax, name, results in zip([ax1, ax2], ['Without BN', 'With BN'], [without_bn, with_bn]):\n    for lr, accs in zip(lrs, results):\n        ax.plot(accs, label=f'lr={lr}')\n        ax.set_title(f'{name}')\n        ax.set_xlabel('Epoch')\n\nax1.set_ylabel('Validation Accuracy')\nax2.legend()\nf.tight_layout()\n\n\n\n\n\n\n\n\nAnd we observe that applying batch norm helps early training (we achieve better performance earlier) and makes training more robust to learning rate selection.\nUnderstanding Batch Normalization poses that batch norm’s main benefit is that it allows for greater learning rates by containing activation blowup (especially in later layers), which in turn biases the optimization to “flatter” minimas with better generalization.\nIt seems our small experiment aligns with the paper, even though our network is quite shallow."
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q3",
    "href": "dl-playground/d2l-exercises/8-5.html#q3",
    "title": "Batch Norm exercises",
    "section": "Q3",
    "text": "Q3\nDo we need batch normalization in every layer? Experiment with it.\nThe paper demonstrates that it is more beneficial in later layers. Let’s see if it is in our case and remove each batch norm layer in turn. We logged test accuracies and activations and tested 5 nets per configuration.\n\nclass BNLeNet(nn.Module):\n    def __init__(self, num_classes = 10, ex_bn_layers = []):\n        # ex_bn_layers: list of BN layers to exclude (1, ..., 4)\n\n        super().__init__()\n        for i in ex_bn_layers: assert i in range(1, 5), 'There are only 4 BN layers'\n\n        layers = [\n            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.LazyLinear(120), nn.LazyBatchNorm1d(), nn.Sigmoid(),\n            nn.LazyLinear(84), nn.LazyBatchNorm1d(), nn.Sigmoid(),\n            nn.LazyLinear(num_classes)\n        ]\n\n        bn_idx = [i for i, module in enumerate(layers) if 'BatchNorm' in str(module)]\n        for i in ex_bn_layers: layers[bn_idx[i - 1]] = nn.Identity()\n\n        self.net = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return self.net(x)\n\n\ntrain_loader = DataLoader(training_data, batch_size = 128, shuffle = True)\ntest_loader = DataLoader(testing_data, batch_size = 128, shuffle = False)\nimgs, labels = next(iter(train_loader))\n\nex_bn_layers = [[], [1], [2], [3], [4]]\nmag_layers = [2, 6, 11, 14]\nresults = []\n\nfor ex in ex_bn_layers:\n\n    print(ex)\n\n    for epochs in [2, 4, 8]:\n\n        print(epochs)\n    \n        for i in range(5):\n\n            net = BNLeNet(ex_bn_layers = ex)\n            test_accs = train(net, train_loader, test_loader, num_epochs = epochs, lr = 0.1, verbose = False)\n\n            tmp = {\n                'Excluded BN layers': ex[0] if ex else 0,\n                'Epochs': epochs,\n                'Test accuracy': test_accs[-1],\n                'Iter': i\n            }\n            \n            for lyr in mag_layers:\n                sub = net.net[:lyr]\n                with torch.no_grad(): \n                    mag = (sub(imgs) ** 2).mean()\n                    tmp[f'Mean mag at {lyr}'] = mag.item()\n            \n            results.append(tmp)\n\nDoes performance change?\n\nres = pd.DataFrame(results)\nres['Excluded BN layers'].replace({0: 'None', 1: '2', 2: '6', 3: '11', 4: '14'}, inplace = True)\n\nsns.barplot(data = res, x = 'Excluded BN layers', y = 'Test accuracy', hue = 'Epochs')\nplt.ylim(0.5, 1)\n\n\n\n\n\n\n\n\nIt seems that removing the first batch norm (in the second layer) has the strongest hit on performance.\nWhat about activations? Let’s plot the \\(l_2\\) norm of activations of layers that come right after batch norm layers for a single minibatch:\n\nf, axs = plt.subplots(1, 4, figsize=(16, 4))\n\nfor i, lyr in enumerate(mag_layers):\n    sns.barplot(data = res, x = 'Excluded BN layers', y = f'Mean mag at {lyr}', ax = axs[i], hue='Epochs')\n    axs[i].set_title(f'Layer {lyr}')\n    axs[i].set_ylabel('')\n\naxs[0].set_ylabel('Mean act L2 magnitude')\nf.tight_layout()\n\n\n\n\n\n\n\n\nWe can definitely see an effect when we remove batch norm layers. In general, the activation magnitudes decrease in the removed layer. I.e. layer 2’s magnitudes when its batch norm is removed are lower than in the original network, and so on for the other layers. It also seems that other layers compensate for this decrease by increasing their magnitudes. Finally, as the paper points out, the effects appear stronger in early epochs."
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q4",
    "href": "dl-playground/d2l-exercises/8-5.html#q4",
    "title": "Batch Norm exercises",
    "section": "Q4",
    "text": "Q4\nImplement a “lite” version of batch normalization that only removes the mean, or alternatively one that only removes the variance. How does it behave?\nWe can freeze BatchNorm’s weight and bias params respectively:\n\nfor remove_mean, remove_var in [[True, True], [False, True], [True, False], [False, False]]:\n\n    # All nets \"see\" the same data\n    torch.manual_seed(0); np.random.seed(0); random.seed(0)\n    g = torch.Generator()\n    g.manual_seed(0)\n\n    net = BNLeNet()\n    for module in net.net:\n        if 'BatchNorm' in str(module):\n            if remove_mean:\n                module.bias.requires_grad = False\n            if remove_var:\n                module.weight.requires_grad = False\n\n    train_loader = DataLoader(training_data, batch_size = 128, shuffle = True, worker_init_fn = seed_worker, generator = g)\n\n    test_accs = train(net, train_loader, test_loader, num_epochs = 10, lr = 0.1, verbose = False)\n    print(f'Remove mean: {remove_mean} Remove var: {remove_var}\\t\\tfinal test acc: {test_accs[-1]}')\n\nRemove mean: True Remove var: True      final test acc: 0.9674\nRemove mean: False Remove var: True     final test acc: 0.9676\nRemove mean: True Remove var: False     final test acc: 0.9796\nRemove mean: False Remove var: False        final test acc: 0.98\n\n\nIt appears that only removing the variance has more of an effect in our case. Is this generally true? Haven’t found anything online yet."
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q5",
    "href": "dl-playground/d2l-exercises/8-5.html#q5",
    "title": "Batch Norm exercises",
    "section": "Q5",
    "text": "Q5\nFix the parameters beta and gamma. Observe and analyze the results.\nWe can accomplish this by affine = False in BatchNorm layers.\n\nclass BNLeNet(nn.Module):\n    def __init__(self, num_classes = 10, affine_bn = True):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.LazyConv2d(6, kernel_size=5), nn.LazyBatchNorm2d(affine = affine_bn), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.LazyConv2d(16, kernel_size=5), nn.LazyBatchNorm2d(affine = affine_bn), nn.Sigmoid(),\n            nn.AvgPool2d(kernel_size=2, stride=2),\n            nn.Flatten(),\n            nn.LazyLinear(120), nn.LazyBatchNorm1d(affine = affine_bn), nn.Sigmoid(),\n            nn.LazyLinear(84), nn.LazyBatchNorm1d(affine = affine_bn), nn.Sigmoid(),\n            nn.LazyLinear(num_classes))\n        \n    def forward(self, x):\n        return self.net(x)\n\n\nresults = {'affine': [], 'non affine': []}\nos.makedirs('nets', exist_ok = True)\n\nfor affine in [True, False]:\n    aff = 'affine' if affine else 'non affine'\n    for seed in range(5):\n\n        # All nets \"see\" the same data\n        torch.manual_seed(seed); np.random.seed(seed); random.seed(seed)\n        g = torch.Generator()\n        g.manual_seed(seed)\n\n        net = BNLeNet(affine_bn = affine)\n\n        train_loader = DataLoader(training_data, batch_size = 128, shuffle = True, worker_init_fn = seed_worker, generator = g)\n        test_accs = train(net, train_loader, test_loader, num_epochs = 10, lr = 0.1, verbose = False)\n\n        results[aff].append(test_accs)\n        torch.save(net, f'nets/{aff}_{seed}.pt')\n\n        print(f'Affine: {affine}\\t\\tSeed: {seed}\\t\\tfinal test acc: {test_accs[-1]}')\n\n\ntmp = []\nfor aff, res in results.items():\n    for i, test_accs in enumerate(res):\n        for epoch, acc in enumerate(test_accs):\n            tmp.append({'Affine': aff, 'Val accuracy': acc, 'Seed': i, 'Epoch': epoch + 1})\ntmp = pd.DataFrame(tmp)\nsns.lineplot(data = tmp, x = 'Epoch', y = 'Val accuracy', hue = 'Affine')\n\n\n\n\n\n\n\n\nIt seems performance is worse during early epochs but converges as training progresses. Let’s see the batch norm with affine=True layers learned weights and biases somewhat departed from their defaults (\\(1\\) and \\(0\\) respectively).\n\nlayers = [1, 5, 10, 13]\nweights = {l:[] for l in layers}\nbiases = {l:[] for l in layers}\n\nfor fname in os.listdir('nets'):\n    if 'non' in fname: continue\n    net = torch.load('nets/' + fname)\n    for layer in layers:\n        weights[layer].extend(net.net[layer].weight.detach().tolist())\n        biases[layer].extend(net.net[layer].bias.detach().tolist())\n\nf, axs = plt.subplots(2, 4, figsize=(12, 6))\nfor i, (layer, w) in enumerate(weights.items()):\n    sns.histplot(w, ax = axs[0, i])\n    axs[0, i].set_title(f'Layer {layer} BN weights')\n    axs[0, i].axvline(1, c = 'r')\n\n    sns.histplot(biases[layer], ax = axs[1, i])\n    axs[1, i].set_title(f'Layer {layer} BN biases')\n    axs[1, i].axvline(0, c = 'r')\n\n    if i &gt; 0: axs[0, i].set_ylabel(''); axs[1, i].set_ylabel('')\n\nf.tight_layout()\n\n\n\n\n\n\n\n\nIt seems that they did, especially the weights.\nNote: In Q4 and Q5 we removed various components of batch norm layers were removed and performance was compared. However, we used the same fixed learning rate \\(0.1\\) for all configurations. A more thorough analysis would have found the optimal learning rate for each configuration, as Appendix F of the paper does:"
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q6",
    "href": "dl-playground/d2l-exercises/8-5.html#q6",
    "title": "Batch Norm exercises",
    "section": "Q6",
    "text": "Q6\nCan you replace dropout by batch normalization? How does the behavior change?"
  },
  {
    "objectID": "dl-playground/d2l-exercises/8-5.html#q7",
    "href": "dl-playground/d2l-exercises/8-5.html#q7",
    "title": "Batch Norm exercises",
    "section": "Q7",
    "text": "Q7\nResearch ideas: think of other normalization transforms that you can apply:\n\nCan you apply the probability integral transform?\nCan you use a full-rank covariance estimate? Why should you probably not do that?\nCan you use other compact matrix variants (block-diagonal, low-displacement rank, Monarch, etc.)?\nDoes a sparsification compression act as a regularizer?\nAre there other projections (e.g., convex cone, symmetry group-specific transforms) that you can use?"
  },
  {
    "objectID": "posts/lsh/index.html",
    "href": "posts/lsh/index.html",
    "title": "Approximate Nearest Cosine Neighbors",
    "section": "",
    "text": "Suppose you have some vectors and wish to find, for each point, the \\(k\\) nearest points. While you could compute pairwise distances using a naïve quadratic algorithm for small datasets, this approach becomes infeasible with millions or billions of points. If the points are in a low-dimensional space, clever data structures like kd-trees, ball trees, and M-trees can achieve substantial speedups. However, in high dimensions, performance degrades, and you may need to sacrifice exactness and turn to Approximate Nearest Neighbors (ANN) techniques.\nLocality-sensitive hashing (LSH) is a family of ANN algorithms that aim to group similar points into the same (or nearby) buckets efficiently using specialized hash functions. Recall that traditional hashing tries to map items to a set of buckets uniformly and minimize collisions. Thus, traditionally, slightly changing a point results in a vastly different hash and assigned bucket. LSH uses different hashing functions that often depend on the distance metric employed. Here, we explore a simple approach using cosine distance based on Random Projection.\n\nHow it works\nThe mechanics are not very complicated. We first generate \\(N_h\\) random hyperplanes. Let’s visualize this in two dimensions with \\(n=5\\) points and \\(N_h=2\\):\n\n\nRandom data and visualisation\n# Note that we sample from a standard normal distribution\nX = torch.randn((5, 2))\nrand_planes = torch.randn((2, 2)) - 0.5\n\ndef lims(X, ax, eps = 0.1):\n    m = X[:, ax].abs().max().item()\n    m += eps\n    return (-m, m)\n\nplt.scatter(X[:, 0], X[:, 1])\nplt.xlim(lims(X, 0)); plt.ylim(lims(X, 1))\n\n# Axes\nplt.axline((0,0), (0,1), c = '0', linewidth = 0.8)\nplt.axline((0,0), (1,0), c = '0', linewidth = 0.8)\n\n# Random planes\nfor v in rand_planes:\n    x, y = v.tolist()\n    plt.axline((0,0), (y, -x), c = 'g', linewidth = 0.8)\n\n\n\n\n\n\n\n\n\nNote that the plane is effectively partitioned into four regions. The main idea is to treat the regions as buckets and assign all the points in a region to the same bucket. When a new point comes along, we simply find out which region it belongs to and then search for points in that region and nearby regions until we accumulate \\(k\\) of them.\nGreat! How do we do it computationally? We mainly need to remember that a hyperplane is characterized by its normal vector \\(\\vec{v}\\) and that we can determine on which side of it a point \\(x\\) lies by the sign of their dot product, \\(\\text{sign}(\\vec{v} \\cdot \\vec{x})\\). For example, points 0 and 2 are on opposite sides of hyperplane 0:\n\nv = rand_planes[0]\nv @ X[0] &gt;= 0, v @ X[2] &gt;= 0\n\n(tensor(True), tensor(False))\n\n\n\n\nDot product example\nplt.scatter(*X[0].tolist())\nplt.scatter(*X[2].tolist())\nplt.xlim(lims(X, 0)); plt.ylim(lims(X, 1))\nplt.axline((0,0), (0,1), c = '0', linewidth = 0.8)\nplt.axline((0,0), (1,0), c = '0', linewidth = 0.8)\n\nx, y = rand_planes[0].tolist()\nplt.axline((0,0), (y, -x), c = 'g', linewidth = 0.8, label = 'hyperplane')\nplt.quiver(*([0], [0]), x, y, color = '0.5', label = 'v')\nplt.legend()\n\n\n\n\n\n\n\n\n\nThus, to find a point’s region, we can repeat this process for all (in our case, two) of the hyperplanes. For example:\n\nX[0] @ rand_planes[0] &gt;= 0, X[0] @ rand_planes[1] &gt;= 0\n\n(tensor(True), tensor(False))\n\n\n\nX[2] @ rand_planes[0] &gt;= 0, X[2] @ rand_planes[1] &gt;= 0\n\n(tensor(False), tensor(False))\n\n\nI.e., points 0 and 2 are in different regions. Using matrix notation, we can obtain every point’s region succinctly:\n\nregions = X @ rand_planes.T &gt;= 0\nregions\n\ntensor([[ True, False],\n        [ True, False],\n        [False, False],\n        [False,  True],\n        [ True, False]])\n\n\nWe can now place each point in a region into a bucket:\n\nbuckets = {}\nfor i, reg in enumerate(regions):\n    reg = tuple(reg.tolist())\n    if reg not in buckets: buckets[reg] = []\n    buckets[reg].append(i)\nbuckets\n\n{(True, False): [0, 1, 4], (False, False): [2], (False, True): [3]}\n\n\nAnd that’s all the preprocessing we need to do. Now, to find the nearest neighbors of a query point \\(\\vec{q}\\), we find its region:\n\nq = torch.randn((2,))\nq_reg = tuple((q @ rand_planes.T &gt;= 0).tolist())\nq, q_reg, buckets.get(q_reg, [])\n\n(tensor([-0.9890,  0.9580]), (True, False), [0, 1, 4])\n\n\nIn this case, we had three points in the query’s bucket. When the number of elements in the bucket is less than \\(k\\), the common approach is to look to nearby buckets in terms of Hamming distance. I.e., we first add points in buckets one bit flip away, then two flips away, and so on until we accumulate \\(k\\) (or slightly more) points.\n\n\nConsiderations\nTo help root out any false positives—points that are not in the top \\(k\\) nearest but happened to land in or near the buckets—we can calculate the actual cosine distance from the candidate points we retrieved and return only those below a specified threshold.\nYou can imagine that the number of hyperplanes presents an accuracy-speed tradeoff: more hyperplanes imply (exponentially) more buckets, which means fewer false positives but also more computation to calculate regions. You can find details in the references.\nIf you get unlucky with the random generation of the hyperplanes, you might have a high false negative rate—actually close points that end up in far-away buckets. In this case, we could generate other sets of hyperplanes, obtain their respective candidates, and obtain a candidate pool by taking their union. See the LSH Forest paper.\nAs a final note, how we generate hyperplanes is somewhat important. You can imagine that we generally want them to be evenly distributed on the unit sphere, which is why we sample from a normal distribution. We’d also like them to be spaced out, which is why some methods generate (expensive) orthogonal random matrices. Or you might not care in practice, remembering that in high dimensions, pairs of sampled points are almost surely orthogonal.\n\n\nResources\n\nBlog post w/some probabilty details\nMining of Massive Datasets (book and course, chapter 3)\nSimilarity Estimation Techniques from Rounding Algorithms"
  }
]