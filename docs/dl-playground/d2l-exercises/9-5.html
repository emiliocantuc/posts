<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.42">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="E. Cantu">
<meta name="dcterms.date" content="2024-09-27">

<title>posts – RNNs from scratch exercises</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body>

<div id="quarto-search-results"></div>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#context" id="toc-context" class="nav-link active" data-scroll-target="#context">Context</a></li>
  <li><a href="#section" id="toc-section" class="nav-link" data-scroll-target="#section">1</a></li>
  <li><a href="#section-1" id="toc-section-1" class="nav-link" data-scroll-target="#section-1">2</a></li>
  <li><a href="#section-2" id="toc-section-2" class="nav-link" data-scroll-target="#section-2">3</a></li>
  <li><a href="#section-3" id="toc-section-3" class="nav-link" data-scroll-target="#section-3">5</a></li>
  <li><a href="#section-4" id="toc-section-4" class="nav-link" data-scroll-target="#section-4">6</a></li>
  <li><a href="#section-5" id="toc-section-5" class="nav-link" data-scroll-target="#section-5">7</a></li>
  <li><a href="#section-6" id="toc-section-6" class="nav-link" data-scroll-target="#section-6">8</a></li>
  <li><a href="#section-7" id="toc-section-7" class="nav-link" data-scroll-target="#section-7">9</a></li>
  </ul>
<div class="quarto-code-links"><h2>Code Links</h2><ul><li><a href="9-5.ipynb"><i class="bi bi-link-45deg"></i>This notebook</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">RNNs from scratch exercises</h1>
  <div class="quarto-categories">
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">exercises</div>
  </div>
  </div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p><a href="https://emiliocantuc.github.io">E. Cantu</a> </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">September 27, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<p>Where I build up intuition for RNNs and attempt to solve the exercises in <a href="https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html">section 9.5 of the d2l book</a> from scratch in pytorch (without using the d2l library).</p>
<p>First, a little context and intermediate implementations that helped me understand the one the book and Pytorch follow. For better explainers see <a href="https://explained.ai/rnn/">[1]</a>,<a href="https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb">[2]</a>,<a href="https://gist.github.com/karpathy/d4dee566867f8291f086">[3]</a>.</p>
<section id="context" class="level1">
<h1>Context</h1>
<p>RNNs are layers that maintain a state <span class="math inline">\(\mathbf{H}_t\)</span> and update it every time you a forward pass. The new value at “time” <span class="math inline">\(t\)</span> depends on the current input <span class="math inline">\(\mathbf{X}_t\)</span> and the previous value <span class="math inline">\(\mathbf{H}_{t-1}\)</span> according to</p>
<p><span class="math display">\[
\mathbf{H}_t = \phi(\mathbf{X}_t \mathbf{W}_{\textrm{xh}} + \mathbf{H}_{t-1} \mathbf{W}_{\textrm{hh}}  + \mathbf{b}_\textrm{h})
\]</span></p>
<p>After updating its state, the layer can then produce an output.</p>
<p><span class="math display">\[
\mathbf{O}_t = \mathbf{H}_t \mathbf{W}_{\textrm{hq}} + \mathbf{b}_\textrm{q}
\]</span></p>
<p>What about the dimensions? There are two choices we get to make. The first is the dimension <span class="math inline">\(d\)</span> of the vectors we represent our words/characters/tokens with that will dictate the shape of our input <span class="math inline">\(\mathbf{X}_t \in \mathbb{R}^{n \times d}\)</span>, where <span class="math inline">\(n\)</span> is the batch size. The second is the hidden dimension <span class="math inline">\(h\)</span> we wish to transform our tokens to, which will dictate the shape of the hidden state as <span class="math inline">\(\mathbf{H}_t \in \mathbb{R}^{n \times h}\)</span>.</p>
<p>We can visualize the layer by displaying how it behaves across forward passes <span class="math inline">\(t-1\)</span>, <span class="math inline">\(t\)</span>, <span class="math inline">\(t+1\)</span> by “unrolling time” horizontally.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/RNN_diagram.svg" class="img-fluid figure-img"></p>
<figcaption>Fig. 9.4.1 From the book.</figcaption>
</figure>
</div>
<p>We first sort out the data we will feed the RNN, mainly following sections <a href="">9.2</a> and <a href="">9.3</a> implemented from scratch.</p>
<div id="cell-7" class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Get and preprocess data</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># url = 'http://d2l-data.s3-accelerate.amazonaws.com/' + 'timemachine.txt'</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># with open('data/timemachine.txt', 'w') as f:</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co">#     f.write(requests.get(url).text)</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="st">'data/timemachine.txt'</span>, <span class="st">'r'</span>) <span class="im">as</span> f:</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> f.read()</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> re.sub(<span class="st">'[^A-Za-z]+'</span>, <span class="st">' '</span>, text).lower() <span class="co"># ignore punctuation: 'the time machine by h g wells i the time traveller ...'</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>tokens <span class="op">=</span> <span class="bu">list</span>(text) <span class="co"># character-level tokens: ['t', 'h', 'e', ' ', 't', 'i', 'm', 'e', ...]</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>vocab <span class="op">=</span> <span class="bu">set</span>(tokens) <span class="co"># unique characters: {' ', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', ...}</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>char_to_ix <span class="op">=</span> {c:i <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(vocab)} <span class="co"># character to index: {' ': 0, 'a': 1, 'b': 2, 'c': 3, ...}</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>ix_to_char <span class="op">=</span> {i:c <span class="cf">for</span> i, c <span class="kw">in</span> <span class="bu">enumerate</span>(vocab)} <span class="co"># index to character: {0: ' ', 1: 'a', 2: 'b', 3: 'c', ...}</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co"># Just so we can use torch's DataLoaders</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OffsetSequences(Dataset):</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, tokens, seq_len): <span class="va">self</span>.tokens <span class="op">=</span> tokens<span class="op">;</span> <span class="va">self</span>.seq_len <span class="op">=</span> seq_len</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__len__</span>(<span class="va">self</span>): <span class="cf">return</span> <span class="bu">len</span>(<span class="va">self</span>.tokens) <span class="op">-</span> <span class="va">self</span>.seq_len</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__getitem__</span>(<span class="va">self</span>, idx):</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> idx <span class="op">&gt;=</span> <span class="bu">len</span>(<span class="va">self</span>): <span class="cf">raise</span> <span class="pp">IndexError</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>        t <span class="op">=</span> <span class="kw">lambda</span> l: torch.Tensor(l).<span class="bu">long</span>()</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> t(<span class="va">self</span>.tokens[idx:idx <span class="op">+</span> <span class="va">self</span>.seq_len]), t(<span class="va">self</span>.tokens[idx <span class="op">+</span> <span class="dv">1</span>:idx <span class="op">+</span> <span class="dv">1</span> <span class="op">+</span> <span class="va">self</span>.seq_len])</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co"># Hyperparams</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_data(batch_size, num_steps, train_prop <span class="op">=</span> <span class="fl">0.7</span>):</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>    train_cutoff <span class="op">=</span> <span class="bu">int</span>(train_prop <span class="op">*</span> <span class="bu">len</span>(tokens))</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> OffsetSequences([char_to_ix[t] <span class="cf">for</span> t <span class="kw">in</span> tokens[:train_cutoff]], num_steps)</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>    test_dataset  <span class="op">=</span> OffsetSequences([char_to_ix[t] <span class="cf">for</span> t <span class="kw">in</span> tokens[train_cutoff:]], num_steps)</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a>    train_loader  <span class="op">=</span> DataLoader(train_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">True</span>, drop_last <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    test_loader   <span class="op">=</span> DataLoader(test_dataset,  batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">False</span>, drop_last <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_dataset, test_dataset, train_loader, test_loader</span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a>batch_size, num_steps <span class="op">=</span> <span class="dv">1024</span>, <span class="dv">16</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>train_dataset, test_dataset, train_loader, test_loader <span class="op">=</span> get_data(batch_size, num_steps)</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>train_dataset_demo, test_dataset_demo, train_loader_demo, test_loader_demo <span class="op">=</span> get_data(<span class="dv">2</span>, <span class="dv">3</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can now obtain sequences of our text data offset by one to feed as input and targets. Let’s display the first few tokenized characters of the text and the first (input, target) pair when we set the sequence length <code>num_steps</code> to 3:</p>
<div id="cell-9" class="cell" data-execution_count="1100">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> train_dataset_demo[<span class="dv">0</span>]</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>[char_to_ix[i] <span class="cf">for</span> i <span class="kw">in</span> tokens[:<span class="dv">5</span>]], X, Y</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1100">
<pre><code>([13, 12, 8, 25, 13], tensor([13, 12,  8]), tensor([12,  8, 25]))</code></pre>
</div>
</div>
<p>And can use data loaders to chunk several such pairs. For example, if we set the <code>batch_size=2</code>:</p>
<div id="cell-11" class="cell" data-execution_count="1101">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(DataLoader(train_dataset_demo, batch_size <span class="op">=</span> <span class="dv">2</span>, shuffle <span class="op">=</span> <span class="va">False</span>)))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'X: </span><span class="sc">{</span>X<span class="sc">}</span><span class="ch">\n</span><span class="ss">Y: </span><span class="sc">{</span>Y<span class="sc">}</span><span class="ch">\n\n</span><span class="ss">Both shaped as: </span><span class="sc">{</span>X<span class="sc">.</span>shape<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>X: tensor([[13, 12,  8],
        [12,  8, 25]])
Y: tensor([[12,  8, 25],
        [ 8, 25, 13]])

Both shaped as: torch.Size([2, 3])</code></pre>
</div>
</div>
<p>In practice, we’ll have much larger sequence lengths, and batch sizes and will set <code>shuffle=True</code> to grab random sequences from the corpus. Finally, we need to one-hot encode the tokens (<a href="https://d2l.ai/chapter_recurrent-neural-networks/rnn-scratch.html#one-hot-encoding">Sec. 9.5.2.1</a>) and end up with the shape (sequence length, batch size, vocab size):</p>
<div id="cell-13" class="cell" data-execution_count="1102">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> F.one_hot(X.T, <span class="bu">len</span>(vocab)).<span class="bu">type</span>(torch.float32)</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>X.shape <span class="co"># seq_length, batch_size, vocab_size</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1102">
<pre><code>torch.Size([3, 2, 27])</code></pre>
</div>
</div>
<p>Since we are now ready to start feeding in data let’s start with a literal implementation of the above description of the RNN layer:</p>
<div id="cell-15" class="cell" data-execution_count="1103">
<details open="" class="code-fold">
<summary>A literal implementation</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNNScratch(nn.Module):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""The RNN model implemented from scratch."""</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_inputs, num_hiddens, num_outputs, sigma <span class="op">=</span> <span class="fl">0.01</span>):</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_inputs <span class="op">=</span> num_inputs<span class="op">;</span> <span class="va">self</span>.num_hiddens <span class="op">=</span> num_hiddens<span class="op">;</span> <span class="va">self</span>.num_outputs <span class="op">=</span> num_outputs</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_xh <span class="op">=</span> nn.Parameter(torch.randn(num_inputs, num_hiddens) <span class="op">*</span> sigma)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_hh <span class="op">=</span> nn.Parameter(torch.randn(num_hiddens, num_hiddens) <span class="op">*</span> sigma)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_h <span class="op">=</span> nn.Parameter(torch.zeros(num_hiddens))</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_hq <span class="op">=</span> nn.Parameter(torch.randn(num_hiddens, num_outputs) <span class="op">*</span> sigma)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_q <span class="op">=</span> nn.Parameter(torch.zeros(num_outputs))</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X, H <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># X: (batch_size, vocab_size)</span></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> H <span class="kw">is</span> <span class="va">None</span>: H <span class="op">=</span> torch.zeros(X.shape[<span class="dv">0</span>], <span class="va">self</span>.num_hiddens)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>        H <span class="op">=</span> torch.tanh(torch.matmul(X, <span class="va">self</span>.W_xh) <span class="op">+</span> torch.matmul(H, <span class="va">self</span>.W_hh) <span class="op">+</span> <span class="va">self</span>.b_h)</span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a>        O <span class="op">=</span> torch.matmul(H, <span class="va">self</span>.W_hq) <span class="op">+</span> <span class="va">self</span>.b_q</span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> O, H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Remember we feed RNNs tokens sequentially. Let’s first feed in the first tokens and inspect the output shapes.</p>
<div id="cell-17" class="cell" data-execution_count="1104">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> RNNScratch(num_inputs <span class="op">=</span> <span class="bu">len</span>(vocab), num_hiddens <span class="op">=</span> <span class="dv">64</span>, num_outputs <span class="op">=</span> <span class="bu">len</span>(vocab))</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>output, hidden_state <span class="op">=</span> m(X[<span class="dv">0</span>])</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>X[<span class="dv">0</span>].shape, output.shape, hidden_state.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1104">
<pre><code>(torch.Size([2, 27]), torch.Size([2, 27]), torch.Size([2, 64]))</code></pre>
</div>
</div>
<p>Which makes sense since the layer must output, for each example in the batch, (the logits for) which of the 27 characters is more likely to follow the input. We also have the layer output the updated hidden state <code>H</code> to pass in the next forward pass(es):</p>
<div id="cell-19" class="cell" data-execution_count="1105">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>output, hidden_state <span class="op">=</span> m(X[<span class="dv">1</span>], hidden_state)</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>output, hidden_state <span class="op">=</span> m(X[<span class="dv">2</span>], hidden_state)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Since we’ve now reached the end of the sequence we can compare the predicted and actual last word to compute a loss:</p>
<div id="cell-21" class="cell" data-execution_count="1106">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>F.cross_entropy(output, Y.T[<span class="op">-</span><span class="dv">1</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1106">
<pre><code>tensor(3.2961, grad_fn=&lt;NllLossBackward0&gt;)</code></pre>
</div>
</div>
<p>With this simple setup, we can already train a simple language model:</p>
<div id="cell-23" class="cell" data-execution_count="1108">
<details class="code-fold">
<summary>Train small lm</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train on train_loader, not train_loader_demo</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> RNNScratch(num_inputs <span class="op">=</span> <span class="bu">len</span>(vocab), num_hiddens <span class="op">=</span> <span class="dv">32</span>, num_outputs <span class="op">=</span> <span class="bu">len</span>(vocab))</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> optim.SGD(m.parameters(), lr <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> X, Y <span class="kw">in</span> train_loader:</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> F.one_hot(X.T, <span class="bu">len</span>(vocab)).<span class="bu">type</span>(torch.float32)</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    opt.zero_grad()</span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>    hidden_state <span class="op">=</span> <span class="va">None</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        output, hidden_state <span class="op">=</span> m(X[i], hidden_state)</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(output, Y.T[<span class="op">-</span><span class="dv">1</span>])</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>    opt.step()</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>    losses.append(np.exp(loss.item()))</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>plt.plot(losses)<span class="op">;</span> plt.ylabel(<span class="st">'perplexity'</span>)<span class="op">;</span> plt.xlabel(<span class="st">'batch'</span>)<span class="op">;</span> plt.title(<span class="ss">f'Final perplexity: </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1108">
<pre><code>Text(0.5, 1.0, 'Final perplexity: 12.89')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="9-5_files/figure-html/cell-11-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Now instead of only teaching the network to predict the last character of the sequence using all the previous ones <span class="math inline">\((x_t | x_{t-1}, ~\dots~x_1)\)</span> we could <a href="https://github.com/fastai/fastbook/blob/master/12_nlp_dive.ipynb">“increase de signal”</a> in our loss by evaluating all intermediate predictions <span class="math inline">\((x_2 | x_1)\)</span>, <span class="math inline">\((x_3|x_2, x_1)\)</span>, …, <span class="math inline">\((x_t | x_{t-1}, ~\dots~x_1)\)</span>. We can do so by accumulating the intermediate logits in a list, stacking them, and then making sure our shapes make sense before passing to <code>cross_entropy</code>. Here we assume all intermediate predictions are equally weighted in the loss, but you could play around with this.</p>
<div id="cell-25" class="cell" data-execution_count="1109">
<details class="code-fold">
<summary>Train with intermediate predictions</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># train on train_loader, not train_loader_demo</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> RNNScratch(num_inputs <span class="op">=</span> <span class="bu">len</span>(vocab), num_hiddens <span class="op">=</span> <span class="dv">32</span>, num_outputs <span class="op">=</span> <span class="bu">len</span>(vocab))</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> optim.SGD(m.parameters(), lr <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>losses <span class="op">=</span> []</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> X, Y <span class="kw">in</span> train_loader:</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> F.one_hot(X.T, <span class="bu">len</span>(vocab)).<span class="bu">type</span>(torch.float32)</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    opt.zero_grad()</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> [] <span class="co"># accumulate intermediate predictions</span></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>    hidden_state <span class="op">=</span> <span class="va">None</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(num_steps):</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>        output, hidden_state <span class="op">=</span> m(X[i], hidden_state)</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        outputs.append(output)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> torch.stack(outputs) <span class="co"># (seq_len, batch_size, vocab_size)</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> outputs.view(<span class="op">-</span><span class="dv">1</span>, <span class="bu">len</span>(vocab)) <span class="co"># (seq_len * batch_size, vocab_size)</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    targets <span class="op">=</span> Y.view(<span class="op">-</span><span class="dv">1</span>) <span class="co"># (batch_size, seq_len) -&gt; (seq_len * batch_size)</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> F.cross_entropy(outputs, targets)</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>    loss.backward()</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>    opt.step()</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>    losses.append(np.exp(loss.item()))</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a>plt.plot(losses)<span class="op">;</span> plt.ylabel(<span class="st">'perplexity'</span>)<span class="op">;</span> plt.xlabel(<span class="st">'batch'</span>)<span class="op">;</span> plt.title(<span class="ss">f'Final perplexity: </span><span class="sc">{</span>losses[<span class="op">-</span><span class="dv">1</span>]<span class="sc">:.2f}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1109">
<pre><code>Text(0.5, 1.0, 'Final perplexity: 17.08')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="9-5_files/figure-html/cell-12-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Why do we get a higher loss? Well, asking the RNN to make correct intermediate predictions is a harder task than just predicting the last character. Thus we probably require more training, capacity, and tuning.</p>
<p>Anyway, our training loop is getting pretty messy. To match the book’s implementation and tidy up our loop we can first let the RNN loop though the sequence itself:</p>
<div id="cell-28" class="cell" data-execution_count="1110">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNNScratch(nn.Module):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""The RNN model implemented from scratch."""</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_inputs, num_hiddens, num_outputs, sigma <span class="op">=</span> <span class="fl">0.01</span>):</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_inputs <span class="op">=</span> num_inputs<span class="op">;</span> <span class="va">self</span>.num_hiddens <span class="op">=</span> num_hiddens<span class="op">;</span> <span class="va">self</span>.num_outputs <span class="op">=</span> num_outputs</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_xh <span class="op">=</span> nn.Parameter(torch.randn(num_inputs, num_hiddens) <span class="op">*</span> sigma)</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_hh <span class="op">=</span> nn.Parameter(torch.randn(num_hiddens, num_hiddens) <span class="op">*</span> sigma)</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_h <span class="op">=</span> nn.Parameter(torch.zeros(num_hiddens))</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_hq <span class="op">=</span> nn.Parameter(torch.randn(num_hiddens, num_outputs) <span class="op">*</span> sigma)</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_q <span class="op">=</span> nn.Parameter(torch.zeros(num_outputs))</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X, H <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Expects X: (seq_length, batch_size, num_inputs i.e. vocab_size)</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Outputs X: (seq_len, batch_size, num_outputs i.e. vocab_size)</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> H <span class="kw">is</span> <span class="va">None</span>: H <span class="op">=</span> torch.zeros(X.shape[<span class="dv">1</span>], <span class="va">self</span>.num_hiddens) <span class="co"># (batch_size, num_hiddens)</span></span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> []</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X_i <span class="kw">in</span> X: <span class="co"># loop over first dim</span></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>            H <span class="op">=</span> torch.tanh(torch.matmul(X_i, <span class="va">self</span>.W_xh) <span class="op">+</span> torch.matmul(H, <span class="va">self</span>.W_hh) <span class="op">+</span> <span class="va">self</span>.b_h)</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>            O <span class="op">=</span> torch.matmul(H, <span class="va">self</span>.W_hq) <span class="op">+</span> <span class="va">self</span>.b_q</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>            outputs.append(O)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.stack(outputs), H</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>And make sure the output shapes still make sense:</p>
<div id="cell-30" class="cell" data-execution_count="1111">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> RNNScratch(num_inputs <span class="op">=</span> <span class="bu">len</span>(vocab), num_hiddens <span class="op">=</span> <span class="dv">32</span>, num_outputs <span class="op">=</span> <span class="bu">len</span>(vocab))</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>X, Y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(train_loader_demo))</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> F.one_hot(X.T, <span class="bu">len</span>(vocab)).<span class="bu">type</span>(torch.float32)</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>outputs, H <span class="op">=</span> m(X)</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>outputs.shape, H.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1111">
<pre><code>(torch.Size([3, 2, 27]), torch.Size([2, 32]))</code></pre>
</div>
</div>
<p>Finally, the book and Pytorch don’t include the output linear layer in the RNN layer. This way we can have the RNN layer focus only on updating the hidden state and could use it to generate output using a linear layer or something more complicated like a whole decoder module.</p>
<div id="cell-32" class="cell" data-execution_count="1112">
<details class="code-fold">
<summary>RNN layer without output layer</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNNScratch(nn.Module):</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""The RNN model implemented from scratch."""</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_inputs, num_hiddens, sigma <span class="op">=</span> <span class="fl">0.01</span>):</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.num_inputs <span class="op">=</span> num_inputs<span class="op">;</span> <span class="va">self</span>.num_hiddens <span class="op">=</span> num_hiddens</span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_xh <span class="op">=</span> nn.Parameter(torch.randn(num_inputs, num_hiddens) <span class="op">*</span> sigma)</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_hh <span class="op">=</span> nn.Parameter(torch.randn(num_hiddens, num_hiddens) <span class="op">*</span> sigma)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_h <span class="op">=</span> nn.Parameter(torch.zeros(num_hiddens))</span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X, H <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Expects X: (seq_length, batch_size, num_inputs i.e. vocab_size)</span></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Outputs: (seq_len, batch_size, num_outputs i.e. vocab_size)</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> H <span class="kw">is</span> <span class="va">None</span>: H <span class="op">=</span> torch.zeros(X.shape[<span class="dv">1</span>], <span class="va">self</span>.num_hiddens)</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>        hidden_states <span class="op">=</span> [] <span class="co"># we now return all hidden states</span></span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X_i <span class="kw">in</span> X:</span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>            H <span class="op">=</span> torch.tanh(torch.matmul(X_i, <span class="va">self</span>.W_xh) <span class="op">+</span> torch.matmul(H, <span class="va">self</span>.W_hh) <span class="op">+</span> <span class="va">self</span>.b_h)</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>            hidden_states.append(H)</span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.stack(hidden_states)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We can now define a language model module to deal with generating the output, embedding the input, and sampling sequences from the model.</p>
<div id="cell-34" class="cell" data-execution_count="1113">
<details open="" class="code-fold">
<summary>RNN Language Model</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNNLM(nn.Module):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, num_hiddens, sigma <span class="op">=</span> <span class="fl">0.01</span>):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> RNNScratch(vocab_size, num_hiddens, sigma)</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.W_hq <span class="op">=</span> nn.Parameter(torch.randn(num_hiddens, vocab_size) <span class="op">*</span> sigma)</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.b_q <span class="op">=</span> nn.Parameter(torch.zeros(vocab_size))</span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> embed(<span class="va">self</span>, X): <span class="cf">return</span> F.one_hot(X.T, <span class="va">self</span>.vocab_size).<span class="bu">type</span>(torch.float32)</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer(<span class="va">self</span>, hidden_states):</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.stack([torch.matmul(H, <span class="va">self</span>.W_hq) <span class="op">+</span> <span class="va">self</span>.b_q <span class="cf">for</span> H <span class="kw">in</span> hidden_states])</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X, H <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        hidden_states <span class="op">=</span> <span class="va">self</span>.rnn(<span class="va">self</span>.embed(X), H)</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.output_layer(hidden_states)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb22-20"><a href="#cb22-20" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, preamble, num_chars, char_to_ix):</span>
<span id="cb22-21"><a href="#cb22-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb22-22"><a href="#cb22-22" aria-hidden="true" tabindex="-1"></a>        generation <span class="op">=</span> preamble</span>
<span id="cb22-23"><a href="#cb22-23" aria-hidden="true" tabindex="-1"></a>        prepare_X <span class="op">=</span> <span class="kw">lambda</span> char: <span class="va">self</span>.embed(torch.Tensor([[char_to_ix[char]]]).<span class="bu">long</span>())</span>
<span id="cb22-24"><a href="#cb22-24" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> <span class="va">None</span></span>
<span id="cb22-25"><a href="#cb22-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-26"><a href="#cb22-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> char <span class="kw">in</span> preamble: <span class="co"># warm-up</span></span>
<span id="cb22-27"><a href="#cb22-27" aria-hidden="true" tabindex="-1"></a>            hidden_state <span class="op">=</span> <span class="va">self</span>.rnn(prepare_X(char), hidden_state)[<span class="op">-</span><span class="dv">1</span>] <span class="co"># only the last hidden state</span></span>
<span id="cb22-28"><a href="#cb22-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-29"><a href="#cb22-29" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_chars): <span class="co"># generation</span></span>
<span id="cb22-30"><a href="#cb22-30" aria-hidden="true" tabindex="-1"></a>            hidden_state <span class="op">=</span> <span class="va">self</span>.rnn(prepare_X(generation[<span class="op">-</span><span class="dv">1</span>]), hidden_state)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb22-31"><a href="#cb22-31" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.output_layer([hidden_state])</span>
<span id="cb22-32"><a href="#cb22-32" aria-hidden="true" tabindex="-1"></a>            generation <span class="op">+=</span> ix_to_char[output.argmax(dim <span class="op">=</span> <span class="dv">2</span>).item()]</span>
<span id="cb22-33"><a href="#cb22-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-34"><a href="#cb22-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> generation</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s test it out. We first sample a sequence continuation before training the model.</p>
<div id="cell-36" class="cell" data-execution_count="1129">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> RNNLM(vocab_size <span class="op">=</span> <span class="bu">len</span>(vocab), num_hiddens <span class="op">=</span> <span class="dv">32</span>)</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>m.generate(<span class="st">'it has'</span>, <span class="dv">20</span>, char_to_ix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1129">
<pre><code>'it hasxhkzwytyqdmuvngggggg'</code></pre>
</div>
</div>
<p>And train the model, now with the simplified training loop. To imitate Section 9.5.4. of the book, we do not use intermediate predictions and clip the gradients to magnitude 1 using <code>nn.utils.clip_grad_norm_</code>.</p>
<div id="cell-38" class="cell" data-execution_count="1130">
<details class="code-fold">
<summary>Evaluation function</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> <span class="bu">eval</span>(m, test_loader):</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    m.<span class="bu">eval</span>()</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> X, Y <span class="kw">in</span> test_loader:</span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> m(X)</span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> outputs[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>        targets <span class="op">=</span> Y[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">+=</span> F.cross_entropy(outputs, targets).item()</span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(loss <span class="op">/</span> <span class="bu">len</span>(test_loader)) <span class="co"># perplexity</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="cell-39" class="cell" data-execution_count="1131">
<details class="code-fold">
<summary>Train the model</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> optim.SGD(m.parameters(), lr <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>train_perplexities, test_perplexities <span class="op">=</span> [], []</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> X, Y <span class="kw">in</span> train_loader:</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>        opt.zero_grad()</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> m(X)</span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> outputs[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>        targets <span class="op">=</span> Y[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(outputs, targets)</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>        nn.utils.clip_grad_norm_(m.parameters(), <span class="dv">1</span>) <span class="co"># Clip the gradient</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>        opt.step()</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>        train_perplexities.append(np.exp(loss.item()))</span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>    test_perplexities.append(<span class="bu">eval</span>(m, test_loader))</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>plt.plot(train_perplexities, label <span class="op">=</span> <span class="st">'train'</span>)</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(train_perplexities), <span class="bu">len</span>(train_loader))),test_perplexities, label <span class="op">=</span> <span class="st">'test'</span>)</span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> plt.xlabel(<span class="st">'batch'</span>)<span class="op">;</span> plt.ylabel(<span class="st">'perplexity'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1131">
<pre><code>Text(0, 0.5, 'perplexity')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="9-5_files/figure-html/cell-19-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Finally, let’s see what the trained model generates:</p>
<div id="cell-41" class="cell" data-execution_count="1133">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>m.generate(<span class="st">'it has'</span>, <span class="dv">20</span>, char_to_ix)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="1133">
<pre><code>'it hase the stound the sto'</code></pre>
</div>
</div>
<p>I think that is enough context. Let’s get to some of the exercises I found interesting.</p>
</section>
<section id="section" class="level1">
<h1>1</h1>
<blockquote class="blockquote">
<p>Does the implemented language model predict the next token based on all the past tokens up to the very first token in The Time Machine?</p>
</blockquote>
<p>In general no. The model is trained to predict the next token based only on the previous <code>num_steps</code> tokens. However, after generation, you could pass in the whole of The Time Machine to the model in the warm-up phase and then ask it to predict the next token. In principle the prediction would be based on all tokens up to the very first token but since the model has to compress every token it has seen into the hidden state it’s likely to not remember much of the beginning of the text.</p>
</section>
<section id="section-1" class="level1">
<h1>2</h1>
<blockquote class="blockquote">
<p>Which hyperparameter controls the length of history used for prediction?</p>
</blockquote>
<p><code>num_steps</code> during training and the length of <code>prefix</code> during inference.</p>
</section>
<section id="section-2" class="level1">
<h1>3</h1>
<blockquote class="blockquote">
<p>Show that one-hot encoding is equivalent to picking a different embedding for each object.</p>
</blockquote>
<p>One-hot embedding assigns each object a vector of zeros that is of size # of objects with a 1 in the unique entry that corresponds to the object. Thus every object has a distinct embedding.</p>
</section>
<section id="section-3" class="level1">
<h1>5</h1>
<blockquote class="blockquote">
<p>Replace one-hot encoding with learnable embeddings. Does this lead to better performance?</p>
</blockquote>
<p>We can simply add an embedding module to the language model:</p>
<div id="cell-51" class="cell" data-execution_count="1134">
<details open="" class="code-fold">
<summary>RNN LM with learnable embeddings</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNNLMLearnableEmbs(RNNLM):</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, num_hiddens, sigma <span class="op">=</span> <span class="fl">0.01</span>):</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>(vocab_size, num_hiddens, sigma)</span>
<span id="cb30-4"><a href="#cb30-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.emb <span class="op">=</span> nn.Embedding(num_embeddings <span class="op">=</span> <span class="bu">len</span>(vocab), embedding_dim <span class="op">=</span> <span class="bu">len</span>(vocab))</span>
<span id="cb30-5"><a href="#cb30-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb30-6"><a href="#cb30-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> embed(<span class="va">self</span>, X): <span class="cf">return</span> <span class="va">self</span>.emb(X.T)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now the learnable parameters of the model are:</p>
<div id="cell-53" class="cell" data-execution_count="1137">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>m <span class="op">=</span> RNNLMLearnableEmbs(vocab_size <span class="op">=</span> <span class="bu">len</span>(vocab), num_hiddens <span class="op">=</span> <span class="dv">32</span>)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> name, param <span class="kw">in</span> m.named_parameters(): <span class="bu">print</span>(name, param.shape)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>W_hq torch.Size([32, 27])
b_q torch.Size([27])
rnn.W_xh torch.Size([27, 32])
rnn.W_hh torch.Size([32, 32])
rnn.b_h torch.Size([32])
emb.weight torch.Size([27, 27])</code></pre>
</div>
</div>
<div id="cell-54" class="cell" data-execution_count="1138">
<details class="code-fold">
<summary>Train the model</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> optim.SGD(m.parameters(), lr <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb33-2"><a href="#cb33-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-3"><a href="#cb33-3" aria-hidden="true" tabindex="-1"></a>train_perplexities, test_perplexities <span class="op">=</span> [], []</span>
<span id="cb33-4"><a href="#cb33-4" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">100</span>):</span>
<span id="cb33-5"><a href="#cb33-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-6"><a href="#cb33-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> X, Y <span class="kw">in</span> train_loader:</span>
<span id="cb33-7"><a href="#cb33-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-8"><a href="#cb33-8" aria-hidden="true" tabindex="-1"></a>        opt.zero_grad()</span>
<span id="cb33-9"><a href="#cb33-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-10"><a href="#cb33-10" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> m(X)</span>
<span id="cb33-11"><a href="#cb33-11" aria-hidden="true" tabindex="-1"></a>        outputs <span class="op">=</span> outputs[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb33-12"><a href="#cb33-12" aria-hidden="true" tabindex="-1"></a>        targets <span class="op">=</span> Y[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb33-13"><a href="#cb33-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-14"><a href="#cb33-14" aria-hidden="true" tabindex="-1"></a>        loss <span class="op">=</span> F.cross_entropy(outputs, targets)</span>
<span id="cb33-15"><a href="#cb33-15" aria-hidden="true" tabindex="-1"></a>        loss.backward()</span>
<span id="cb33-16"><a href="#cb33-16" aria-hidden="true" tabindex="-1"></a>        nn.utils.clip_grad_norm_(m.parameters(), <span class="dv">1</span>) <span class="co"># Clip the gradient</span></span>
<span id="cb33-17"><a href="#cb33-17" aria-hidden="true" tabindex="-1"></a>        opt.step()</span>
<span id="cb33-18"><a href="#cb33-18" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb33-19"><a href="#cb33-19" aria-hidden="true" tabindex="-1"></a>        train_perplexities.append(np.exp(loss.item()))</span>
<span id="cb33-20"><a href="#cb33-20" aria-hidden="true" tabindex="-1"></a>    test_perplexities.append(<span class="bu">eval</span>(m, test_loader))</span>
<span id="cb33-21"><a href="#cb33-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb33-22"><a href="#cb33-22" aria-hidden="true" tabindex="-1"></a>plt.plot(train_perplexities, label <span class="op">=</span> <span class="st">'train'</span>)</span>
<span id="cb33-23"><a href="#cb33-23" aria-hidden="true" tabindex="-1"></a>plt.plot(<span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">0</span>, <span class="bu">len</span>(train_perplexities), <span class="bu">len</span>(train_loader))),test_perplexities, label <span class="op">=</span> <span class="st">'test'</span>)</span>
<span id="cb33-24"><a href="#cb33-24" aria-hidden="true" tabindex="-1"></a>plt.legend()<span class="op">;</span> plt.xlabel(<span class="st">'batch'</span>)<span class="op">;</span> plt.ylabel(<span class="st">'perplexity'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1138">
<pre><code>Text(0, 0.5, 'perplexity')</code></pre>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="9-5_files/figure-html/cell-23-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>I.e. we got comparable results using one-hot encoding and learnable embeddings did not seem to improve performance.</p>
</section>
<section id="section-4" class="level1">
<h1>6</h1>
<blockquote class="blockquote">
<p>Conduct an experiment to determine how well this language model trained on The Time Machine works on other books by H. G. Wells, e.g., The War of the Worlds.</p>
</blockquote>
<div id="cell-57" class="cell" data-execution_count="1139">
<details class="code-fold">
<summary>Evaluate on other texts</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> eval_on_other_text(m, url):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> requests.get(url).text</span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a>    text <span class="op">=</span> re.sub(<span class="st">'[^A-Za-z]+'</span>, <span class="st">' '</span>, text).lower()</span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a>    tokens <span class="op">=</span> <span class="bu">list</span>(text)</span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a>    test_dataset <span class="op">=</span> OffsetSequences([char_to_ix[t] <span class="cf">for</span> t <span class="kw">in</span> tokens[:<span class="dv">10000</span>]], num_steps)</span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a>    test_loader  <span class="op">=</span> DataLoader(test_dataset,  batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">False</span>, drop_last <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="bu">eval</span>(m, test_loader)</span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a>perplexities <span class="op">=</span> {</span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a>    <span class="st">'time machine'</span>: <span class="bu">eval</span>(m, test_loader),</span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a>    <span class="st">'the sleeper awakes'</span>: eval_on_other_text(m, <span class="st">'https://gutenberg.org/cache/epub/12163/pg12163.txt'</span>),</span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a>    <span class="st">'the war of the worlds'</span>: eval_on_other_text(m, <span class="st">'https://gutenberg.org/cache/epub/36/pg36.txt'</span>),</span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>    <span class="st">'britling sees it through'</span>: eval_on_other_text(m, <span class="st">'https://gutenberg.org/cache/epub/14060/pg14060.txt'</span>),</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>    <span class="st">'certain personal matters'</span>: eval_on_other_text(m, <span class="st">'https://gutenberg.org/cache/epub/17508/pg17508.txt'</span>),</span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>perplexities</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1139">
<pre><code>{'time machine': 5.590489799933571,
 'the sleeper awakes': 7.463832230616033,
 'the war of the worlds': 6.850138662194433,
 'britling sees it through': 8.232215427642316,
 'certain personal matters': 7.385997756118561}</code></pre>
</div>
</div>
<p>We get increased losses on texts by the same author, presumably (and hopefully) because of changes in the stories, plot lines, etc. What about texts from other authors?</p>
</section>
<section id="section-5" class="level1">
<h1>7</h1>
<blockquote class="blockquote">
<p>Conduct another experiment to evaluate the perplexity of this model on books written by other authors.</p>
</blockquote>
<div id="cell-60" class="cell" data-execution_count="1140">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>other_perplexities <span class="op">=</span> {</span>
<span id="cb37-2"><a href="#cb37-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">'The Mysterious Affair (Agatha)'</span>: eval_on_other_text(m, <span class="st">'https://gutenberg.org/cache/epub/12163/pg12163.txt'</span>),</span>
<span id="cb37-3"><a href="#cb37-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Odessy (Homer)'</span>: eval_on_other_text(m, <span class="st">'https://gutenberg.org/cache/epub/1727/pg1727.txt'</span>),</span>
<span id="cb37-4"><a href="#cb37-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Candice (Voltaire)'</span>: eval_on_other_text(m, <span class="st">'https://gutenberg.org/cache/epub/19942/pg19942.txt'</span>),</span>
<span id="cb37-5"><a href="#cb37-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">'Don Quixote (Cervantes)'</span>: eval_on_other_text(m, <span class="st">'https://gutenberg.org/cache/epub/2000/pg2000.txt'</span>),</span>
<span id="cb37-6"><a href="#cb37-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb37-7"><a href="#cb37-7" aria-hidden="true" tabindex="-1"></a>other_perplexities</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="1140">
<pre><code>{'The Mysterious Affair (Agatha)': 7.463832230616033,
 'Odessy (Homer)': 7.94175036576811,
 'Candice (Voltaire)': 8.60364584900993,
 'Don Quixote (Cervantes)': 28.923269995282546}</code></pre>
</div>
</div>
<p>With the first 3 texts I tried at first, there wasn’t that much of a jump in perplexities. Evaluating the model in the last text in Spanish served as a sanity check.</p>
</section>
<section id="section-6" class="level1">
<h1>8</h1>
<blockquote class="blockquote">
<p>Modify the prediction method so as to use sampling rather than picking the most likely next character.</p>
<ul>
<li>What happens?</li>
<li>Bias the model towards more likely outputs, e.g., by sampling from <span class="math inline">\(q(x_t | x_{t-1}, ~\dots~, x_1) \propto P(x_t | x_{t-1}, ~\dots~, x_1)^\alpha\)</span> for $&gt; 1 $</li>
</ul>
</blockquote>
<p>We can do so by multiplying the logits by <span class="math inline">\(\alpha\)</span> (inverse temperature) before applying softmax. Notice that as <span class="math inline">\(\alpha \rightarrow 0\)</span> we sample tokens uniformly at random, and as <span class="math inline">\(\alpha \rightarrow +\infty\)</span> we’ll select the most probable token.</p>
<div id="cell-64" class="cell" data-execution_count="1142">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb39"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb39-1"><a href="#cb39-1" aria-hidden="true" tabindex="-1"></a>f, (ax1, ax2, ax3) <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">8</span>, <span class="dv">3</span>), sharey <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb39-2"><a href="#cb39-2" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> torch.no_grad():</span>
<span id="cb39-3"><a href="#cb39-3" aria-hidden="true" tabindex="-1"></a>    X, Y <span class="op">=</span> <span class="bu">next</span>(<span class="bu">iter</span>(test_loader))</span>
<span id="cb39-4"><a href="#cb39-4" aria-hidden="true" tabindex="-1"></a>    logits <span class="op">=</span> m(X)[<span class="op">-</span><span class="dv">1</span>][<span class="dv">0</span>]</span>
<span id="cb39-5"><a href="#cb39-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> ax, alpha <span class="kw">in</span> <span class="bu">zip</span>([ax1, ax2, ax3], [<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">20</span>]):</span>
<span id="cb39-6"><a href="#cb39-6" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> F.softmax(logits <span class="op">*</span> alpha, dim <span class="op">=</span> <span class="dv">0</span>).numpy()</span>
<span id="cb39-7"><a href="#cb39-7" aria-hidden="true" tabindex="-1"></a>        ax.bar(<span class="bu">list</span>(<span class="bu">range</span>(<span class="bu">len</span>(vocab))), probs)</span>
<span id="cb39-8"><a href="#cb39-8" aria-hidden="true" tabindex="-1"></a>        ax.set_title(<span class="vs">r'$\alpha = $'</span> <span class="op">+</span> <span class="bu">str</span>(alpha))</span>
<span id="cb39-9"><a href="#cb39-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb39-10"><a href="#cb39-10" aria-hidden="true" tabindex="-1"></a>ax2.set_xlabel(<span class="st">'Character Index'</span>)<span class="op">;</span> ax1.set_ylabel(<span class="vs">r'Predicted probability $q(x_</span><span class="sc">{t}</span><span class="vs">)$'</span>)<span class="op">;</span> f.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="9-5_files/figure-html/cell-26-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<p>Let’s observe qualitatively how generation is affected. We should expect to see less diverse but more probable sequences as we increase <span class="math inline">\(\alpha\)</span>.</p>
<div id="cell-66" class="cell" data-execution_count="1144">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="at">@torch.no_grad</span>()</span>
<span id="cb40-2"><a href="#cb40-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> generate_by_sampling(model, preamble, num_chars, alpha, char_to_ix):</span>
<span id="cb40-3"><a href="#cb40-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-4"><a href="#cb40-4" aria-hidden="true" tabindex="-1"></a>    generation <span class="op">=</span> preamble</span>
<span id="cb40-5"><a href="#cb40-5" aria-hidden="true" tabindex="-1"></a>    prepare_X <span class="op">=</span> <span class="kw">lambda</span> char: model.embed(torch.Tensor([[char_to_ix[char]]]).<span class="bu">long</span>())</span>
<span id="cb40-6"><a href="#cb40-6" aria-hidden="true" tabindex="-1"></a>    hidden_state <span class="op">=</span> <span class="va">None</span></span>
<span id="cb40-7"><a href="#cb40-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-8"><a href="#cb40-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> char <span class="kw">in</span> preamble: <span class="co"># warm-up</span></span>
<span id="cb40-9"><a href="#cb40-9" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> model.rnn(prepare_X(char), hidden_state)[<span class="op">-</span><span class="dv">1</span>] <span class="co"># only the last hidden state</span></span>
<span id="cb40-10"><a href="#cb40-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-11"><a href="#cb40-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_chars): <span class="co"># generation</span></span>
<span id="cb40-12"><a href="#cb40-12" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> model.rnn(prepare_X(generation[<span class="op">-</span><span class="dv">1</span>]), hidden_state)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb40-13"><a href="#cb40-13" aria-hidden="true" tabindex="-1"></a>        output <span class="op">=</span> model.output_layer([hidden_state])</span>
<span id="cb40-14"><a href="#cb40-14" aria-hidden="true" tabindex="-1"></a>        weights <span class="op">=</span> F.softmax(output.squeeze() <span class="op">*</span> alpha, dim <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb40-15"><a href="#cb40-15" aria-hidden="true" tabindex="-1"></a>        generation <span class="op">+=</span> ix_to_char[torch.multinomial(weights, <span class="dv">1</span>).item()]</span>
<span id="cb40-16"><a href="#cb40-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-17"><a href="#cb40-17" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> generation</span>
<span id="cb40-18"><a href="#cb40-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb40-19"><a href="#cb40-19" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> alpha <span class="kw">in</span> [<span class="fl">0.1</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">10</span>]:</span>
<span id="cb40-20"><a href="#cb40-20" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'alpha: </span><span class="sc">{</span>alpha<span class="sc">}</span><span class="ch">\t</span><span class="ss">generation:</span><span class="sc">{</span>generate_by_sampling(m, <span class="st">"friday"</span>, <span class="dv">20</span>, alpha, char_to_ix)<span class="sc">}</span><span class="ss">'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>alpha: 0.1  generation:friday furzziis   ushxthfr
alpha: 1    generation:friday thought filltise to
alpha: 2    generation:friday to little the prese
alpha: 10   generation:friday the shate in the si</code></pre>
</div>
</div>
</section>
<section id="section-7" class="level1">
<h1>9</h1>
<blockquote class="blockquote">
<p>Run the code in this section without clipping the gradient. What happens?</p>
</blockquote>
<p>I was expecting exploding gradients and, thus, for the network to be untrainable, but surprisingly I did not observe it. I tried increasing the sequence length from 32 to 256, adding hooks to debug, etc. Nothing.</p>
<div id="cell-69" class="cell" data-execution_count="1146">
<details class="code-fold">
<summary>Train with and without graddient clipping</summary>
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1" aria-hidden="true" tabindex="-1"></a>act_means, grad_norms <span class="op">=</span> [], []</span>
<span id="cb42-2"><a href="#cb42-2" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_activations(module, <span class="bu">input</span>, output): act_means.append(output[<span class="op">-</span><span class="dv">1</span>].mean().item())</span>
<span id="cb42-3"><a href="#cb42-3" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_grad_norms(module, grad_input, grad_output): grad_norms.append(np.linalg.norm(grad_output[<span class="dv">0</span>]))</span>
<span id="cb42-4"><a href="#cb42-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-5"><a href="#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> nine(clipping):</span>
<span id="cb42-6"><a href="#cb42-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-7"><a href="#cb42-7" aria-hidden="true" tabindex="-1"></a>    <span class="kw">global</span> act_means, grad_norms</span>
<span id="cb42-8"><a href="#cb42-8" aria-hidden="true" tabindex="-1"></a>    act_means, grad_norms <span class="op">=</span> [], []</span>
<span id="cb42-9"><a href="#cb42-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-10"><a href="#cb42-10" aria-hidden="true" tabindex="-1"></a>    batch_size, num_steps <span class="op">=</span> <span class="dv">1024</span>, <span class="dv">256</span></span>
<span id="cb42-11"><a href="#cb42-11" aria-hidden="true" tabindex="-1"></a>    train_dataset <span class="op">=</span> OffsetSequences([char_to_ix[t] <span class="cf">for</span> t <span class="kw">in</span> tokens[:train_cutoff]], num_steps)</span>
<span id="cb42-12"><a href="#cb42-12" aria-hidden="true" tabindex="-1"></a>    test_dataset  <span class="op">=</span> OffsetSequences([char_to_ix[t] <span class="cf">for</span> t <span class="kw">in</span> tokens[train_cutoff:]], num_steps)</span>
<span id="cb42-13"><a href="#cb42-13" aria-hidden="true" tabindex="-1"></a>    train_loader  <span class="op">=</span> DataLoader(train_dataset, batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">True</span>, drop_last <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb42-14"><a href="#cb42-14" aria-hidden="true" tabindex="-1"></a>    test_loader   <span class="op">=</span> DataLoader(test_dataset,  batch_size <span class="op">=</span> batch_size, shuffle <span class="op">=</span> <span class="va">False</span>, drop_last <span class="op">=</span> <span class="va">True</span>)</span>
<span id="cb42-15"><a href="#cb42-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-16"><a href="#cb42-16" aria-hidden="true" tabindex="-1"></a>    m <span class="op">=</span> RNNLM(vocab_size <span class="op">=</span> <span class="bu">len</span>(vocab), num_hiddens <span class="op">=</span> <span class="dv">32</span>)</span>
<span id="cb42-17"><a href="#cb42-17" aria-hidden="true" tabindex="-1"></a>    opt <span class="op">=</span> optim.SGD(m.parameters(), lr <span class="op">=</span> <span class="dv">1</span>)</span>
<span id="cb42-18"><a href="#cb42-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-19"><a href="#cb42-19" aria-hidden="true" tabindex="-1"></a>    hooks <span class="op">=</span> [m.register_forward_hook(log_activations), m.register_backward_hook(log_grad_norms)]</span>
<span id="cb42-20"><a href="#cb42-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-21"><a href="#cb42-21" aria-hidden="true" tabindex="-1"></a>    train_perplexities, test_perplexities <span class="op">=</span> [], []</span>
<span id="cb42-22"><a href="#cb42-22" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb42-23"><a href="#cb42-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-24"><a href="#cb42-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> X, Y <span class="kw">in</span> train_loader:</span>
<span id="cb42-25"><a href="#cb42-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-26"><a href="#cb42-26" aria-hidden="true" tabindex="-1"></a>            opt.zero_grad()</span>
<span id="cb42-27"><a href="#cb42-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-28"><a href="#cb42-28" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> m(X)</span>
<span id="cb42-29"><a href="#cb42-29" aria-hidden="true" tabindex="-1"></a>            outputs <span class="op">=</span> outputs[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb42-30"><a href="#cb42-30" aria-hidden="true" tabindex="-1"></a>            targets <span class="op">=</span> Y[:, <span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb42-31"><a href="#cb42-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-32"><a href="#cb42-32" aria-hidden="true" tabindex="-1"></a>            loss <span class="op">=</span> F.cross_entropy(outputs, targets)</span>
<span id="cb42-33"><a href="#cb42-33" aria-hidden="true" tabindex="-1"></a>            loss.backward()</span>
<span id="cb42-34"><a href="#cb42-34" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> clipping: nn.utils.clip_grad_norm_(m.parameters(), <span class="dv">1</span>)</span>
<span id="cb42-35"><a href="#cb42-35" aria-hidden="true" tabindex="-1"></a>            opt.step()</span>
<span id="cb42-36"><a href="#cb42-36" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb42-37"><a href="#cb42-37" aria-hidden="true" tabindex="-1"></a>            train_perplexities.append(np.exp(loss.item()))</span>
<span id="cb42-38"><a href="#cb42-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-39"><a href="#cb42-39" aria-hidden="true" tabindex="-1"></a>        test_perplexities.append(<span class="bu">eval</span>(m, test_loader))</span>
<span id="cb42-40"><a href="#cb42-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-41"><a href="#cb42-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> hook <span class="kw">in</span> hooks: hook.remove()</span>
<span id="cb42-42"><a href="#cb42-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-43"><a href="#cb42-43" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_perplexities, test_perplexities</span>
<span id="cb42-44"><a href="#cb42-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-45"><a href="#cb42-45" aria-hidden="true" tabindex="-1"></a>f, axs <span class="op">=</span> plt.subplots(<span class="dv">1</span>, <span class="dv">3</span>, figsize <span class="op">=</span> (<span class="dv">12</span>, <span class="dv">5</span>))</span>
<span id="cb42-46"><a href="#cb42-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-47"><a href="#cb42-47" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> clipping <span class="kw">in</span> [<span class="va">False</span>, <span class="va">True</span>]:</span>
<span id="cb42-48"><a href="#cb42-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-49"><a href="#cb42-49" aria-hidden="true" tabindex="-1"></a>    train_perplexities, _ <span class="op">=</span> nine(clipping <span class="op">=</span> clipping)</span>
<span id="cb42-50"><a href="#cb42-50" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">0</span>].plot(train_perplexities)</span>
<span id="cb42-51"><a href="#cb42-51" aria-hidden="true" tabindex="-1"></a>    <span class="co">#axs[0].plot(list(range(0, len(train_perplexities), len(train_loader))), test_perplexities, label = 'test')</span></span>
<span id="cb42-52"><a href="#cb42-52" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">0</span>].set_title(<span class="st">'Perplexity'</span>)<span class="co">#; axs[0].set_yscale('log')</span></span>
<span id="cb42-53"><a href="#cb42-53" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">1</span>].plot(act_means, label <span class="op">=</span> <span class="ss">f'With</span><span class="sc">{</span><span class="st">"out"</span> <span class="cf">if</span> <span class="kw">not</span> clipping <span class="cf">else</span> <span class="st">""</span><span class="sc">}</span><span class="ss"> clipping'</span>)<span class="op">;</span> axs[<span class="dv">1</span>].set_title(<span class="st">'Mean Activation'</span>)</span>
<span id="cb42-54"><a href="#cb42-54" aria-hidden="true" tabindex="-1"></a>    axs[<span class="dv">2</span>].plot(grad_norms)<span class="op">;</span> axs[<span class="dv">2</span>].set_title(<span class="st">'Mean Gradient Norm'</span>)</span>
<span id="cb42-55"><a href="#cb42-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb42-56"><a href="#cb42-56" aria-hidden="true" tabindex="-1"></a>axs[<span class="dv">1</span>].set_xlabel(<span class="st">'batch'</span>)<span class="op">;</span> axs[<span class="dv">1</span>].legend()</span>
<span id="cb42-57"><a href="#cb42-57" aria-hidden="true" tabindex="-1"></a>f.tight_layout()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="9-5_files/figure-html/cell-28-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="cell-70" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>RNN Language Model</summary>
<div class="sourceCode cell-code" id="cb43"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb43-1"><a href="#cb43-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RNNLM(nn.Module):</span>
<span id="cb43-2"><a href="#cb43-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, vocab_size, num_hiddens):</span>
<span id="cb43-3"><a href="#cb43-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb43-4"><a href="#cb43-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.vocab_size <span class="op">=</span> vocab_size</span>
<span id="cb43-5"><a href="#cb43-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-6"><a href="#cb43-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rnn <span class="op">=</span> nn.RNN(vocab_size, num_hiddens)</span>
<span id="cb43-7"><a href="#cb43-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.linear <span class="op">=</span> nn.Linear(num_hiddens, vocab_size)</span>
<span id="cb43-8"><a href="#cb43-8" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb43-9"><a href="#cb43-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> embed(<span class="va">self</span>, X): <span class="cf">return</span> F.one_hot(X.T, <span class="va">self</span>.vocab_size).<span class="bu">type</span>(torch.float32)</span>
<span id="cb43-10"><a href="#cb43-10" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-11"><a href="#cb43-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> output_layer(<span class="va">self</span>, hidden_states):</span>
<span id="cb43-12"><a href="#cb43-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> torch.stack([<span class="va">self</span>.linear(H) <span class="cf">for</span> H <span class="kw">in</span> hidden_states])</span>
<span id="cb43-13"><a href="#cb43-13" aria-hidden="true" tabindex="-1"></a>        <span class="co"># return torch.stack([torch.matmul(H, self.W_hq) + self.b_q for H in hidden_states])</span></span>
<span id="cb43-14"><a href="#cb43-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, X, H <span class="op">=</span> <span class="va">None</span>):</span>
<span id="cb43-15"><a href="#cb43-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">self</span>.rnn(<span class="va">self</span>.embed(X), H)</span>
<span id="cb43-16"><a href="#cb43-16" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-17"><a href="#cb43-17" aria-hidden="true" tabindex="-1"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb43-18"><a href="#cb43-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, preamble, num_chars, char_to_ix):</span>
<span id="cb43-19"><a href="#cb43-19" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb43-20"><a href="#cb43-20" aria-hidden="true" tabindex="-1"></a>        generation <span class="op">=</span> preamble</span>
<span id="cb43-21"><a href="#cb43-21" aria-hidden="true" tabindex="-1"></a>        prepare_X <span class="op">=</span> <span class="kw">lambda</span> char: <span class="va">self</span>.embed(torch.Tensor([[char_to_ix[char]]]).<span class="bu">long</span>())</span>
<span id="cb43-22"><a href="#cb43-22" aria-hidden="true" tabindex="-1"></a>        hidden_state <span class="op">=</span> <span class="va">None</span></span>
<span id="cb43-23"><a href="#cb43-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-24"><a href="#cb43-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> char <span class="kw">in</span> preamble: <span class="co"># warm-up</span></span>
<span id="cb43-25"><a href="#cb43-25" aria-hidden="true" tabindex="-1"></a>            hidden_state <span class="op">=</span> <span class="va">self</span>.rnn(prepare_X(char), hidden_state)[<span class="op">-</span><span class="dv">1</span>] <span class="co"># only the last hidden state</span></span>
<span id="cb43-26"><a href="#cb43-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-27"><a href="#cb43-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(num_chars): <span class="co"># generation</span></span>
<span id="cb43-28"><a href="#cb43-28" aria-hidden="true" tabindex="-1"></a>            hidden_state <span class="op">=</span> <span class="va">self</span>.rnn(prepare_X(generation[<span class="op">-</span><span class="dv">1</span>]), hidden_state)[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb43-29"><a href="#cb43-29" aria-hidden="true" tabindex="-1"></a>            output <span class="op">=</span> <span class="va">self</span>.output_layer([hidden_state])</span>
<span id="cb43-30"><a href="#cb43-30" aria-hidden="true" tabindex="-1"></a>            generation <span class="op">+=</span> ix_to_char[output.argmax(dim <span class="op">=</span> <span class="dv">2</span>).item()]</span>
<span id="cb43-31"><a href="#cb43-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb43-32"><a href="#cb43-32" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> generation</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>